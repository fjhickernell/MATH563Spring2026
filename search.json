[
  {
    "objectID": "slides/02-estimator.html#summary-statistics",
    "href": "slides/02-estimator.html#summary-statistics",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Summary statistics",
    "text": "Summary statistics\nGiven IID data, \\(X_1, \\ldots, X_n\\), we often compute\n\nEmpirical Distribution \\(F_{\\{X_i\\}}(x) := \\frac 1n \\sum_{i=1}^n \\indic(X_i \\le x)\\)\nSample Mean \\(\\displaystyle \\barX_n := \\frac{1}{n} \\sum_{i=1}^n X_i = \\int x \\, \\dif F_{\\{X_i\\}}(x)\\) to approximate the population mean \\(\\mu := \\Ex[X_1]\\)\nSample Variance \\(S^2 := \\displaystyle \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\barX_n)^2\\) to approximate the population variance \\(\\sigma^2 := \\var(X_1) := \\Ex[(X_1-\\mu)^2]\\)\n\nSometimes \\(\\hsigma^2 := \\displaystyle \\frac{1}{\\class{alert}{n}} \\sum_{i=1}^n (X_i - \\barX_n)^2\\)"
  },
  {
    "objectID": "slides/02-estimator.html#maximum-likelihood-estimators",
    "href": "slides/02-estimator.html#maximum-likelihood-estimators",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Maximum likelihood estimators",
    "text": "Maximum likelihood estimators\nThe joint density of data, \\(\\vX = (X_1, \\ldots, X_n)^\\top\\) given a parameter, \\(\\vtheta\\), is \\(\\varrho_{X_1, \\ldots, X_n| \\vtheta}\\). The likelihood, \\(L\\) turns that around to make the parameter the variable, so \\[\\begin{align*}\nL(\\vtheta | x_1, \\ldots, x_n) & := \\varrho_{X_1, \\ldots, X_n | \\vtheta}(x_1, \\ldots, x_n) \\\\\n& = \\prod_{i=1}^n \\varrho_{X_1 |\\vtheta}(x_i) \\qquad \\text{if } X_1, \\ldots, X_n \\text{ are } \\IID\n\\end{align*}\\]\nThe maximum likelihood estimator (MLE) of \\(\\vtheta\\) is the one that fits the observed data best in terms of \\[\n\\vTheta_{\\MLE}  = \\Argmax{\\vtheta} L(\\vtheta | X_1, \\ldots, X_n)\n\\]\n\\(\\exstar\\) What is the MLE of \\(p\\) for the distribution \\(\\Bern(p)\\)?\n\n\\(\\exstar\\)What is the MLE of \\(\\lambda\\) for \\(\\Exp(\\lambda)\\)? What are the MLE of \\(\\mu=\\Ex(X)\\) and \\(\\sigma^2=\\var(X)\\) for \\(X\\sim\\Exp(\\lambda)\\)?\n\n\\(\\exstar\\) What are the MLE of \\(\\mu\\) and \\(\\sigma\\) for \\(X \\sim \\Norm(\\mu,\\sigma^2)\\)?"
  },
  {
    "objectID": "slides/02-estimator.html#plug-in-estimators",
    "href": "slides/02-estimator.html#plug-in-estimators",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Plug-in estimators",
    "text": "Plug-in estimators\n\nIf \\(\\hTheta_1\\) is an estimator of \\(\\theta_1\\) and \\(\\theta_2 = g(\\theta_1)\\), then \\(\\hTheta_2 : = g(\\hTheta_1)\\) is a plug-in estimator of \\(\\theta_2\\)\nIf \\(\\hTheta_1\\) is MLE of \\(\\theta_1\\) and \\(\\theta_2 = g(\\theta_1)\\), then \\(\\hTheta_2 : = g(\\hTheta_1)\\) is an MLE of \\(\\theta_2\\)"
  },
  {
    "objectID": "slides/02-estimator.html#biasvar",
    "href": "slides/02-estimator.html#biasvar",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Bias, variance, and mean squared error of estimators",
    "text": "Bias, variance, and mean squared error of estimators\nSuppose that \\(\\Theta\\) is an estimator of a parameter, \\(\\theta\\), of a population\n\nBias \\(\\bias(\\Theta) = \\Ex(\\Theta) - \\theta\\)\n\nAsymptotic bias is \\(\\displaystyle \\lim_{n \\to \\infty} \\bias(\\Theta_n)\\), where \\(n\\) is the size of the sample on which the estimator is based\nAn estimator is unbiased if \\(\\bias(\\Theta) = 0\\)\n\\(\\barX_n\\) is an unbiased estimator of \\(\\mu = \\Ex(X_1)\\) for identically distributed data\n\nVariance we already know this definition\n\n\\(\\var(\\barX_n) \\exeq \\var(X_1)/n\\) for uncorrelated, identically distributed data\n\nMean squared error (MSE) \\(\\mse(\\Theta) := \\Ex[(\\Theta - \\theta)^2] \\exeq [\\bias(\\Theta)]^2 + \\var(\\Theta)\\)"
  },
  {
    "objectID": "slides/02-estimator.html#distribest",
    "href": "slides/02-estimator.html#distribest",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Distributions of estimators (see Important Distributions)",
    "text": "Distributions of estimators (see Important Distributions)\nFor the sample mean \\(\\barX_n\\), based on IID data\n\n\\(n \\barX_n \\sim \\Bin(n,p)\\) if \\(X \\sim \\Bern(p)\\)\n\\(\\barX_n \\exsim \\Gam(n, n \\lambda)\\) if \\(X \\sim \\Exp(\\lambda)\\) where \\(\\displaystyle \\varrho_{\\Gam(\\alpha, \\beta)}(x) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\, x^{\\alpha-1} \\exp(-\\beta x)\n\\quad x&gt;0\\)\n\nNote: \\(\\Gamma(n) = (n-1)!\\) for integer \\(n\\)\n\\(\\Gam(\\alpha, \\beta)\\) is the generic gamma distribution with shape \\(\\alpha\\) and rate \\(\\beta\\)\n\\(\\lambda n \\barX_n \\exsim \\Gam(n, 1)\\)\n\\(2 \\lambda n \\barX_n \\exsim \\chi^2_{2n}\\), where \\(\\displaystyle \\varrho_{\\chi^2_\\nu}(x) = \\frac{x^{\\nu/2 - 1} \\exp(-x/2)}{2^{\\nu/2} \\Gamma(\\nu/2)} \\quad x &gt; 0\\)"
  },
  {
    "objectID": "slides/02-estimator.html#estimators-for-exponential-families-of-distributions",
    "href": "slides/02-estimator.html#estimators-for-exponential-families-of-distributions",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Estimators for exponential families of distributions",
    "text": "Estimators for exponential families of distributions\nRecall that exponential families of distributions have PMF or PDF that can be expressed as \\[\n\\varrho(\\vx ; \\vtheta) = h(\\vx) \\, c(\\vtheta) \\, \\exp \\biggl(\\sum_k w_k(\\vtheta) t_k(\\vx) \\biggr) \\quad \\vx \\in \\reals^d\n\\] The binomial, exponential, and normal are all exponential families\n1. Likelihood equations depend on sample averages\nThe log-likelihood for IID \\(X_1,\\dots,X_n\\) is \\[\n\\ell(\\vtheta) : = \\log(L(\\vtheta))\n= \\sum_{i=1}^n \\log \\bigl(h(\\vX_i) \\bigr) +     n \\log \\bigl( c(\\vtheta) \\bigr) +\\sum_{k} w_k(\\vtheta)\\Biggl[ \\sum_{i=1}^{n} t_k(\\vX_i) \\Biggr]\n\\] Solving for the MLE depends on the data only through the averages: \\(\\displaystyle \\frac 1n \\sum_{i=1}^{n} t_k(\\vX_i)\\)\n\n\\(\\Bern(\\mu)\\) and \\(\\Exp(\\lambda)\\): \\(\\displaystyle \\frac 1n \\sum_{i=1}^{n} X_i\\), Â  Â  Â  \\(\\Norm(\\mu,\\sigma^2)\\): \\(\\displaystyle \\frac 1n \\sum_{i=1}^{n} X_i\\) and \\(\\displaystyle \\frac 1n \\sum_{i=1}^{n} X_i^2\\)"
  },
  {
    "objectID": "slides/02-estimator.html#large-sample-confidence-intervals-for-means",
    "href": "slides/02-estimator.html#large-sample-confidence-intervals-for-means",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Large sample size confidence intervals for means",
    "text": "Large sample size confidence intervals for means\nIf \\(X_1, \\ldots, X_n\\) are IID with mean \\(\\mu\\) and variance \\(\\sigma^2 &lt; \\infty\\), and \\(\\barX_n\\) is the sample mean, then by the Central Limit Theorem \\[\n\\frac{\\barX_n - \\mu}{\\sigma/\\sqrt{n}} \\appxsim \\Norm(0,1) \\quad \\text{for large } n\n\\] Letting \\(z_{\\alpha/2}\\) be the upper \\(\\alpha/2\\) quantile of \\(\\Norm(0,1)\\), i.e., \\(z_{\\alpha/2} = Q_{\\Norm(0,1)}(1 - \\alpha/2)\\), then \\[\\begin{align*}\n1 - \\alpha & \\approx\n\\Prob \\biggl( -z_{\\alpha/2} \\le \\frac{\\barX_n - \\mu}{\\sigma/\\sqrt{n}} \\le z_{\\alpha/2} \\biggr) \\\\\n& \\approx \\Prob \\biggl( \\barX_n - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\le \\mu \\le \\barX_n + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\biggr) \\\\\n& \\approx \\Prob \\biggl( \\underbrace{\\barX_n - z_{\\alpha/2} \\frac{S}{\\sqrt{n}}}_{\\Theta_L} \\le \\mu \\le \\underbrace{\\barX_n + z_{\\alpha/2} \\frac{S}{\\sqrt{n}}}_{\\Theta_U} \\biggr)\n\\end{align*}\\] where \\(S^2\\) is some estimate of the unknown population variance \\(\\sigma^2\\) (e.g., unbiased or MLE)"
  },
  {
    "objectID": "slides/02-estimator.html#small-sample-confidence-intervals-for-means-when-the-distribution-is-known",
    "href": "slides/02-estimator.html#small-sample-confidence-intervals-for-means-when-the-distribution-is-known",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Small sample size confidence intervals for means when the distribution is known",
    "text": "Small sample size confidence intervals for means when the distribution is known\nIf the sample size is not large enough for the Central Limit Theorem to apply, but the sample mean has a known distribution, then exact confidence intervals can sometimes be constructed\nExample: You observe \\(\\barX_n = 12.0\\) minutes for taxis to arrive.based on \\(n\\). You construct a \\(95\\%\\) confidence interval for the mean arrival time, \\(\\mu\\), assuming that the arrival times are distributed \\(\\Exp(1/\\mu)\\). Recall that \\(\\mu = \\sigma = 1/\\lambda\\).\nBut now we have the true distribution of \\(\\barX_n\\):\n\\[\\begin{align*}\n2 \\lambda n \\barX_n \\sim \\chi^2_{2n} \\implies 1 - \\alpha &= \\Prob \\biggl( \\chi^2_{2n, \\alpha/2} \\le 2 \\lambda n \\barX_n \\le \\chi^2_{2n, 1-\\alpha/2} \\biggr) \\\\\n& = \\Prob \\biggl( \\frac{\\chi^2_{2n, \\alpha/2}}{2 n \\barX_n} \\le \\lambda \\le \\frac{\\chi^2_{2n, 1-\\alpha/2}}{2 n \\barX_n} \\biggr) \\\\\n& = \\Prob \\biggl( \\frac{2 n \\barX_n}{\\chi^2_{2n, 1-\\alpha/2}} \\le \\mu \\le \\frac{2 n \\barX_n}{\\chi^2_{2n, \\alpha/2}} \\biggr)\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/02-estimator.html#students-t-confidence-interval-for-normal-data-with-unknown-variance",
    "href": "slides/02-estimator.html#students-t-confidence-interval-for-normal-data-with-unknown-variance",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Studentâ€™s \\(t\\) Confidence Interval for Normal Data with Unknown Variance",
    "text": "Studentâ€™s \\(t\\) Confidence Interval for Normal Data with Unknown Variance\nFor \\(X_1,\\dots,X_n \\IIDsim \\Norm(\\mu, \\sigma^2)\\)\n\nSample mean: \\(\\barX\\)\nSample standard deviation: \\(S\\)\n\nWe have an exact confidence interval for all \\(n\\): \\[\n\\Prob\\left[ \\barX - t_{1-\\alpha/2,n-1}\\frac{S}{\\sqrt{n}} \\le \\mu \\le \\barX + t_{1-\\alpha/2,n-1}\\frac{S}{\\sqrt{n}} \\right] = 1 - \\alpha\n\\] where \\(t_{1-\\alpha/2,n-1}\\) is the upper \\(\\alpha/2\\) quantile of the Studentâ€™s t distribution with \\(n-1\\) degrees of freedom\n\nFor large \\(n\\), this interval is close to the CLT-based interval"
  },
  {
    "objectID": "slides/02-estimator.html#confidence-intervals-for-means-of-differences",
    "href": "slides/02-estimator.html#confidence-intervals-for-means-of-differences",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Confidence Intervals for Means of Differences",
    "text": "Confidence Intervals for Means of Differences\nFor paired or matched data (before/after, twins, same subject measured twice) \\[\nD_i = X_i - Y_i, \\quad i = 1,\\dots,n\n\\]\nInference is about the mean difference \\(\\mu_D\\)\nNot difference of means \\(\\mu_X - \\mu_Y\\), even though \\(\\barD = \\barX - \\barY\\)\nIf \\(D_1,\\dots,D_n \\IIDsim \\Norm(\\mu_D, \\sigma_D^2)\\) \\[\n\\Prob\\left[ \\barD - t_{1-\\alpha/2,n-1}\\frac{S_D}{\\sqrt{n}} \\le \\mu_D \\le \\barD + t_{1-\\alpha/2,n-1}\\frac{S_D}{\\sqrt{n}} \\right] = 1 - \\alpha\n\\]\nIf \\(D_1,\\dots,D_n\\) are IID with finite variance, and \\(n\\) is large \\[\n\\Prob\\left[ \\barD - z_{1-\\alpha/2}\\frac{S_D}{\\sqrt{n}} \\le \\mu_D \\le \\barD + z_{1-\\alpha/2}\\frac{S_D}{\\sqrt{n}} \\right] \\approx 1 - \\alpha\n\\]"
  },
  {
    "objectID": "slides/02-estimator.html#confidence-intervals-for-differences-of-means",
    "href": "slides/02-estimator.html#confidence-intervals-for-differences-of-means",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Confidence Intervals for Differences of Means",
    "text": "Confidence Intervals for Differences of Means\nFor two independent samples (control/treatment, two groups)\n\\[\nX_1,\\dots,X_{n_X} \\sim \\text{population 1}, \\quad\nY_1,\\dots,Y_{n_Y} \\sim \\text{population 2}\n\\] with sample means \\(\\barX,\\barY\\) and sample variances \\(s_X^2,s_Y^2\\)\nIf the \\(X\\) and \\(Y\\) are Normal, we have an approximate confidence interval for \\(\\mu_X - \\mu_Y\\)\n\\[\n\\Prob\\left[\n(\\barX-\\barY)\n-\nt_{1-\\alpha/2,\\nu}\n\\sqrt{\\frac{S_X^2}{n_X}+\\frac{S_Y^2}{n_Y}} \\le \\mu_X - \\mu_Y \\le\n(\\barX-\\barY)\n+\nt_{1-\\alpha/2,\\nu}\n\\sqrt{\\frac{S_X^2}{n_X}+\\frac{S_Y^2}{n_Y}}  \n\\right] = 1 - \\alpha\n\\]\nwhere \\(\\nu\\) is an approximate degrees of freedom (value not critical here)\nOther variations exist, e.g., pooled t, Welch"
  },
  {
    "objectID": "slides/02-estimator.html#confidence-intervals-for-proportions",
    "href": "slides/02-estimator.html#confidence-intervals-for-proportions",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\nTypical setting: Bernoulli trials\nLet\n[ X (n,p), p = X/n ]\nParameter of interest: population proportion \\(p\\).\nCommon methods\n\nNormal approximation interval\n\nRequires \\(np\\) and \\(n(1-p)\\) large\nSimple but can undercover\n\nWilson / Agrestiâ€“Coull interval\n\nImproved accuracy over normal approximation\nNear-nominal coverage for moderate \\(n\\)\n\nExact (Clopperâ€“Pearson) interval\n\nNo sample size restriction\nGuaranteed coverage\nConservative (wider than necessary)\n\n\nKey tradeoff: simplicity vs coverage accuracy."
  },
  {
    "objectID": "slides/02-estimator.html#confidence-intervals-for-variances",
    "href": "slides/02-estimator.html#confidence-intervals-for-variances",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Confidence Intervals for Variances",
    "text": "Confidence Intervals for Variances\nTypical setting: inference about variability\nParameter of interest: population variance \\(\\sigma^2\\).\nCommon methods\n\nChi-squared interval\n\nAssumes data are IID Normal\nExact for any \\(n\\)\nVery sensitive to non-normality\n\nAsymptotic (CLT-based) intervals\n\nAssume finite fourth moments\nRequire large \\(n\\)\nRarely used in introductory courses\n\n\nImportant: variance inference is far less robust than mean inference."
  },
  {
    "objectID": "slides/02-estimator.html#assumptions-behind-common-confidence-intervals",
    "href": "slides/02-estimator.html#assumptions-behind-common-confidence-intervals",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Assumptions Behind Common Confidence Intervals",
    "text": "Assumptions Behind Common Confidence Intervals\nData are IID from a distribution with finite variance\n\n\n\n\n\n\n\n\n\n\nParameter\nDistributional Assumptions\nSample Size\nMethod\nNotes\n\n\n\n\n\\(\\mu\\)\nAny distribution\nLarge \\(n\\)\nCentral Limit Theorem (CLT)\nApproximate, accuracy improves as \\(n \\to \\infty\\)\n\n\n\\(\\mu\\)\nNormal data, \\(\\sigma\\) unknown\nAny \\(n\\)\nStudentâ€™s t\nExact\n\n\n\\(\\mu = p\\)\nBernoulli trials\nAny \\(n\\)\nBinomial(Clopperâ€“Pearson)\nExactConservative\n\n\n\\(\\mu = p\\)\nBernoulli trials\nLarge \\(np\\), \\(n(1-p)\\)\nCentral Limit Theorem (CLT)\nApproximate\n\n\n\\(\\mu\\)\nExponential data\nAny \\(n\\)\nGamma/Chi-squared\nExact"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/statistics/intro/intro-stats-examples.html",
    "href": "classlib/classlib/quarto/slides/shared/statistics/intro/intro-stats-examples.html",
    "title": "Statistics",
    "section": "",
    "text": "Principled\n\n\nClear assumptions\nProbability as a basis\nEfficient and accurate computation\n\n\nInference about a population\n\n\nWant to know parameters that describe a large number of individuals or the relationships among them\nShould provide a level of uncertainty\n\n\nFrom Data\n\n\nCarefully sampled\nObservations, surveys, experiments, or computer simulations\n\n\nSee Big Statistics Questions"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/statistics/intro/intro-stats-examples.html#approval-ratings",
    "href": "classlib/classlib/quarto/slides/shared/statistics/intro/intro-stats-examples.html#approval-ratings",
    "title": "Statistics",
    "section": "Approval ratings",
    "text": "Approval ratings\nThe Gallup Poll tracks the approval ratings of US presidents according to a careful polling methodology\n\nEach week they telephone \\(n=1500\\) adults\nTheir sampling error for the approval ratings is about \\(4\\%\\)\n\nLet \\(Y \\sim \\Bern(\\mu)\\), a Bernoulli (free throw shooting) distribution with probability of success \\(\\mu\\), \\[\n\\Prob(Y =y) = \\begin{cases} \\mu, & y=1, \\text{ (yes, approve)}\\\\\n1-\\mu, & y = 0, \\text{ (no, do not approve)}\n\\end{cases}\n\\]\n\n\nIf \\(Y_1, \\ldots, Y_n \\IIDsim \\Bern(\\mu)\\), then \\[\\begin{align*}\nT &:= Y_1 + \\cdots + Y_n \\sim \\Bin(n,\\mu), \\; \\Prob(T = t) = \\binom{n}{t} \\mu^t (1-\\mu)^{n-t}\\\\\n\\barY &:= \\frac 1n (Y_1 + \\cdots + Y_n) \\\\\n& \\appxsim  \\Norm\\bigl(\\mu,\\mu(1-\\mu)/n\\bigr) \\quad \\text{by the }\\class{alert}{\\text{Central Limit Theorem}}\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\nIf \\(Y_1, \\ldots, Y_n \\IIDsim \\Bern(\\mu)\\), then we can construct a confidence interval that captures the true approval rating with high probability: \\[\\begin{align*}\nT &:= Y_1 + \\cdots + Y_n \\sim \\Bin(n,\\mu), \\quad \\Prob(T = y) = \\binom{n}{t} \\mu^t (1-\\mu)^{n-t}\\\\\n\\barY &:= \\frac 1n (Y_1 + \\cdots + Y_n) \\appxsim  \\Norm\\bigl(\\mu,\\mu(1-\\mu)/n\\bigr) \\quad \\text{by the }\\class{alert}{\\text{Central Limit Theorem}}\n\\end{align*}\\]\n\n\n\n\\[\\begin{align*}\n95\\% & \\approx \\Prob\\Bigl[\\mu - 1.96\\sqrt{\\mu(1-\\mu)/n} \\le \\class{alert}{\\barY} \\le \\mu + 1.96\\sqrt{\\mu(1-\\mu)/n}\\Bigr] \\\\\n& = \\Prob\\Bigl[\\barY - 1.96\\sqrt{\\mu(1-\\mu)/n} \\le \\class{alert}{\\mu} \\le \\barY + 1.96\\sqrt{\\mu(1-\\mu)/n}\\Bigr] \\\\\n& \\approx \\Prob\\Bigl[\\barY - 1.96\\sqrt{\\barY(1-\\barY)/n} \\le \\class{alert}{\\mu} \\le \\barY + 1.96\\sqrt{\\barY(1-\\barY)/n}\\Bigr] \\\\\n& \\le \\Prob\\Bigl[\\barY - 1/\\sqrt{n} \\le \\class{alert}{\\mu} \\le \\barY + 1/\\sqrt{n} \\Bigr] \\quad \\text{since } \\sqrt{\\barY(1-\\barY)} \\le 1/2\n\\end{align*}\\] For \\(n = 1000\\) we get \\(1/\\sqrt{n} \\approx 3\\%\\).  â¬‡ Approval Rating Jupyter ðŸ““ \n\n\n  \n\n\n\n\nPopulation is US adults\nModel\n\nBernoulli distribution\nIgnores demographic factors that might explain approvals\n\nUncertainty of the estimate of the proportion, \\(\\mu\\)\n\nProvided by a confidence interval (CI)\nFacilitated by the Central Limit Theorem\nThis is the CI capturing \\(\\mu\\), not probability of \\(\\mu\\) in the CI"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/statistics/intro/intro-stats-examples.html#life-gdp",
    "href": "classlib/classlib/quarto/slides/shared/statistics/intro/intro-stats-examples.html#life-gdp",
    "title": "Statistics",
    "section": "Life expectancy vs gross domestic product (GDP) per capita",
    "text": "Life expectancy vs gross domestic product (GDP) per capita\nCountries differ dramatically in both life expectancy and economic output. A classic dataset (World Bank / Gapminder) records, for each country,\n\nLife \\(=\\) Life expectancy at birth (years)\nGDP \\(=\\) Gross domestic product per capita (USD, purchasing-power adjusted)\n\nEach point represents one country, not one individual. We are interested in the conditional behavior of life expectancy with given national income:\n\n\n\\[\n\\Ex(\\text{Life} | \\text{GDP} = x)\n\\]\n\n\n\n\n\n\n\n\n\nThe scatterplot of life expectancy versus GDP per capita reveals two important statistical features.\n\nNonlinearity\nIncreases in GDP have a much larger effect on life expectancy at low income levels than at high income levels\n\n\nHeteroscedasticity\nCountries with low GDP exhibit much greater variability in life expectancy\n\n\nTake logarithm to treat this \\[\n\\Ex(\\text{Life}  \\mid \\text{GDP} = x) \\approx \\alpha + \\beta \\log(x)\n\\]\n\n\\(\\beta\\) measures years of life gained per multiplicative increase in GDP\nThe log scale naturally reflects diminishing returns\n\n â¬‡ Health vs.Â Wealth ðŸ““ \n\n\n\n\n\n\n\n\n\n\nPopulation is all hypothetical countries\nModel\n\nLinear regression\nIncorporates transformation of the explanatory variable\n\nUncertainty\n\n\\(R^2\\) measures the proportion of variance in \\(Y\\) captured by the model\n\nInterpretation\n\nThis describes relationship, not cause and effect\nThe variation in the residuals point to other explanatory factors that have been left out of the model"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/statistics/intro/intro-stats-examples.html#trapezoidal",
    "href": "classlib/classlib/quarto/slides/shared/statistics/intro/intro-stats-examples.html#trapezoidal",
    "title": "Statistics",
    "section": "Numerical integration",
    "text": "Numerical integration\nComputational mathematics seems distinct from statistics, but there is an overlap.\nSuppose that \\(f\\) is a random function defined in \\([0,1]\\), in particular, a Gaussian process, i.e., \\(f \\sim \\GP(0,K)\\). This means that for any distinct \\(x_1, \\ldots, x_n \\in [0,1]\\), the random vector of function values, \\(\\vf := \\bigl(f(x_1), \\ldots, f(x_n) \\bigr)^\\top\\), has a multivariate Gaussian distribution with\n\nMean \\(\\vmu = \\Ex(\\vf) = \\vzero\\)\nCovariance matrix \\(\\mSigma := \\Ex(\\vf \\vf^\\top) = \\bigl(\\Ex[f(x_i)f(x_j)]\\bigr)_{i,j=1}^n  = \\bigl(K(x_i,x_j)\\bigr)_{i,j=1}^n =: \\mK\\)\n\n\nIf this is the Bayesian prior belief about \\(f\\), then the Bayesian posterior mean of the function conditioned on the data \\(\\vf = \\vy\\) is \\[\n\\Ex\\bigl[f(x) \\mid \\vf  = \\vy\\bigr] = \\vy^\\top \\mK^{-1} \\vk(x) , \\quad\n\\text{where } \\vk(x) = \\bigl( K(x,x_i) \\bigr)_{i=1}^n\n\\]\nThe Bayesian posterior mean integral of \\(f\\) conditioned on the data is\n\\[\n\\Ex\\biggl[\\int_0^1 f(x) \\, \\dif x \\mid \\vf  = \\vy\\biggr] = \\vy^\\top \\mK^{-1} \\int_0^1 \\vk(x) \\, \\dif x\n\\]\nFor \\(K(t,x) = 2 - \\lvert t - x\\rvert\\) this is the trapezoidal rule.  â¬‡ Bayesian Quadrature ðŸ““ \n\n\n\nPopulation all (continuous) functions\nModel\n\nStochastic processes\nBayesian inference\nOften one would estimate the parameters in \\(K\\) from data\n\nUncertainty\n\nYou may construct Bayesian credible intervals, but they depend on your confidence in choosing a reasonable \\(K\\)\n\nInterpretation\n\nProvides a statistical interpretation of computational mathematics"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/statistics/estimation/estimators.html",
    "href": "classlib/classlib/quarto/slides/shared/statistics/estimation/estimators.html",
    "title": "Estimators/Estimates",
    "section": "",
    "text": "Estimator: a random variable (or function of the sample) used to approximate an unknown parameter Estimate: the realized numerical value of an estimator after observing the data\n\nSummary statistics\nMaximum likelihood estimators (MLE)\nPlug-in estimators\n\n\n\nGiven IID data, \\(X_1, \\ldots, X_n\\), we often compute\n\nEmpirical Distribution \\(F_{\\{X_i\\}}(x) := \\frac 1n \\sum_{i=1}^n \\indic(X_i \\le x)\\)\nSample Mean \\(\\displaystyle \\barX_n := \\frac{1}{n} \\sum_{i=1}^n X_i = \\int x \\, \\dif F_{\\{X_i\\}}(x)\\) to approximate the population mean \\(\\mu := \\Ex[X_1]\\)\nSample Variance \\(S^2 := \\displaystyle \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\barX_n)^2\\) to approximate the population variance \\(\\sigma^2 := \\var(X_1) := \\Ex[(X_1-\\mu)^2]\\)\n\nSometimes \\(\\hsigma^2 := \\displaystyle \\frac{1}{\\class{alert}{n}} \\sum_{i=1}^n (X_i - \\barX_n)^2\\)\n\n\n\n\nOrder Statistics \\(X_{(1)}, X_{(2)}, \\ldots\\), reorder the data so that \\[ X_{(1)} \\le X_{(2)} \\le \\cdots \\le X_{(n)}, \\qquad \\text{i.e., } X_{(i)} = Q_{\\{X_i\\}}(i/n)\n\\] where \\(Q_{\\{X_i\\}}\\) is the quantile function corresponding to the empirical distribution\n\n\\(X_{(1)}\\) is the minimum and \\(X_{(n)}\\) is the maximum of the data\n\\(X_{(i)}\\) is often an estimator of the population quantile \\(Q_X(p)\\) for \\(p \\approx i/(n+1)\\) or \\((i-1/2)/n\\)\n\n\nGiven IID data, \\((X_1, Y_1), \\ldots, (X_n,Y_n)\\), with sample mean \\((\\barX_n, \\barY_n)\\), we often compute\n\nSample Covariance \\(\\displaystyle  S_{XY} := \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\barX_n)(Y_i - \\barY_n)\\) to approximate the population covariance \\(\\cov(X_1,Y_1) := \\Ex[(X_1 - \\mu_X)(Y_1 - \\mu_Y)]\\)\nSample Correlation \\(\\displaystyle  R_{XY} := \\frac{S_{XY}}{\\sqrt{S^2_X S^2_Y}}\\) to approximate the population correlation \\(\\displaystyle \\corr(X_1,Y_1) := \\frac{\\cov(X_1,Y_1)}{\\sigma_X \\sigma_Y}\\)\n\n\n\n\nThe joint density of data, \\(\\vX = (X_1, \\ldots, X_n)^\\top\\) given a parameter, \\(\\vtheta\\), is \\(\\varrho_{X_1, \\ldots, X_n| \\vtheta}\\). The likelihood, \\(L\\) turns that around to make the parameter the variable, so \\[\\begin{align*}\nL(\\vtheta | x_1, \\ldots, x_n) & := \\varrho_{X_1, \\ldots, X_n | \\vtheta}(x_1, \\ldots, x_n) \\\\\n& = \\prod_{i=1}^n \\varrho_{X_1 |\\vtheta}(x_i) \\qquad \\text{if } X_1, \\ldots, X_n \\text{ are } \\IID\n\\end{align*}\\]\nThe maximum likelihood estimator (MLE) of \\(\\vtheta\\) is the one that fits the observed data best in terms of \\[\n\\vTheta_{\\MLE}  = \\Argmax{\\vtheta} L(\\vtheta | X_1, \\ldots, X_n)\n\\]\n\\(\\exstar\\) What is the MLE of \\(p\\) for the distribution \\(\\Bern(p)\\)?\n\n\\(\\exstar\\)What is the MLE of \\(\\lambda\\) for \\(\\Exp(\\lambda)\\)? What are the MLE of \\(\\mu=\\Ex(X)\\) and \\(\\sigma^2=\\var(X)\\) for \\(X\\sim\\Exp(\\lambda)\\)?\n\n\\(\\exstar\\) What are the MLE of \\(\\mu\\) and \\(\\sigma\\) for \\(X \\sim \\Norm(\\mu,\\sigma^2)\\)?\n\n\n\n\nIf \\(\\hTheta_1\\) is an estimator of \\(\\theta_1\\) and \\(\\theta_2 = g(\\theta_1)\\), then \\(\\hTheta_2 : = g(\\hTheta_1)\\) is a plug-in estimator of \\(\\theta_2\\)\nIf \\(\\hTheta_1\\) is MLE of \\(\\theta_1\\) and \\(\\theta_2 = g(\\theta_1)\\), then \\(\\hTheta_2 : = g(\\hTheta_1)\\) is an MLE of \\(\\theta_2\\)"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/statistics/estimation/estimators.html#summary-statistics",
    "href": "classlib/classlib/quarto/slides/shared/statistics/estimation/estimators.html#summary-statistics",
    "title": "Estimators/Estimates",
    "section": "",
    "text": "Given IID data, \\(X_1, \\ldots, X_n\\), we often compute\n\nEmpirical Distribution \\(F_{\\{X_i\\}}(x) := \\frac 1n \\sum_{i=1}^n \\indic(X_i \\le x)\\)\nSample Mean \\(\\displaystyle \\barX_n := \\frac{1}{n} \\sum_{i=1}^n X_i = \\int x \\, \\dif F_{\\{X_i\\}}(x)\\) to approximate the population mean \\(\\mu := \\Ex[X_1]\\)\nSample Variance \\(S^2 := \\displaystyle \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\barX_n)^2\\) to approximate the population variance \\(\\sigma^2 := \\var(X_1) := \\Ex[(X_1-\\mu)^2]\\)\n\nSometimes \\(\\hsigma^2 := \\displaystyle \\frac{1}{\\class{alert}{n}} \\sum_{i=1}^n (X_i - \\barX_n)^2\\)\n\n\n\n\nOrder Statistics \\(X_{(1)}, X_{(2)}, \\ldots\\), reorder the data so that \\[ X_{(1)} \\le X_{(2)} \\le \\cdots \\le X_{(n)}, \\qquad \\text{i.e., } X_{(i)} = Q_{\\{X_i\\}}(i/n)\n\\] where \\(Q_{\\{X_i\\}}\\) is the quantile function corresponding to the empirical distribution\n\n\\(X_{(1)}\\) is the minimum and \\(X_{(n)}\\) is the maximum of the data\n\\(X_{(i)}\\) is often an estimator of the population quantile \\(Q_X(p)\\) for \\(p \\approx i/(n+1)\\) or \\((i-1/2)/n\\)\n\n\nGiven IID data, \\((X_1, Y_1), \\ldots, (X_n,Y_n)\\), with sample mean \\((\\barX_n, \\barY_n)\\), we often compute\n\nSample Covariance \\(\\displaystyle  S_{XY} := \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\barX_n)(Y_i - \\barY_n)\\) to approximate the population covariance \\(\\cov(X_1,Y_1) := \\Ex[(X_1 - \\mu_X)(Y_1 - \\mu_Y)]\\)\nSample Correlation \\(\\displaystyle  R_{XY} := \\frac{S_{XY}}{\\sqrt{S^2_X S^2_Y}}\\) to approximate the population correlation \\(\\displaystyle \\corr(X_1,Y_1) := \\frac{\\cov(X_1,Y_1)}{\\sigma_X \\sigma_Y}\\)"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/statistics/estimation/estimators.html#maximum-likelihood-estimators",
    "href": "classlib/classlib/quarto/slides/shared/statistics/estimation/estimators.html#maximum-likelihood-estimators",
    "title": "Estimators/Estimates",
    "section": "",
    "text": "The joint density of data, \\(\\vX = (X_1, \\ldots, X_n)^\\top\\) given a parameter, \\(\\vtheta\\), is \\(\\varrho_{X_1, \\ldots, X_n| \\vtheta}\\). The likelihood, \\(L\\) turns that around to make the parameter the variable, so \\[\\begin{align*}\nL(\\vtheta | x_1, \\ldots, x_n) & := \\varrho_{X_1, \\ldots, X_n | \\vtheta}(x_1, \\ldots, x_n) \\\\\n& = \\prod_{i=1}^n \\varrho_{X_1 |\\vtheta}(x_i) \\qquad \\text{if } X_1, \\ldots, X_n \\text{ are } \\IID\n\\end{align*}\\]\nThe maximum likelihood estimator (MLE) of \\(\\vtheta\\) is the one that fits the observed data best in terms of \\[\n\\vTheta_{\\MLE}  = \\Argmax{\\vtheta} L(\\vtheta | X_1, \\ldots, X_n)\n\\]\n\\(\\exstar\\) What is the MLE of \\(p\\) for the distribution \\(\\Bern(p)\\)?\n\n\\(\\exstar\\)What is the MLE of \\(\\lambda\\) for \\(\\Exp(\\lambda)\\)? What are the MLE of \\(\\mu=\\Ex(X)\\) and \\(\\sigma^2=\\var(X)\\) for \\(X\\sim\\Exp(\\lambda)\\)?\n\n\\(\\exstar\\) What are the MLE of \\(\\mu\\) and \\(\\sigma\\) for \\(X \\sim \\Norm(\\mu,\\sigma^2)\\)?"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/statistics/estimation/estimators.html#plug-in-estimators",
    "href": "classlib/classlib/quarto/slides/shared/statistics/estimation/estimators.html#plug-in-estimators",
    "title": "Estimators/Estimates",
    "section": "",
    "text": "If \\(\\hTheta_1\\) is an estimator of \\(\\theta_1\\) and \\(\\theta_2 = g(\\theta_1)\\), then \\(\\hTheta_2 : = g(\\hTheta_1)\\) is a plug-in estimator of \\(\\theta_2\\)\nIf \\(\\hTheta_1\\) is MLE of \\(\\theta_1\\) and \\(\\theta_2 = g(\\theta_1)\\), then \\(\\hTheta_2 : = g(\\hTheta_1)\\) is an MLE of \\(\\theta_2\\)"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/statistics/estimation/estimators.html#biasvar",
    "href": "classlib/classlib/quarto/slides/shared/statistics/estimation/estimators.html#biasvar",
    "title": "Estimators/Estimates",
    "section": "Bias, variance, and mean squared error of estimators",
    "text": "Bias, variance, and mean squared error of estimators\nSuppose that \\(\\Theta\\) is an estimator of a parameter, \\(\\theta\\), of a population\n\nBias \\(\\bias(\\Theta) = \\Ex(\\Theta) - \\theta\\)\n\nAsymptotic bias is \\(\\displaystyle \\lim_{n \\to \\infty} \\bias(\\Theta_n)\\), where \\(n\\) is the size of the sample on which the estimator is based\nAn estimator is unbiased if \\(\\bias(\\Theta) = 0\\)\n\\(\\barX_n\\) is an unbiased estimator of \\(\\mu = \\Ex(X_1)\\) for identically distributed data\n\nVariance we already know this definition\n\n\\(\\var(\\barX_n) \\exeq \\var(X_1)/n\\) for uncorrelated, identically distributed data\n\nMean squared error (MSE) \\(\\mse(\\Theta) := \\Ex[(\\Theta - \\theta)^2] \\exeq [\\bias(\\Theta)]^2 + \\var(\\Theta)\\)\n\n\n\\(\\exstar\\) Show that \\(S^2 := \\displaystyle \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\barX_n)^2\\) is an unbiased estimator of \\(\\sigma^2\\)\n\\(\\exstar\\) Show that \\(S = \\sqrt{S^2}\\) as an estimator of \\(\\sigma\\) has negative bias (see Jensenâ€™s inequality)\n\\(\\exstar\\) Is the MLE of \\(\\sigma=\\std(X)\\) for \\(X\\sim\\Exp(\\lambda)\\) unbiased?\n\\(\\exstar\\) What is the MLE \\(\\theta\\) of \\(\\theta\\) for \\(X \\sim \\Unif(0,\\theta)\\)? Is it unbiased? Can you modify it to be unbiased?"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/statistics/estimation/estimators.html#distribest",
    "href": "classlib/classlib/quarto/slides/shared/statistics/estimation/estimators.html#distribest",
    "title": "Estimators/Estimates",
    "section": "Distributions of estimators (see Important Distributions)",
    "text": "Distributions of estimators (see Important Distributions)\nFor the sample mean \\(\\barX_n\\), based on IID data\n\n\\(n \\barX_n \\sim \\Bin(n,p)\\) if \\(X \\sim \\Bern(p)\\)\n\\(\\barX_n \\exsim \\Gam(n, n \\lambda)\\) if \\(X \\sim \\Exp(\\lambda)\\) where \\(\\displaystyle \\varrho_{\\Gam(\\alpha, \\beta)}(x) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\, x^{\\alpha-1} \\exp(-\\beta x)\n\\quad x&gt;0\\)\n\nNote: \\(\\Gamma(n) = (n-1)!\\) for integer \\(n\\)\n\\(\\Gam(\\alpha, \\beta)\\) is the generic gamma distribution with shape \\(\\alpha\\) and rate \\(\\beta\\)\n\\(\\lambda n \\barX_n \\exsim \\Gam(n, 1)\\)\n\\(2 \\lambda n \\barX_n \\exsim \\chi^2_{2n}\\), where \\(\\displaystyle \\varrho_{\\chi^2_\\nu}(x) = \\frac{x^{\\nu/2 - 1} \\exp(-x/2)}{2^{\\nu/2} \\Gamma(\\nu/2)} \\quad x &gt; 0\\)\n\n\n\nFor the sample mean \\(\\barX_n\\), based on IID data (contâ€™d)\n\n\\(\\barX_n \\exsim \\Norm(\\mu,\\sigma^2/n)\\) if \\(X \\sim \\Norm(\\mu,\\sigma^2)\\)\n\\(\\barX_n \\appxsim \\Norm(\\mu,\\sigma^2/n)\\) for arbitrary distributions and large \\(n\\) by the Central Limit Theorem\n\\(\\displaystyle \\frac{\\barX_n - \\mu}{S/\\sqrt{n}} \\sim t_{n-1}\\) if \\(X \\sim \\Norm(\\mu,\\sigma^2)\\) where\n\n\\(\\displaystyle S^2 := \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\barX_n)^2\\)\n\\(t_\\nu\\) is the Studentâ€™s t distribution with \\(\\nu\\) degrees of freedom\n\n\\(\\displaystyle \\varrho_{t_\\nu}(x) = \\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\nu\\pi}\\,\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)}\\left(1+\\frac{x^2}{\\nu}\\right)^{-(\\nu+1)/2}, \\quad \\infty &lt; x &lt; \\infty\\)\nSymmetric about \\(0\\)\nHeavier tails than the standard normal\n\\(\\exstar\\) Converges to \\(N(0,1)\\) as \\(\\nu \\to \\infty\\)\n\n\n\n\nFor the unbiased sample variance, \\(S^2\\), for \\(\\Norm(\\mu,\\sigma^2)\\) based on IID data\n\n\\(\\displaystyle \\frac{(n-1) S^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\)\n\nÂ \nFor order statistics, \\(X_{(k)}\\), \\(\\displaystyle F_{X_{(k)}}(x) = \\sum_{j=k}^n \\binom{n}{j} [F_X(x)]^j [1 - F_X(x)]^{n-j}\\) for IID data from CDF \\(F_X\\)\nÂ \nÂ \n\\(\\exstar\\) Is the MLE \\(\\theta\\) of \\(\\theta\\) for \\(X \\sim \\Unif(0,\\theta)\\) unbiased? Can you modify it to be unbiased?"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/probability/probability-review-distrib-part-2.html",
    "href": "classlib/classlib/quarto/slides/shared/probability/probability-review-distrib-part-2.html",
    "title": "MATH 563 â€” Spring 2026",
    "section": "",
    "text": "Discrete random variables have a PMF\nContinuous random variables have a PDF\nSome random variables are mixed\n\nZero-inflated exponential distribution â€” \\(X\\) is the waiting time for a taxi\n\nA taxi waiting for you with probability \\(p\\)\nBut if there is none, then the waiting time is exponential with mean \\(1/\\lambda\\) \\[\\begin{align*}\nF(x) &= \\begin{cases}\n0, &-\\infty &lt; x &lt; 0 \\\\\n1- (1-p)\\exp(-\\lambda x),  & 0 \\le x &lt; \\infty\n\\end{cases}\n\\end{align*}\\] Note: \\(F\\) has a jump of size \\(p\\) at \\(x=0\\), corresponding to \\(\\Prob(X=0)=p\\)\n\n\n\\[\\begin{align*}\n\\mu & = \\Ex(X) = \\int_{-\\infty}^{\\infty} x \\, \\dif F(x) \\qquad   \n     \\mathlink{https://en.wikipedia.org/wiki/Lebesgue-Stieltjes_integration}{(Lebesgueâ€“Stieltjes integral)}\\\\\n& = \\underbrace{0 \\times p}_{\\text{from the jump at } 0} + \\int_0^{\\infty} x \\times (1-p)\\exp(-\\lambda x) \\, \\dif x = (1-p)/ \\lambda\n\\\\\n\\sigma^2 & = \\var(X) = \\Ex(X^2) - \\mu^2 \\exeq \\frac{1-p^2}{\\lambda^2}\n\\end{align*}\\]"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/probability/probability-review-distrib-part-2.html#a-mixed-partially-discrete-partially-continuous-random-variable",
    "href": "classlib/classlib/quarto/slides/shared/probability/probability-review-distrib-part-2.html#a-mixed-partially-discrete-partially-continuous-random-variable",
    "title": "MATH 563 â€” Spring 2026",
    "section": "",
    "text": "Discrete random variables have a PMF\nContinuous random variables have a PDF\nSome random variables are mixed\n\nZero-inflated exponential distribution â€” \\(X\\) is the waiting time for a taxi\n\nA taxi waiting for you with probability \\(p\\)\nBut if there is none, then the waiting time is exponential with mean \\(1/\\lambda\\) \\[\\begin{align*}\nF(x) &= \\begin{cases}\n0, &-\\infty &lt; x &lt; 0 \\\\\n1- (1-p)\\exp(-\\lambda x),  & 0 \\le x &lt; \\infty\n\\end{cases}\n\\end{align*}\\] Note: \\(F\\) has a jump of size \\(p\\) at \\(x=0\\), corresponding to \\(\\Prob(X=0)=p\\)\n\n\n\\[\\begin{align*}\n\\mu & = \\Ex(X) = \\int_{-\\infty}^{\\infty} x \\, \\dif F(x) \\qquad   \n     \\mathlink{https://en.wikipedia.org/wiki/Lebesgue-Stieltjes_integration}{(Lebesgueâ€“Stieltjes integral)}\\\\\n& = \\underbrace{0 \\times p}_{\\text{from the jump at } 0} + \\int_0^{\\infty} x \\times (1-p)\\exp(-\\lambda x) \\, \\dif x = (1-p)/ \\lambda\n\\\\\n\\sigma^2 & = \\var(X) = \\Ex(X^2) - \\mu^2 \\exeq \\frac{1-p^2}{\\lambda^2}\n\\end{align*}\\]"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/probability/probability-review-distrib-part-2.html#lebesguestieltjes-integral-integral-jumps",
    "href": "classlib/classlib/quarto/slides/shared/probability/probability-review-distrib-part-2.html#lebesguestieltjes-integral-integral-jumps",
    "title": "MATH 563 â€” Spring 2026",
    "section": "Lebesgueâ€“Stieltjes integral = integral + jumps",
    "text": "Lebesgueâ€“Stieltjes integral = integral + jumps\nLet the CDF \\(F\\)\n\nBe a nondecreasing, right-continuous function on \\(\\mathbb{R}\\)\nHave (possible) discontinuities at \\(-\\infty &lt;  x_1 &lt; \\ldots &lt; x_N &lt; \\infty\\)\nBe differentiable between its jump points, with \\(\\varrho(x)=F'(x)\\) there\n\nand let \\(f\\) be a suitable function\nThe Lebesgueâ€“Stieltjes integral of \\(f\\) with respect to \\(F\\) can be written as \\[\\begin{align*}\n  \\int f(x)\\, dF(x) & = \\int_{-\\infty}^{x_{1}} f(x) \\,\\varrho(x) \\, \\dif x + \\sum_{k=1}^{N-1} \\int_{x_k}^{x_{k+1}} f(x) \\, \\varrho(x) \\, \\dif x + \\int_{x_N}^{\\infty} f(x) \\, \\varrho(x) \\, \\dif x \\\\\n  & \\qquad \\qquad + \\sum_{k=1}^N f(x_k)[F(x_k) - F(x_k^{-})]\n  \\end{align*}\\]\nIntegrate \\(f\\) where \\(F\\) is smooth, and add a weighted sum of \\(f\\) at the jump points of \\(F\\)"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/probability/converge-clt.html",
    "href": "classlib/classlib/quarto/slides/shared/probability/converge-clt.html",
    "title": "Types of convergence{#type-conv}",
    "section": "",
    "text": "Let \\(X_1, X_2, \\ldots\\) be random variables and \\(X\\) another random variable"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/probability/converge-clt.html#clt",
    "href": "classlib/classlib/quarto/slides/shared/probability/converge-clt.html#clt",
    "title": "Types of convergence{#type-conv}",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nIf \\(X_1, X_2, \\ldots \\IIDsim\\) some distribution with finite moment generating function, \\(M(t) := \\Ex\\bigl(\\exp(tX_1)\\bigr)\\), that exists for \\(t\\) near \\(0\\), and \\(\\mu = \\Ex(X_1)\\) and \\(\\sigma^2 = \\var(X_1)\\), and \\[\n\\barX_n := \\frac 1n \\left(X_1 + \\cdots + X_n \\right),\n\\] then \\[\n\\frac{\\barX_n - \\mu}{\\sigma /\\sqrt{n}} \\convd \\Norm(0,1)\n\\] Note that IID means independent and identically distributed"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/probability/converge-clt.html#proof-of-the-central-limit-theorem",
    "href": "classlib/classlib/quarto/slides/shared/probability/converge-clt.html#proof-of-the-central-limit-theorem",
    "title": "Types of convergence{#type-conv}",
    "section": "Proof of the Central Limit Theorem",
    "text": "Proof of the Central Limit Theorem\n\nDefine a standardized (mean \\(0\\), variance \\(1\\)) random variable, \\(Y_i := (X_i - \\mu)/\\sigma\\) and note that \\(Y_1, Y_2, \\dots\\) are IID with mean \\(0\\) and variance \\(1\\):\n\n\\[\\begin{align*}\nZ_n: &= \\frac{\\barX_n - \\mu}{\\sigma /\\sqrt{n}} = \\frac{Y_1 + \\cdots + Y_n}{\\sqrt{n}} \\\\\nM_{Z_n}(t) &= \\Ex[\\exp(tZ_n)] = \\Ex[\\exp(tY_1/\\sqrt{n} + \\cdots + tY_n/\\sqrt{n})]\\\\\n& = \\Ex[\\exp(tY_1/\\sqrt{n}) \\cdots \\exp(tY_n/\\sqrt{n})] \\\\\n& =  \\{\\Ex[\\exp(tY_1/\\sqrt{n})]\\}^n \\qquad \\text{by independence} \\\\\n& = [M_Y(t/\\sqrt{n})]^n \\\\\n\\lim_{n \\to \\infty}M_{Z_n}(t) &= [M_Y(0)]^\\infty = 1^\\infty \\quad \\class{alert}{\\text{(undetermined)}}\n\\end{align*}\\]\n\n\nTake the limit of \\(M_{Z_n}(t)\\) as \\(n \\to \\infty\\) using Lâ€™HÃ´pitalâ€™s rule \\[\\begin{align*}\n\\text{Note that } M_Y(0) & = 1 \\\\\nM'_Y(0) & = \\Ex(Y_1) = 0 \\\\\nM''_Y(0) & = \\Ex(Y_1^2) = 1 \\\\\n\\text{Then }\\lim_{n \\to \\infty} \\log\\bigl(M_{Z_n}(t)  \\bigr) & = \\lim_{n \\to \\infty} n  \\log \\bigl(M_Y(t/\\sqrt{n}) \\bigr)\n=  \\lim_{n \\to \\infty} \\frac{\\log \\bigl(M_Y(t/\\sqrt{n}) \\bigr) }{n^{-1}} = \\frac{0}{0} \\quad \\class{alert}{\\text{(undetermined)}}\\\\\n& = \\lim_{n \\to \\infty}\n\\frac{ \\dfrac{-t}{2n^{3/2}} \\dfrac{M'_Y(t/\\sqrt{n})}{M_Y(t/\\sqrt{n})} }\n  { -n^{-2} }\n=   \\frac t2  \\lim_{n \\to \\infty}  \\frac{\\dfrac{M'_Y(t/\\sqrt{n})}{M_Y(t/\\sqrt{n})}}{ n^{-1/2} }\n= \\frac{0}{0} \\quad \\class{alert}{\\text{(undetermined)}} \\\\\n& =  \\frac t2  \\lim_{n \\to \\infty} \\frac{ \\dfrac{-t}{2 n^{3/2}} \\dfrac{M_Y(t/\\sqrt{n}) M''_Y(t/\\sqrt{n}) - [M'_Y(t/\\sqrt{n})]^2 }{ [M_Y(t/\\sqrt{n})]^2}} { -n^{-3/2}/2 } = \\frac{t^2}{2} \\\\\n\\lim_{n \\to \\infty} M_{Z_n}(t)  & = \\exp(t^2/2)\n\\end{align*}\\]\n\n\n\nNote that if \\(Z \\sim \\Norm(0,1)\\) then \\[\\begin{align*}\nM_Z(t) & = \\Ex[\\exp(tZ)] = \\int_{-\\infty}^{\\infty} \\frac 1{\\sqrt{2 \\pi}} \\exp(tz - z^2/2) \\, \\dif z \\\\\n& = \\int_{-\\infty}^{\\infty} \\frac 1{\\sqrt{2 \\pi}} \\exp\\bigl(-(z - t)^2/2 \\bigr) \\, \\exp(t^2/2) \\, \\dif z = \\exp(t^2/2) \\\\\n&= \\lim_{n \\to \\infty} M_{Z_n}(t)\n\\end{align*}\\]\n\nBy uniqueness of moment generating functions, \\[\nZ_n \\convd \\Norm(0,1)\n\\] \\(\\square\\)"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/probability/converge-clt.html#chebyshev-markov-inequality",
    "href": "classlib/classlib/quarto/slides/shared/probability/converge-clt.html#chebyshev-markov-inequality",
    "title": "Types of convergence{#type-conv}",
    "section": "Chebyshev (Markov) Inequality",
    "text": "Chebyshev (Markov) Inequality\n\\[\n\\Prob\\bigl( f(X) \\ge r \\bigr) \\le \\frac{\\Ex[f(X)]}{r} \\qquad f \\text{ is }\\class{alert}{\\text{non-negative}}\n\\]\n\nProof\n\\[\\begin{align*}\n\\Prob(f(X)\\ge r)\n&= \\Ex\\bigl[\\indic\\bigl(f(X)\\ge r\\bigr)\\bigr] \\\\\n&= \\Ex\\Bigl[\\Ex\\bigl[\\indic\\bigl(f(X)\\ge r\\bigr)\\mid X\\bigr]\\Bigr]\n\\qquad \\text{(total expectation)} \\\\\n&\\le \\Ex\\Bigl[\\Ex\\bigl[f(X)/r \\mid X\\bigr]\\Bigr]\n\\qquad \\text{since } \\indic\\bigl(f(X)\\ge r\\bigr)\\le f(X)/r \\text{ a.s.} \\\\\n&= \\Ex\\bigl[f(X)/r\\bigr] \\\\\n&= \\Ex[f(X)]/r\n\\end{align*}\\]\n\n\nImportant special cases\n\\[\\begin{gather*}\n\\Prob\\bigl( \\lvert X - \\mu \\rvert^2 \\ge r^2 \\bigr) \\le \\frac{\\sigma^2}{r^2} \\quad \\text{where } \\mu = \\Ex(X), \\ \\sigma^2 = \\var(X) \\\\\n\\Prob\\bigl( \\lvert X - \\med(X) \\rvert \\ge r \\bigr) \\le \\frac{\\Ex (\\lvert X - \\med(X)\\rvert)}{r}\n\\end{gather*}\\]"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/probability/converge-clt.html#normal-tails-vs-chebyshev-bounds",
    "href": "classlib/classlib/quarto/slides/shared/probability/converge-clt.html#normal-tails-vs-chebyshev-bounds",
    "title": "Types of convergence{#type-conv}",
    "section": "Normal tails vs Chebyshev bounds",
    "text": "Normal tails vs Chebyshev bounds\nLet \\(X\\sim \\Norm(0,1)\\) and compare \\(\\Prob(|X|&gt;r)\\) with two bounds\nHere \\(\\Ex\\lvert X \\rvert \\exeq \\sqrt{2/\\pi} \\approx 0.798\\) and \\(\\Ex(X^2)=1\\)\nimport math\n\nrs = [1, 2, 6, 10, 100]\nE_absX = math.sqrt(2 / math.pi)\n\ndef tail_abs_normal(r):\n    return math.erfc(r / math.sqrt(2))\n\ndef fmt_prob(x):\n    return f\"{x:.6g}\"\n\ndef fmt_over(x):\n    if x == math.inf:\n        return \"(âˆž)\"\n    return f\"({x:.2g}Ã—)\"\n\nprint(\n    \"| $r$ | $\\\\Prob(|X|&gt;r)$ (exact) | \"\n    \"$\\\\le \\sqrt{2/\\pi}\\,/r$ (Markov) | \"\n    \"$\\\\le 1/r^2$ (Chebyshev) |\"\n)\nprint(\"|-:|---:|-----:|-----:|\")\n\nfor r in rs:\n    exact = tail_abs_normal(r)\n    markov = E_absX / r\n    cheb = 1.0 / (r*r)\n\n    if exact == 0.0:\n        m_over = math.inf\n        c_over = math.inf\n    else:\n        m_over = markov / exact\n        c_over = cheb / exact\n\n    print(\n        f\"| {r} | {fmt_prob(exact)} | \"\n        f\"{fmt_prob(markov)} {fmt_over(m_over)} | \"\n        f\"{fmt_prob(cheb)} {fmt_over(c_over)} |\"\n    )\n\n\n\n\n\n\n\n\n\n\\(r\\)\n\\(\\Prob(|X|&gt;r)\\) (exact)\n\\(\\le \\sqrt{2/\\pi}\\,/r\\) (Markov)\n\\(\\le 1/r^2\\) (Chebyshev)\n\n\n\n\n1\n0.317311\n0.797885 (2.5Ã—)\n1 (3.2Ã—)\n\n\n2\n0.0455003\n0.398942 (8.8Ã—)\n0.25 (5.5Ã—)\n\n\n6\n1.97318e-09\n0.132981 (6.7e+07Ã—)\n0.0277778 (1.4e+07Ã—)\n\n\n10\n1.52397e-23\n0.0797885 (5.2e+21Ã—)\n0.01 (6.6e+20Ã—)\n\n\n100\n0\n0.00797885 (âˆž)\n0.0001 (âˆž)\n\n\n\n\nThe Markov and Chebyshev bounds are much looser, but more general\nCLT bounds for means will resemble the exact; we are willing to live with approximate bounds that are tighter than absolute bounds\nThese guaranteed Markov and Chebyshev bounds still depend on knowing or approximating \\(\\sigma^2\\) or \\(\\Ex \\lvert X - \\med(X)\\rvert\\)"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/probability/converge-clt.html#student-t-tails-vs-markovchebyshev-bounds",
    "href": "classlib/classlib/quarto/slides/shared/probability/converge-clt.html#student-t-tails-vs-markovchebyshev-bounds",
    "title": "Types of convergence{#type-conv}",
    "section": "Student \\(t\\) tails vs Markov/Chebyshev bounds",
    "text": "Student \\(t\\) tails vs Markov/Chebyshev bounds\nLet \\(T_\\nu \\sim t_\\nu\\) and compare \\(\\Prob(|T_\\nu|&gt;r)\\) with two bounds\nimport mpmath as mp\n\nmp.mp.dps = 50\n\ndfs = [2, 3, 5]\nrs  = [1, 2, 6]\n\ndef t_pdf(x, nu):\n    c = mp.gamma((nu+1)/2) / (mp.sqrt(nu*mp.pi) * mp.gamma(nu/2))\n    return c * (1 + (x*x)/nu) ** (-(nu+1)/2)\n\ndef tail_abs_t_exact(r, nu):\n    x = nu / (nu + r*r)\n    return mp.betainc(nu/2, mp.mpf('0.5'), 0, x, regularized=True)\n\ndef E_abs_t(nu):\n    if nu &lt;= 1:\n        return mp.inf\n    f = lambda x: x * t_pdf(x, nu)\n    return 2 * mp.quad(f, [0, mp.inf])\n\ndef E_t2(nu):\n    if nu &lt;= 2:\n        return mp.inf\n    return nu / (nu - 2)\n\ndef fmt_prob(x):\n    if x == mp.inf:\n        return \"inf\"\n    return f\"{float(x):.3g}\"\n\ndef fmt_over(x):\n    if x == mp.inf:\n        return \"(âˆž)\"\n    return f\"({float(x):.0f}Ã—)\"\n\nprint(\"| $\\\\nu$ | $r$ | $\\\\Prob(|T_\\\\nu|&gt;r)$ (exact) | $\\\\le \\\\Ex(|T_\\\\nu|)/r$ (Markov) | $\\\\le \\\\Ex(T_\\\\nu^2)/r^2$ (Chebyshev) |\")\nprint(\"|-:|-:|---:|---:|----:|\")\n\nfor nu in dfs:\n    Eabs = E_abs_t(nu)\n    Et2  = E_t2(nu)\n    for r in rs:\n        exact = tail_abs_t_exact(r, nu)\n\n        markov = (Eabs / r) if mp.isfinite(Eabs) else mp.inf\n        cheb   = (Et2  / (r*r)) if mp.isfinite(Et2) else mp.inf\n\n        m_over = markov / exact if mp.isfinite(markov) else mp.inf\n        c_over = cheb   / exact if mp.isfinite(cheb)   else mp.inf\n\n        print(\n            f\"| {nu} | {r} | {fmt_prob(exact)} | \"\n            f\"{fmt_prob(markov)} {fmt_over(m_over)} | \"\n            f\"{fmt_prob(cheb)} {fmt_over(c_over)} |\"\n        )\n\n\n\n\n\n\n\n\n\n\n\\(\\nu\\)\n\\(r\\)\n\\(\\Prob(|T_\\nu|&gt;r)\\) (exact)\n\\(\\le \\Ex(|T_\\nu|)/r\\) (Markov)\n\\(\\le \\Ex(T_\\nu^2)/r^2\\) (Chebyshev)\n\n\n\n\n2\n1\n0.423\n1.41 (3Ã—)\ninf (âˆž)\n\n\n2\n2\n0.184\n0.707 (4Ã—)\ninf (âˆž)\n\n\n2\n6\n0.0267\n0.236 (9Ã—)\ninf (âˆž)\n\n\n3\n1\n0.391\n1.1 (3Ã—)\n3 (8Ã—)\n\n\n3\n2\n0.139\n0.551 (4Ã—)\n0.75 (5Ã—)\n\n\n3\n6\n0.00927\n0.184 (20Ã—)\n0.0833 (9Ã—)\n\n\n5\n1\n0.363\n0.949 (3Ã—)\n1.67 (5Ã—)\n\n\n5\n2\n0.102\n0.475 (5Ã—)\n0.417 (4Ã—)\n\n\n5\n6\n0.00185\n0.158 (86Ã—)\n0.0463 (25Ã—)"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/probability/converge-clt.html#weak-law-of-large-numbers-via-chebyshev",
    "href": "classlib/classlib/quarto/slides/shared/probability/converge-clt.html#weak-law-of-large-numbers-via-chebyshev",
    "title": "Types of convergence{#type-conv}",
    "section": "Weak Law of Large Numbers (via Chebyshev)",
    "text": "Weak Law of Large Numbers (via Chebyshev)\n\nTheorem (WLLN)\nLet \\(X_1,X_2,\\dots\\) be IID with \\(\\Ex[X_1]=\\mu\\) and \\(\\var(X_1)=\\sigma^2&lt;\\infty\\). Define the sample mean as \\(\\displaystyle\\barX_n := \\frac1n\\sum_{i=1}^n X_i\\). Then \\(\\barX_n \\convp \\mu\\).\n\n\nProof\nBy Chebyshevâ€™s inequality, \\(\\displaystyle \\Prob\\bigl(|\\barX_n-\\mu|\\ge \\varepsilon\\bigr)\n\\le \\frac{\\var(\\barX_n)}{\\varepsilon^2}\\)\nUsing independence, \\[\n\\var(\\barX_n)\n=\n\\var\\!\\left(\\frac1n\\sum_{i=1}^n X_i\\right)\n=\n\\frac{1}{n^2}\\sum_{i=1}^n \\var(X_i)\n=\n\\frac{1}{n^2}\\cdot n\\sigma^2\n=\n\\frac{\\sigma^2}{n}\n\\] Therefore, \\[\n\\Prob\\bigl(|\\barX_n-\\mu|\\ge \\varepsilon\\bigr)\n\\le\n\\frac{\\sigma^2}{n\\varepsilon^2}\n\\to 0 \\quad \\text{as } n \\to \\infty \\qquad \\square\n\\]"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/probability/converge-clt.html#slutskys-theorem",
    "href": "classlib/classlib/quarto/slides/shared/probability/converge-clt.html#slutskys-theorem",
    "title": "Types of convergence{#type-conv}",
    "section": "Slutskyâ€™s Theorem",
    "text": "Slutskyâ€™s Theorem\nIf \\[\\begin{align*}\nX_n &\\convd X  \\quad \\text{and} \\\\\nY_n &\\convp c \\quad \\text{for some constant } c\n\\end{align*}\\] Then \\[\\begin{align*}\nX_n + Y_n &\\convd X + c \\\\\nX_n Y_n &\\convd cX \\\\\n\\frac{X_n}{Y_n} &\\convd \\frac{X}{c} \\quad \\text{if } c \\neq 0\n\\end{align*}\\]\n\nRandom quantities converging to constants behave like constants asymptotically"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/general/how_to_learn.html",
    "href": "classlib/classlib/quarto/slides/shared/general/how_to_learn.html",
    "title": "How to Learn This Subject",
    "section": "",
    "text": "How to Learn This Subject\n\nMake sure you understand the slides\nAsk questions for clarification\nDo the assignments\nWork out problems in the text that are not assigned, or convince yourself that you could do them\nPractice on old tests\nDo the \\(\\exstar\\) exercises\nMake up questions and quiz your friends"
  },
  {
    "objectID": "pages/schedule.html",
    "href": "pages/schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Classes are held in RE 102\n\nJanuary\n\n\n\nDate\nTopic\nSlides\n\n\n\n\nTue Jan 13\nIntroductory Examples, Course Overview\nIntroduction and Probability Review\n\n\nThu Jan 15\nIntroductory Examples, Key Probability Concepts\n\n\n\nFri Jan 16\nDiagnostic Survey Due\n\n\n\nTue Jan 20\nKey Probability Concepts, Important Distributions, Conditional Probability\n\n\n\nThu Jan 22\nConditional Probability, Convergence, Central Limit Theorem\n\n\n\nTue Jan 27\nCentral Limit Theorem, Chebyshevâ€™s Inequality, Summary Statistics\nEstimation and Confidence Intervals\n\n\nThu Jan 29\nSummary Statistics, Maximum Likelihood Estimation, Bias, Mean Squared Error\n\n\n\n\n\n\nFebruary\n\n\n\n\n\n\n\n\nDate\nTopic\nSlides\n\n\n\n\nFri Jan 30\nAssignment 2 Due\n\n\n\nTue Feb 3\n\n\n\n\nThu Feb 5\n\n\n\n\nTue Feb 10\n\n\n\n\nThu Feb 12\n\n\n\n\nFri Feb 13\nAssignment 3 Due\n\n\n\nTue Feb 17\n\n\n\n\nThu Feb 19\n\n\n\n\nTue Feb 24\n\n\n\n\nThu Feb 26\nTest 1\n\n\n\n\n\n\nMarch\n\n\n\nDate\nTopic\nSlides\n\n\n\n\nTue Mar 3\n\n\n\n\nThu Mar 5\n\n\n\n\nTue Mar 10\n\n\n\n\nThu Mar 12\n\n\n\n\nTue Mar 17\nNo Class, Spring Break\n\n\n\nThu Mar 19\nNo Class, Spring Break\n\n\n\nTue Mar 24\nNo Class, Hickernell attending SIAM UQ 2026\n\n\n\nThu Mar 26\n\n\n\n\nTue Mar 31\n\n\n\n\n\n\n\nApril\n\n\n\n\n\n\n\n\nDate\nTopic\nSlides\n\n\n\n\nThu Apr 2\n\n\n\n\nTue Apr 7\nTest 2\n\n\n\nThu Apr 9\n\n\n\n\nTue Apr 14\n\n\n\n\nThu Apr 16\n\n\n\n\nTue Apr 21\n\n\n\n\nThu Apr 23\n\n\n\n\nTue Apr 28\n\n\n\n\nThu Apr 30\n\n\n\n\n\n\n\nMay\n\n\n\n\n\n\n\n\nDate\nTopic\nSlides\n\n\n\n\n?? May ?\nFinal Exam"
  },
  {
    "objectID": "pages/project-assessment.html",
    "href": "pages/project-assessment.html",
    "title": "Project Assessment",
    "section": "",
    "text": "Please provide thoughtful, constructive feedback to help your peers improve their communication skills\n\nPresenterâ€™s name:\nProject title:\nObserverâ€™s name:\n\n\n1. What are the main points of the presentation?\nWhat question was addressed? What was the main claim, result, or conclusion?\nÂ \nÂ \n\n\n\n2. What are the strengths and weaknesses of the article or project?\nStrengths might include clarity, insight, careful analysis, or convincing evidence. Weaknesses might include unclear assumptions, limited scope, missing comparisons, or questionable conclusions.\nÂ \nÂ \nÂ \n\n\n\n3. What did the presenter do well?\nOrganization, pacing, explanation of ideas, visuals, examples, or handling of questions.\nÂ \nÂ \nÂ \n\n\n\n4. How might the presenter have done better?\nWhat specific changes would most improve clarity, correctness, or impact?"
  },
  {
    "objectID": "pages/homework.html",
    "href": "pages/homework.html",
    "title": "Assignments",
    "section": "",
    "text": "Unless otherwise stated, you may work on assignments individually or in pairs, and your choice may change from one assignment to the next.\n\nFor each assignment, I will create a group set in Canvas; you will sign up to form your group.\n\nOnly one person per group submits the assignment in Canvas. The submission is automatically shared with all group members.\n\nGrades and feedback are shared with the whole group. By default, all members receive the same grade.\n\nAssignments are due by 11:59 PM Chicago Time on the due date (Canvas gives due dates in your time zone).\n\nEach submission must include all relevant files:\n\nA self-contained, working copy of any code used\nAND\nA PDF version of item 1, including the output, plus any analytic work.\n\nLegible handwriting and typed work are both acceptable.\nYou may embed the analytic work as markdown in the code if you wish."
  },
  {
    "objectID": "pages/homework.html#ground-rules-for-assignments",
    "href": "pages/homework.html#ground-rules-for-assignments",
    "title": "Assignments",
    "section": "",
    "text": "Unless otherwise stated, you may work on assignments individually or in pairs, and your choice may change from one assignment to the next.\n\nFor each assignment, I will create a group set in Canvas; you will sign up to form your group.\n\nOnly one person per group submits the assignment in Canvas. The submission is automatically shared with all group members.\n\nGrades and feedback are shared with the whole group. By default, all members receive the same grade.\n\nAssignments are due by 11:59 PM Chicago Time on the due date (Canvas gives due dates in your time zone).\n\nEach submission must include all relevant files:\n\nA self-contained, working copy of any code used\nAND\nA PDF version of item 1, including the output, plus any analytic work.\n\nLegible handwriting and typed work are both acceptable.\nYou may embed the analytic work as markdown in the code if you wish."
  },
  {
    "objectID": "pages/homework.html#list-of-assignments",
    "href": "pages/homework.html#list-of-assignments",
    "title": "Assignments",
    "section": "List of assignments",
    "text": "List of assignments\n\n\n\n\n\n\n\nAssignment\nDue Date\n\n\n\n\nAssignment 1 â€” Diagnostic survey (Canvas / Microsoft Forms)\nJanuary 16\n\n\nAssignment 2 â€” Casella & Berger 3.9, 3.28d, 4.26, 5.8,  Find and submit a screenshot, photo or pdf of a statistical claim that you find somewhere in public. Write a paragraph describing what questions you might have about the origin or interpretation of this statistical claim.\nJanuary 30\n\n\nAssignment 3 â€” Casella & Berger 7.3, 7.8, 7.20, 9.2, 9.12\nFebruary 13"
  },
  {
    "objectID": "classlib/classlib/quarto/pages/policies.html",
    "href": "classlib/classlib/quarto/pages/policies.html",
    "title": "Policies",
    "section": "",
    "text": "Reasonable accommodations will be made for students with documented disabilities. In order to receive accommodations, students must obtain a letter of accommodation from the Center for Disability Resources and make an appointment to speak with me, Fred Hickernell, as soon as possible. The Center for Disability Resources (CDR) is located in 3424 S. State St., room 1C3-2 (on the first floor), telephone 312-567-5744 or disabilities@illinoistech.edu."
  },
  {
    "objectID": "classlib/classlib/quarto/pages/policies.html#accommodations-for-disabilities",
    "href": "classlib/classlib/quarto/pages/policies.html#accommodations-for-disabilities",
    "title": "Policies",
    "section": "",
    "text": "Reasonable accommodations will be made for students with documented disabilities. In order to receive accommodations, students must obtain a letter of accommodation from the Center for Disability Resources and make an appointment to speak with me, Fred Hickernell, as soon as possible. The Center for Disability Resources (CDR) is located in 3424 S. State St., room 1C3-2 (on the first floor), telephone 312-567-5744 or disabilities@illinoistech.edu."
  },
  {
    "objectID": "classlib/classlib/quarto/pages/policies.html#sexual-harassment-and-discrimination-information",
    "href": "classlib/classlib/quarto/pages/policies.html#sexual-harassment-and-discrimination-information",
    "title": "Policies",
    "section": "Sexual Harassment and Discrimination Information",
    "text": "Sexual Harassment and Discrimination Information\nIllinois Tech prohibits all sexual harassment, sexual misconduct, and gender discrimination by any member of our community. This includes harassment among students, staff, or faculty. Sexual harassment of a student by a faculty member or sexual harassment of an employee by a supervisor is particularly serious. Such conduct may easily create an intimidating, hostile, or offensive environment.\nIllinois Tech encourages anyone experiencing sexual harassment or sexual misconduct to speak with the Office of Title IX Compliance for information on support options and the resolution process.\nYou can report sexual harassment electronically at iit.edu/incidentreport, which may be completed anonymously. You may additionally report by contacting the Title IX Coordinator, Virginia Foster at foster@illinoistech.edu. For confidential support, you may reach Illinois Techâ€™s Confidential Advisor at (773) 907-1062. You can also contact a licensed practitioner in Illinois Techâ€™s Student Health and Wellness Center at student.health@illinoistech.edu or (312)567-7550."
  },
  {
    "objectID": "classlib/classlib/quarto/pages/policies.html#academic-integrity-technology-and-ai-usage",
    "href": "classlib/classlib/quarto/pages/policies.html#academic-integrity-technology-and-ai-usage",
    "title": "Policies",
    "section": "Academic Integrity, Technology, and AI Usage",
    "text": "Academic Integrity, Technology, and AI Usage\nYou are encouraged to use technology, including AI tools like ChatGPT, to explore ideas and complete assignments or projects, as long as you understand the work and can explain it yourself. On tests and the final exam, no technology or AI will be allowed unless I say otherwise. Always use these tools to support your learning, not to replace it, and give credit if they meaningfully contribute to your work.\nStudents are expected to uphold the highest standards of honesty in their academic work, in accordance with Illinois Techâ€™s Code of Academic Honesty.\n\nPermitted Use\n\nAssignments & Projects: You are encouraged to use technology and AI tools to assist in problem-solving, exploring concepts, and checking work.\nYou must critically evaluate AI-generated content for accuracy, completeness, and clarity.\nYou are responsible for understanding and being able to explain any work produced with technological assistance.\n\n\n\nProhibited Use\n\nTests & Final Exam: No technology, AI tools, or external resources may be used unless explicitly allowed by the instructor.\nSubmitting AI-generated or technological outputs without personal understanding is prohibited and will be treated as a violation.\n\n\n\nCitations & Transparency\nIf you use AI tools, software, or other external resources in a way that significantly shapes your work, you must acknowledge it (e.g., â€œApproach suggested by ChatGPTâ€).\n\n\nConsequences\nViolations will be handled according to Illinois Techâ€™s Code of Academic Honesty."
  },
  {
    "objectID": "classlib/classlib/quarto/pages/git-clone-update-with-submodules.html",
    "href": "classlib/classlib/quarto/pages/git-clone-update-with-submodules.html",
    "title": "Git setup: clone the course repo and keep it updated",
    "section": "",
    "text": "This course uses a Git repository that includes submodules (shared libraries pulled in as separate Git repos). To get everything working, you must clone the repo and initialize the submodules."
  },
  {
    "objectID": "classlib/classlib/quarto/pages/git-clone-update-with-submodules.html#important-do-not-push-to-the-course-repository",
    "href": "classlib/classlib/quarto/pages/git-clone-update-with-submodules.html#important-do-not-push-to-the-course-repository",
    "title": "Git setup: clone the course repo and keep it updated",
    "section": "Important: do not push to the course repository",
    "text": "Important: do not push to the course repository\nStudents should assume the course repository is read-only:\n\nDo not try to push changes to the course repo.\nIf you see errors like â€œpermission deniedâ€ when pushing, that is expected.\n\nYou will not submit homework via Git. Git is used only to:\n\nObtain and update course materials\nExperiment with class notebooks\n\nWhen experimenting, consider working in copies of notebooks or in a separate directory so updates pull cleanly\n\n\nIf you want an online backup of your personal work, you may optionally use your own private repository or a fork (see below)."
  },
  {
    "objectID": "classlib/classlib/quarto/pages/git-clone-update-with-submodules.html#step-0-prerequisites",
    "href": "classlib/classlib/quarto/pages/git-clone-update-with-submodules.html#step-0-prerequisites",
    "title": "Git setup: clone the course repo and keep it updated",
    "section": "Step 0: Prerequisites",
    "text": "Step 0: Prerequisites\nYou need:\n\nGit installed\nA way to authenticate with GitHub (either SSH or HTTPS)\n\nYour instructor will provide the course repo URL. It looks like one of these:\n\nSSH: git@github.com:ORG/MATHXXXSpring20YY.git\nHTTPS: https://github.com/ORG/MATHXXXSpring20YY.git"
  },
  {
    "objectID": "classlib/classlib/quarto/pages/git-clone-update-with-submodules.html#step-1-clone-the-course-repo-with-submodules",
    "href": "classlib/classlib/quarto/pages/git-clone-update-with-submodules.html#step-1-clone-the-course-repo-with-submodules",
    "title": "Git setup: clone the course repo and keep it updated",
    "section": "Step 1: Clone the course repo with submodules",
    "text": "Step 1: Clone the course repo with submodules\nChoose one of the following.\n\nOption A (recommended): clone with submodules in one step\nReplace REPO_URL with the URL you were given.\ngit clone --recurse-submodules REPO_URL\n\n\nOption B: clone first, then fetch submodules\ngit clone REPO_URL\ncd MATHXXXSpring20YY\ngit submodule update --init --recursive"
  },
  {
    "objectID": "classlib/classlib/quarto/pages/git-clone-update-with-submodules.html#step-2-verify-you-have-the-submodule-content",
    "href": "classlib/classlib/quarto/pages/git-clone-update-with-submodules.html#step-2-verify-you-have-the-submodule-content",
    "title": "Git setup: clone the course repo and keep it updated",
    "section": "Step 2: Verify you have the submodule content",
    "text": "Step 2: Verify you have the submodule content\nFrom inside the course repo:\ngit submodule status\nYou should see one or more lines showing submodules and commit hashes. If you see errors or blank output, run:\ngit submodule update --init --recursive"
  },
  {
    "objectID": "classlib/classlib/quarto/pages/git-clone-update-with-submodules.html#keeping-your-local-copy-updated-during-the-semester",
    "href": "classlib/classlib/quarto/pages/git-clone-update-with-submodules.html#keeping-your-local-copy-updated-during-the-semester",
    "title": "Git setup: clone the course repo and keep it updated",
    "section": "Keeping your local copy updated during the semester",
    "text": "Keeping your local copy updated during the semester\nDo this whenever your instructor says â€œpull the latest changes.â€\nFrom inside the course repo:\ngit pull --ff-only\ngit submodule update --init --recursive\nThat second command is important: it updates submodules to the versions pinned by the course repo."
  },
  {
    "objectID": "classlib/classlib/quarto/pages/git-clone-update-with-submodules.html#if-you-already-made-local-changes",
    "href": "classlib/classlib/quarto/pages/git-clone-update-with-submodules.html#if-you-already-made-local-changes",
    "title": "Git setup: clone the course repo and keep it updated",
    "section": "If you already made local changes",
    "text": "If you already made local changes\nIf git pull --ff-only fails, it usually means you have local commits or uncommitted changes.\nCheck what changed:\ngit status\n\nIf you only changed files you donâ€™t care about (quick reset)\ngit reset --hard\ngit pull --ff-only\ngit submodule update --init --recursive\n\n\nIf you want to keep your work\nCommit your changes first so they are safely recorded locally.\ngit add -A\ngit commit -m \"My work\"\ngit pull --ff-only\ngit submodule update --init --recursive\nIf the pull still fails, you may need to resolve a merge conflict (ask for help)."
  },
  {
    "objectID": "classlib/classlib/quarto/pages/git-clone-update-with-submodules.html#optional-fork-workflow-push-only-to-your-own-fork",
    "href": "classlib/classlib/quarto/pages/git-clone-update-with-submodules.html#optional-fork-workflow-push-only-to-your-own-fork",
    "title": "Git setup: clone the course repo and keep it updated",
    "section": "Optional: fork workflow (push only to your own fork)",
    "text": "Optional: fork workflow (push only to your own fork)\nUse this only if you want to push your work to GitHub for backup/versioning.\n\nOne-time setup (fork + clone)\n\nFork the course repo on GitHub into your own GitHub account.\nClone your fork (not the instructor repo) with submodules.\n\nReplace FORK_URL with your forkâ€™s URL:\ngit clone --recurse-submodules FORK_URL\ncd MATHXXXSpring20YY\n\nAdd the instructor repo as upstream so you can pull course updates.\n\nReplace UPSTREAM_URL with the instructor repo URL:\ngit remote add upstream UPSTREAM_URL\ngit remote -v\n\n\nGetting updates from the instructor (sync upstream â†’ your fork)\nFrom inside your cloned fork:\ngit fetch upstream\ngit checkout main\ngit merge --ff-only upstream/main\ngit submodule update --init --recursive\ngit push\nIf the course repo uses a branch other than main (e.g., master), replace main above accordingly."
  },
  {
    "objectID": "classlib/classlib/quarto/pages/git-clone-update-with-submodules.html#common-issues-and-fixes",
    "href": "classlib/classlib/quarto/pages/git-clone-update-with-submodules.html#common-issues-and-fixes",
    "title": "Git setup: clone the course repo and keep it updated",
    "section": "Common issues and fixes",
    "text": "Common issues and fixes\n\nâ€œfatal: not a git repositoryâ€\nYouâ€™re not inside the course repo folder. Change into it:\ncd MATHXXXSpring20YY\n\n\nâ€œPermission denied (publickey)â€ (SSH)\nYouâ€™re using an SSH URL but your SSH key isnâ€™t set up for GitHub. Use the HTTPS URL instead, or configure SSH.\n\n\nSubmodule errors after pulling\nRe-run the submodule update:\ngit submodule update --init --recursive\n\n\nYou accidentally cloned without submodules\nFrom inside the repo:\ngit submodule update --init --recursive\n\n\nAvoid updating submodules beyond what the instructor pinned\nStudents should not run git submodule update --remote unless explicitly instructed."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to MATH 563 Mathematical Statistics",
    "section": "",
    "text": "Theory of sampling distributionsÍ¾ principles of data reductionÍ¾ interval and point estimation, sufficient statistics, order statistics, hypothesis testing, correlation and linear regressionÍ¾ introduction to linear models.\n\n\nInstructor: Fred J. Hickernell\n\n\n\nOffice: Galvin Tower 18C3-1\n\nOffice hours: By appointment (book here)\nPhone: 312-567-8983\n\nEmail: hickernell@illinoistech.edu\n\nWebsite Â Â Â  LinkedIn Â Â Â  Google Scholar Â Â Â  Mathematics Genealogy Profile\n\nBrief bio: Fred J. Hickernell is professor of applied mathematics. His research focuses on improving the efficiency of computer simulations and developing justifiable stopping criteria. He has spent most of his career at Hong Kong Baptist University and Illinois Institute of Technology, often in academic administrative roles. He has served on the editorial boards of the Journal of Complexity, Mathematics of Computation, and the SIAM Journal on Numerical Analysis. He is a Fellow of the Institute of Mathematical Statistics and the recipient of the 2016 Joseph F. Traub Prize for Achievement in Information-Based Complexity.\nHickernell speaks Cantonese and enjoys Chinese food. He is married with adult children. He enjoys discussing matters of substance. His most important identity is a disciple of Jesus.\n\n\n\nTeaching Assistant: Nishi Thakur\n\n\nOffice: RE ???\nOffice hours: ??? in RE 129\nEmail: nthakur5@hawk.illinoistech.edu\n\n\n\nTextbook: G. Casella & R. L. Berger, Statistical Inference, 2nd ed.Â (2002)\n\nRecommended: B. Efron & T. Hastie, Computer Age Statistical Inference (2016)\n\n\n\n\nRecommended resources\n\n\nPython\nAnaconda / Miniconda\npip (Python package manager)\nR\nRStudio (Posit)* VS Code\nJupyter\nGithub\nLaTeX\nOverleaf\nqmcpy\nCourse Repository\n\n\n\n\nPrerequisites\n\n\nMultivariate calculus\nLinear algebra\nCalculus-based probability\nComfortable wiht proofs\nComfortable with code\n\n\n\n\nObjectives\n\nYou will learn\n\nThe concept of statistical inference and the difference between population characteristics and sample estimates;\nThe probabilistic basis for statistical inference and the qualities of a good estimator;\nHow to correctly perform hypothesis tests and construct confidence intervals;\nThe appropriateness of asymptotic considerations;\nUse statistical softwareâ€”primarily Python or Râ€”to perform basic computations for statistical inference and sampling; and\nHow to communicate the results of their statistical analyses of substantial data sets through explanatory text, tables and graphs.\n\n\n\n\nWhere to find it\n\n\n\n\n\n\n\n\nThis course website\nCanvas\n\n\n\n\nBig Questions\nGrades\n\n\nSchedule\nAssignments and Submissions\n\n\nJupyter Notebooks\nAnnouncements\n\n\nHomework\nDiscussions\n\n\nTests and Exams\n\n\n\nPolicies"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Welcome to MATH 563 Mathematical Statistics",
    "section": "",
    "text": "Theory of sampling distributionsÍ¾ principles of data reductionÍ¾ interval and point estimation, sufficient statistics, order statistics, hypothesis testing, correlation and linear regressionÍ¾ introduction to linear models.\n\n\nInstructor: Fred J. Hickernell\n\n\n\nOffice: Galvin Tower 18C3-1\n\nOffice hours: By appointment (book here)\nPhone: 312-567-8983\n\nEmail: hickernell@illinoistech.edu\n\nWebsite Â Â Â  LinkedIn Â Â Â  Google Scholar Â Â Â  Mathematics Genealogy Profile\n\nBrief bio: Fred J. Hickernell is professor of applied mathematics. His research focuses on improving the efficiency of computer simulations and developing justifiable stopping criteria. He has spent most of his career at Hong Kong Baptist University and Illinois Institute of Technology, often in academic administrative roles. He has served on the editorial boards of the Journal of Complexity, Mathematics of Computation, and the SIAM Journal on Numerical Analysis. He is a Fellow of the Institute of Mathematical Statistics and the recipient of the 2016 Joseph F. Traub Prize for Achievement in Information-Based Complexity.\nHickernell speaks Cantonese and enjoys Chinese food. He is married with adult children. He enjoys discussing matters of substance. His most important identity is a disciple of Jesus.\n\n\n\nTeaching Assistant: Nishi Thakur\n\n\nOffice: RE ???\nOffice hours: ??? in RE 129\nEmail: nthakur5@hawk.illinoistech.edu\n\n\n\nTextbook: G. Casella & R. L. Berger, Statistical Inference, 2nd ed.Â (2002)\n\nRecommended: B. Efron & T. Hastie, Computer Age Statistical Inference (2016)\n\n\n\n\nRecommended resources\n\n\nPython\nAnaconda / Miniconda\npip (Python package manager)\nR\nRStudio (Posit)* VS Code\nJupyter\nGithub\nLaTeX\nOverleaf\nqmcpy\nCourse Repository\n\n\n\n\nPrerequisites\n\n\nMultivariate calculus\nLinear algebra\nCalculus-based probability\nComfortable wiht proofs\nComfortable with code\n\n\n\n\nObjectives\n\nYou will learn\n\nThe concept of statistical inference and the difference between population characteristics and sample estimates;\nThe probabilistic basis for statistical inference and the qualities of a good estimator;\nHow to correctly perform hypothesis tests and construct confidence intervals;\nThe appropriateness of asymptotic considerations;\nUse statistical softwareâ€”primarily Python or Râ€”to perform basic computations for statistical inference and sampling; and\nHow to communicate the results of their statistical analyses of substantial data sets through explanatory text, tables and graphs.\n\n\n\n\nWhere to find it\n\n\n\n\n\n\n\n\nThis course website\nCanvas\n\n\n\n\nBig Questions\nGrades\n\n\nSchedule\nAssignments and Submissions\n\n\nJupyter Notebooks\nAnnouncements\n\n\nHomework\nDiscussions\n\n\nTests and Exams\n\n\n\nPolicies"
  },
  {
    "objectID": "index.html#course-outline",
    "href": "index.html#course-outline",
    "title": "Welcome to MATH 563 Mathematical Statistics",
    "section": "Course Outline",
    "text": "Course Outline\n\n\nIntroduction\n\n\nWhat is statistical inference?\n\nReview of probability\n\nRandom variables and their distributions\nPopulations and samples\nSample statistics\n\n\n\n\n\nProperties of a random sample\n\n\nSampling from Normal distribution\nOrder statistics\nConvergence concepts\n\n\n\n\nPrinciples of data reduction\n\n\nThe sufficiency principle\nThe likelihood principle\n\n\n\n\nPoint estimation\n\n\nMethods of finding estimators: moment, MLE, Bayes\nMethods of evaluating estimators: MSE, bias, sufficiency, Rao-Â­Blackwell theorem, loss function optimality\n\n\n\n\nHypothesis testing\n\n\nMethods of finding tests: likelihood ratio test, Bayesian test\nMethods of evaluating tests: error probabilities, power function, p-alues, Neyman-Â­Pearson Lemma\n\n\n\n\nInterval estimation\n\n\nPivoting method\nSize and coverage probability\n\n\n\n\nAsymptotic evaluation\n\n\nPoint estimation: consistency, efficiency, comparisons\nAsymptotic distribution of LRTs / confidence intervals\n\n\n\n\nIntroduction to linear models\n\n\nSimple linear regression: least squares\nOne-Â­way ANOVA"
  },
  {
    "objectID": "index.html#assessment",
    "href": "index.html#assessment",
    "title": "Welcome to MATH 563 Mathematical Statistics",
    "section": "Assessment",
    "text": "Assessment\n\nHomework: 15%\n\nMidterm Tests: 25%\n\nProject: 20%\n\nFinal Exam: 40%\n\nExtra Credit: up to 10% Beginning January 20, if you are the first to find an error in\n\nMy lecture slides, Jupyter Notebooks, or assignments at least 24 hours after I have presented them in class, or\nThe textbook,\n\nThen I will award you 0.5 or 1 extra credit points, up to a maximum of 10 points. Â \nFor course materials hosted on GitHub, you may submit a pull request correcting the error. Alternatively, you may report the error by email with sufficient detail. Extra credit points are added to your final weighted total. Note that the weighted total may not correctly reflect the extra credit until grades are recorded for all categories."
  },
  {
    "objectID": "classlib/classlib/quarto/pages/interesting-articles-links.html",
    "href": "classlib/classlib/quarto/pages/interesting-articles-links.html",
    "title": "Interesting Articles & Links",
    "section": "",
    "text": "This page collects optional articles, essays, videos, and interactive resources that connect probability, statistics, and data science to real applications, historical context, or broader ideas.\n\nqmcpy.org: A Python Library for Quasi-Monte Carlo Methods\n\nA Python library for quasi-Monte Carlo methods, which are used to improve the efficiency of numerical integration and simulation"
  },
  {
    "objectID": "classlib/classlib/quarto/pages/stats-qs.html",
    "href": "classlib/classlib/quarto/pages/stats-qs.html",
    "title": "Big Statistics Questions We Ask",
    "section": "",
    "text": "Filter by course\n\n  \n     MATH 476 Statistics\n     MATH 563 Mathematical Statistics\n     MATH 565 Monte Carlo Methods\n  \n  \n\n\n\nModeling\n\n  Course(s)\n  Big question\n\n\n\n  476, 563, 565\n  \n    What is the population?\n    All US adults? All future units from a production line? All students with bachelor's degree? All continuous functions on a given domain?\n  \n\n\n\n  476, 563, 565\n  \n    What features are we interested in?\n    Mean? Variance? Quantiles? Regression function? Entire distribution?\n  \n\n\n\n  476, 563\n  \n    What does the model fail to capture?\n    Explanatory variables? Interactions? Nonlinearities?\n  \n\n\n\n  476, 563\n  \n    Are the distributional assumptions correct?\n    Normality? Skewness? Heavy tails? Independence?\n  \n\n\n\nSampling\n\n  Course(s)\n  Big question\n\n\n\n  476, 563, 565\n  \n    How do we sample from the population?\n    Independent and identically distributed (IID)? Low discrepancy (LD)? Stratified sampling? Designed experiment?\n  \n\n\n\n  476, 563\n  \n    What can go wrong between intended and actual sampling?\n    Nonresponse and missingness? Measurement error? Coverage error?\n  \n\n\n\nPoint Estimation\n\n  Course(s)\n  Big question\n\n\n\n  476, 563\n  \n    What is the estimand?\n    \n      Parameter vs functional of a distribution\n      What does it mean in context?\n      What is random, what is fixed?\n    \n  \n\n\n\n  563\n  \n    What makes an estimator â€œoptimalâ€?\n    \n      Loss functions and risk\n      Invariance/equivariance (as appropriate)\n      Benchmarks and lower bounds (when assumptions hold)"
  },
  {
    "objectID": "pages/notebooks.html",
    "href": "pages/notebooks.html",
    "title": "Jupyter Notebooks",
    "section": "",
    "text": "Here you will find Jupyter notebooks used in the course.\nClick the link to download, and then open in JupyterLab on your computer.\n\nApproval Ratings: Bernoulli, Binomial, CLT, and Confidence Intervals\nBayesian Quadrature\nHealth, Wealth, and Life Expectancy (Gapminder)"
  },
  {
    "objectID": "pages/project.html",
    "href": "pages/project.html",
    "title": "Project",
    "section": "",
    "text": "You are required to complete a course project. The goal is to practice communicating statistical ideas clearly and criticallyâ€”like you are reporting to colleagues whether a line of work is worth pursuing."
  },
  {
    "objectID": "pages/project.html#types",
    "href": "pages/project.html#types",
    "title": "Project",
    "section": "Types",
    "text": "Types\nChoose one of the following project types:\n\nResearch article review\nPick a scholarly research article related to topics in statistical inference (e.g., estimation, testing, likelihood/Bayes, asymptotics, bootstrap, identifiability, robustness, missing data, multiple testing, etc.) _that does not duplicate a topic discussed in class. Your article must be different from your classmatesâ€™ choices.\nSimulation or data analysis study\nDesign and carry out a focused study that uses computation to investigate a statistical question. Examples:\n\nAnalyze a data set of interest, but go beyond any existing analysis,\nCompare procedures (power, bias, coverage, MSE) under realistic departures from assumptions,\nIllustrate a theorem/asymptotic approximation with finite-sample behavior,\nReplicate and critique an empirical claim from a paper,\nBuild a small method â€œpipelineâ€ (e.g., model selection + inference) and evaluate it."
  },
  {
    "objectID": "pages/project.html#submission",
    "href": "pages/project.html#submission",
    "title": "Project",
    "section": "Submission",
    "text": "Submission\n\nSubmit your project topic using the course topic-submission form &lt;To Be Announced&gt; by Friday, March 6, 2026.\nAfter the topic deadline, check the posted topic list &lt;To Be Announced&gt; to confirm approval.\nPrepare a 15-minute oral presentation (+5 minutes for questions) and a handout for the audience."
  },
  {
    "objectID": "pages/project.html#presentations-and-observations",
    "href": "pages/project.html#presentations-and-observations",
    "title": "Project",
    "section": "Presentations and observations",
    "text": "Presentations and observations\n\nProject presentations will be scheduled near the end of the semester &lt;To Be Announced&gt;.\nEach student must observe and assess TWO other presentations (using the Project Assessment Form).\nYour project grade is likely to suffer if:\n\nyou are not on time for your presentation or observations,\nyou are not prepared,\nyou do not observe at least two presentations, and/or\nyour observer feedback is not significant."
  },
  {
    "objectID": "pages/project.html#important-to-keep-in-mind",
    "href": "pages/project.html#important-to-keep-in-mind",
    "title": "Project",
    "section": "Important to keep in mind",
    "text": "Important to keep in mind\nYour presentation should be from the perspective that you are reporting to your colleagues why this article/method/result is or is not worth pursuing further.\n\nYour audience is smart, but not specialized in your exact topic. Provide background, definitions, and motivation.\nBe critical: clearly state strengths and weaknesses, and what assumptions matter.\nYou have access to tools (including AI) that were not commonly available a few years agoâ€”so I will expect more polish and deeper checking.\nI may not be an expert in what you present, but I expect you to be, and I will ask probing questions.\nYour grade will depend on both:\n\nhow clearly you present, and\nhow well you answer questions."
  },
  {
    "objectID": "pages/project.html#guidelines",
    "href": "pages/project.html#guidelines",
    "title": "Project",
    "section": "Guidelines",
    "text": "Guidelines\n\nArticle review.\n\nChoose a topic not already covered in lecture in essentially the same way.\nPrefer papers with enough technical content to support real discussion (not just an application report).\nYour job is to explain the idea, demonstrate it on a simple example, and assess it.\n\nSimulation/data study.\n\nKeep the scope appropriate: one sharp question is better than many shallow ones.\nPredefine your evaluation metrics (e.g., coverage, power, MSE, false discovery rate).\nUse clear plots/tables and report uncertainty when relevant.\n\n\nGeneral presentation advice:\n\nKeep the big picture in view; donâ€™t get bogged down in algebra.\nChoose one simple example that illustrates the main point.\nLook at your audience while speaking; do not read your notes.\nMotivate your talk: background, prior work, and why anyone should care.\nDiscuss pros/cons and failure modes.\nDonâ€™t use too many slides; about one per minute is usually enough.\nDonâ€™t overload slides; prioritize figures/tables over dense text.\nInclude key formulas/figures directly in your slides rather than making the audience hunt in the paper.\nUse your own words. Do not plagiarize.\nPractice beforehand. Time yourself.\n\nYou will be assessed by your observers and by me using this form:\nProject Assessment Form"
  },
  {
    "objectID": "pages/tests.html",
    "href": "pages/tests.html",
    "title": "Tests",
    "section": "",
    "text": "Information about midterms, finals, and archived tests is posted here."
  },
  {
    "objectID": "pages/tests.html#this-semesters-tests-and-exams",
    "href": "pages/tests.html#this-semesters-tests-and-exams",
    "title": "Tests",
    "section": "This semesterâ€™s tests and exams",
    "text": "This semesterâ€™s tests and exams\nDates, coverage, and PDFs for this semester will appear here as they become available.\n\n\n\n\n\n\n\n\nTipðŸ”Ž How to search the archives (PDFs) for keywords\n\n\n\n\n\nGoal: find which past exams mention a term (e.g., Brownian, Sobol, CLT) inside PDF test and exam files.\nThe folder assets/tests/archive in the course repository contains subfolders for archived tests from past semesters for MATH 476, MATH 563, and MATH 565. The subfolders are the course numbers, e.g., archive/563 for MATH 563 tests.\n\nMac (Homebrew) / Linux\nOne-time setup (install pdfgrep):\nbrew install pdfgrep\nEach time you want to search the archive:\n\nNavigate to the root of your local course repository root, then run:\n\ncd assets/tests\n\nUse one of the following commands to search the archives:\n\nSearch all archives in this repo:\npdfgrep -rniH -C2 'your term here' archive\nSearch one specific archive folder:\npdfgrep -rniH -C2 'your term here' archive/563\nSearch multiple archives at once:\npdfgrep -rniH -C2 'your term here' archive/476 archive/563\nExamples:\npdfgrep -rniH -C2 'brownian' archive\npdfgrep -rniH -C2 'sobol' archive\npdfgrep -rniH -C2 'central limit|CLT' archive\n\n\nWindows (recommended: WSL)\nOn Windows, the recommended approach is to use Windows Subsystem for Linux (WSL), which lets you run the same commands as Mac/Linux users.\nAfter installing WSL (e.g., Ubuntu):\ncd /mnt/c/Users/YourName/path/to/course-repo/assets/tests\nsudo apt update\nsudo apt install pdfgrep\npdfgrep -rniH -C2 'your term here' archive\nIf you prefer not to use WSL, note that Windows does not include a native equivalent of pdfgrep; setup and commands vary by third-party tools.\n\n\n\n\n\nArchive\n\n\n\nMATH476 563FinalS11Answers\nMATH563Test1S11Answers\n\n\n\n\n\nArchive from MATH 476 (related material)\n\n\n\nMATH476FinalExamS10Answers\nMATH476FinalS11Answers\nMATH476StatFinalS06Answers\nMATH476StatTestS06Answers\nMATH476StatTestTwoS06Answers\nMATH476Test1S10Answers\nMATH476Test1S11Answers\nMATH476Test2S10Answers\nMATH476Test2S11Answers\nMATH476Test2S11MakeUpAnswers\nMATH476Test2v2S10Answers"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/probability/conditional-prob.html",
    "href": "classlib/classlib/quarto/slides/shared/probability/conditional-prob.html",
    "title": "MATH 563 â€” Spring 2026",
    "section": "",
    "text": "The conditional probability means restricting attention to outcomes where \\(B\\) occurs, then measure how often \\(A\\) occurs within that restricted universe of \\(A\\) given \\(B\\) (with \\(\\Prob(B)&gt;0\\)): \\[\n\\Prob(A \\mid B)\n=\n\\frac{\\Prob(A \\cap B)}{\\Prob(B)},\n\\] which means restricting attention to outcomes where \\(B\\) occurs, then measuring how often \\(A\\) occurs within that restricted universe.\n\nMultiplication rule: \\(\\Prob(A \\cap B)\n=\n\\Prob(A \\mid B) \\Prob(B)\\)\n\\(A\\) and \\(B\\) are independent if \\(\\Prob(A \\mid B) = \\Prob(A)\\) or equivalently \\(\\Prob(A \\cap B)=\\Prob(A)\\Prob(B)\\)\n\n\n\n\\[\n\\Prob(A \\mid B)\n=\n\\frac{\\Prob(B \\mid A) \\, \\Prob(A)}{\\Prob(B)},\n\\qquad \\Prob(B)&gt;0\n\\]\n\nLet \\(X\\) and \\(Y\\) have joint density \\(\\varrho_{X,Y}(x,y)\\). The conditional density of \\(X\\) given \\(Y=y\\) is \\[\n\\varrho_{X\\mid Y}(x \\mid y)\n=\n\\frac{\\varrho_{X,Y}(x,y)}{\\varrho_Y(y)},\n\\qquad\n\\varrho_Y(y)=\\int \\varrho_{X,Y}(x,y) \\, \\dif x \\text{ is the }\\class{alert}{\\text{marginal density}} \\text{ of } Y\n\\]\n\n\n\n\\[\n\\varrho_{X\\mid Y}(x\\mid y)\n=\n\\frac{\\varrho_{Y\\mid X}(y\\mid x)\\,\\varrho_X(x)}{\\varrho_Y(y)}\n\\]\n\\(\\exstar\\) If you meet an introvert, is she more likely to be a librarian or a salesman?\n\\(\\exstar\\) What probability distributions defined on \\([0,\\infty)\\) satisfy the memoryless property, i.e.Â \\(\\Prob[X &gt; t + x | X &gt; t] = \\Prob[X &gt; x]\\)?"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/probability/conditional-prob.html#conditional-prob",
    "href": "classlib/classlib/quarto/slides/shared/probability/conditional-prob.html#conditional-prob",
    "title": "MATH 563 â€” Spring 2026",
    "section": "",
    "text": "The conditional probability means restricting attention to outcomes where \\(B\\) occurs, then measure how often \\(A\\) occurs within that restricted universe of \\(A\\) given \\(B\\) (with \\(\\Prob(B)&gt;0\\)): \\[\n\\Prob(A \\mid B)\n=\n\\frac{\\Prob(A \\cap B)}{\\Prob(B)},\n\\] which means restricting attention to outcomes where \\(B\\) occurs, then measuring how often \\(A\\) occurs within that restricted universe.\n\nMultiplication rule: \\(\\Prob(A \\cap B)\n=\n\\Prob(A \\mid B) \\Prob(B)\\)\n\\(A\\) and \\(B\\) are independent if \\(\\Prob(A \\mid B) = \\Prob(A)\\) or equivalently \\(\\Prob(A \\cap B)=\\Prob(A)\\Prob(B)\\)\n\n\n\n\\[\n\\Prob(A \\mid B)\n=\n\\frac{\\Prob(B \\mid A) \\, \\Prob(A)}{\\Prob(B)},\n\\qquad \\Prob(B)&gt;0\n\\]\n\nLet \\(X\\) and \\(Y\\) have joint density \\(\\varrho_{X,Y}(x,y)\\). The conditional density of \\(X\\) given \\(Y=y\\) is \\[\n\\varrho_{X\\mid Y}(x \\mid y)\n=\n\\frac{\\varrho_{X,Y}(x,y)}{\\varrho_Y(y)},\n\\qquad\n\\varrho_Y(y)=\\int \\varrho_{X,Y}(x,y) \\, \\dif x \\text{ is the }\\class{alert}{\\text{marginal density}} \\text{ of } Y\n\\]\n\n\n\n\\[\n\\varrho_{X\\mid Y}(x\\mid y)\n=\n\\frac{\\varrho_{Y\\mid X}(y\\mid x)\\,\\varrho_X(x)}{\\varrho_Y(y)}\n\\]\n\\(\\exstar\\) If you meet an introvert, is she more likely to be a librarian or a salesman?\n\\(\\exstar\\) What probability distributions defined on \\([0,\\infty)\\) satisfy the memoryless property, i.e.Â \\(\\Prob[X &gt; t + x | X &gt; t] = \\Prob[X &gt; x]\\)?"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/probability/conditional-prob.html#bayesian-statistics-to-approximate-and-integrate-functions",
    "href": "classlib/classlib/quarto/slides/shared/probability/conditional-prob.html#bayesian-statistics-to-approximate-and-integrate-functions",
    "title": "MATH 563 â€” Spring 2026",
    "section": "Bayesian statistics to approximate and integrate functions",
    "text": "Bayesian statistics to approximate and integrate functions\nWe assume that \\(f\\), the function to approximate or integrate, is an instance of a Gaussian process, \\(\\GP(0,K)\\). This means that\n\\[\\begin{gather*}\n\\vf := \\bigl(f(x_1), \\ldots, f(x_n) \\bigr)^\\top \\sim \\Norm(\\vzero, \\mK), \\quad \\text{where }\\mK : = \\bigl(K(x_i,x_j)\\bigr)_{i,j=1}^n \\\\\n\\tvf := \\bigl(f(x_1), \\ldots, f(x_n), f(x) \\bigr)^\\top \\sim \\Norm(\\vzero, \\tmK) \\\\\n\\text{where }\n\\tmK : = \\left(\\begin{array}{c|c}\n\\mK & \\vk(x) \\\\\n\\hline\n\\vk^\\top(x) & K(x,x)\n\\end{array}\n\\right)\n, \\quad \\vk(x) = \\bigl( K(x,x_i) \\bigr)_{i=1}^n\n\\end{gather*}\\] and so the conditional density of \\(f(x)\\) given \\(\\vf = \\vy\\), where \\(\\vy\\) is the observed data, is\n\\[\\begin{align*}\n\\varrho_{f(x) | \\vf}(z | \\vy) = \\frac{\\varrho_{\\tvf}(\\tvy)}{\\varrho_{\\vf}(\\vy)} = \\frac{\\frac{\\exp(-\\tvy^\\top \\tmK^{-1} \\tvy/2)}{\\sqrt{(2 \\pi)^{n+1} \\det(\\tmK)}}}{\\frac{\\exp(-\\vy^\\top \\mK^{-1} \\vy/2)}{\\sqrt{(2 \\pi)^{n} \\det(\\mK)}}}\n= \\frac{\\exp([-\\tvy^\\top \\tmK^{-1} \\tvy + \\vy^\\top \\mK^{-1} \\vy]/2)}{\\sqrt{(2 \\pi) \\det(\\tmK)/\\det(\\mK)}}\n\\end{align*}\\] where \\(\\tvy = (\\vy^\\top, z)^\\top\\)\n\nOne can show that the inverse of the covariance matrix is\n\\[\\begin{gather*}\n\\tmK^{-1}\n=\n\\left(\\begin{array}{c|c}\n\\mK^{-1}+\\mK^{-1}\\vk(x)\\sigma^{-2}(x)\\vk^\\top(x)\\mK^{-1}\n&\n-\\mK^{-1}\\vk(x)\\sigma^{-2}(x)\n\\\\\n\\hline\n- \\sigma^{-2}(x)\\vk^\\top(x)\\mK^{-1}\n&\n\\sigma^{-2}(x)\n\\end{array}\n\\right)\\\\\n\\text{where }\n\\sigma^2(x)=K(x,x)-\\vk^\\top(x)\\mK^{-1}\\vk(x), \\qquad\n\\sigma^{-2}(x) := \\big(\\sigma^2(x)\\big)^{-1}\n\\end{gather*}\\]\nSo\n\\[\\begin{align*}\n\\MoveEqLeft{-\\tvy^\\top \\tmK^{-1} \\tvy + \\vy^\\top \\mK^{-1} \\vy}\\\\\n& = -\\vy^\\top[\\mK^{-1}+\\mK^{-1}\\vk(x)\\sigma^{-2}(x)\\vk^\\top(x)\\mK^{-1}]\\vy + 2 \\vy^\\top\\mK^{-1}\\vk(x)\\sigma^{-2}(x) z - z^2 \\sigma^{-2}(x) \\\\\n& \\qquad \\qquad + \\vy^\\top \\mK^{-1} \\vy \\\\\n& = \\sigma^{-2}(x) [ z - \\vy^\\top \\mK^{-1} \\vk(x)]^2\n\\end{align*}\\]\nand \\(f(x) \\mid \\vf = \\vy \\sim \\Norm\\bigl(\\vy^\\top \\mK^{-1} \\vk(x), \\sigma^2(x) \\bigr)\\)\n\nBy a similar argument \\(\\int_0 ^1 f(x)\\, \\dif x \\mid \\vf = \\vy \\sim \\Norm\\Bigl(\\vy^\\top \\mK^{-1} \\int_0^1 \\vk(x) \\, \\dif x, \\int_0^1 \\int_0^1 K(t,x) \\, \\dif t \\dif x - \\int_0^1 \\vk^\\top(x) \\, \\dif x \\, \\mK^{-1} \\, \\int_0^1 \\vk(x) \\, \\dif x  \\Bigr)\\)"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/probability/conditional-prob.html#bayesian-inference",
    "href": "classlib/classlib/quarto/slides/shared/probability/conditional-prob.html#bayesian-inference",
    "title": "MATH 563 â€” Spring 2026",
    "section": "Bayesian inference",
    "text": "Bayesian inference\n\nBayesâ€™ rule for densities\n\\[\n\\varrho_{X\\mid Y}(x\\mid y)\n=\n\\frac{\\varrho_{Y\\mid X}(y\\mid x)\\,\\varrho_X(x)}{\\varrho_Y(y)}\n\\]\nLet \\[\\begin{align*}\nX & = \\text{random unknown parameter} \\\\\nY & = \\text{random observed data} \\\\\n\\varrho_X & = \\text{prior density or belief about the parameter} \\\\\n\\varrho_{Y\\mid X} & = \\text{likelihood} \\\\\n\\varrho_{X\\mid Y} & = \\text{posterior density}\n\\end{align*}\\] Then \\[\\begin{align*}\n\\varrho_{X\\mid Y}( \\text{parameter} \\mid \\text{data})\n& =\n\\frac{\\varrho_{Y\\mid X}(\\text{data}\\mid \\text{parameter} ) \\,\\varrho_X(\\text{parameter})}{\\varrho_Y(\\text{data})}\n\\\\\n\\text{posterior density } &\n\\propto\n\\text{likelihood} \\times \\text{prior density}\n\\end{align*}\\]"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/probability/conditional-prob.html#conditional-exp",
    "href": "classlib/classlib/quarto/slides/shared/probability/conditional-prob.html#conditional-exp",
    "title": "MATH 563 â€” Spring 2026",
    "section": "Conditional expectation",
    "text": "Conditional expectation\nConditional expectation of \\(X\\) given event \\(A\\) (with \\(\\Prob(A)&gt;0\\)) is \\[\n\\Ex[X \\mid A]\n=\n\\frac{\\Ex[X \\, \\indic(X \\in A)]}{\\Prob(A)} \\qquad \\text{and so} \\qquad \\Ex[X \\, \\indic(X \\in A)]\n=\n\\Prob(A)\\,\\Ex[X \\mid A]\n\\] The indicator function, \\(\\indic(\\cdot)\\), equals \\(1\\) if the statement inside the parentheses is true, and \\(0\\) otherwise.\n\nConditional expectation of \\(X\\) given \\(Y=y\\) is \\[\n\\Ex[X \\mid Y=y]\n=\n\\int x \\, \\varrho_{X\\mid Y}(x \\mid y)\\,\\dif x\n\\] If \\(g(y)=\\Ex[X \\mid Y=y]\\), then \\(\\Ex[X \\mid Y] = g(Y)\\) is a random variable depending only on \\(Y\\)\n\n\nIf \\(X\\) is independent of \\(Y\\), then \\(\\Ex[X \\mid Y] = \\Ex[X]\\)\n\n\n\nConditioning on an event\nFor any event \\(A\\) with \\(0&lt;\\Prob(A)&lt;1\\), \\[\n\\Ex[X]\n=\n\\Prob(A)\\,\\Ex[X \\mid A]\n+\n\\Prob(A^c)\\,\\Ex[X \\mid A^c]\n\\]\nMore generally, if \\(\\{A_1,\\ldots,A_n\\}\\) is a partition of the sample space with \\(\\Prob(A_i)&gt;0\\), then \\[\n\\Ex[X]\n=\n\\sum_{i=1}^n\n\\Prob(A_i)\\,\\Ex[X \\mid A_i].\n\\]\n\n\nLaw of total expectation\n\\[\n\\Ex\\!\\bigl[\\Ex[X \\mid Y]\\bigr]\n=\n\\Ex[X]\n\\]\n\n\nLaw of total variance\n\\[\n\\var(X)\n=\n\\Ex\\!\\bigl[\\var(X \\mid Y)\\bigr]\n+\n\\var\\!\\bigl(\\Ex[X \\mid Y]\\bigr)\n\\]"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/probability/probability-review-distrib-part-1.html",
    "href": "classlib/classlib/quarto/slides/shared/probability/probability-review-distrib-part-1.html",
    "title": "Probability Review",
    "section": "",
    "text": "Key concepts\nImportant distributions\nConditional probability\nTypes of convergence\nCentral Limit Theorem\n\n\n\n\nOutcome â€” a single possible result of a random process, e.g., yes, \\(0.5\\), \\((1,-2)\\).\nSample space â€” the set of all possible outcomes, e.g., \\(\\{\\text{yes},\\text{no}\\}\\), \\(\\reals\\).\nEvent â€” a subset of the sample space to which we may assign a probability, e.g., intervals like \\([0,\\infty)\\) or regions such as \\(\\{(x,y) : x^2 + y^2 \\le 1\\}\\).\nEvent space â€” a collection of events (subsets of the sample space) that we are allowed to assign probabilities to.\nProbability â€” a function\n[ :] satisfying:\n\n\\(0 \\le \\Prob(A) \\le 1\\) for every event \\(A\\)\n\\(\\Prob(\\text{sample space}) = 1\\)\nIf \\(A \\cap B = \\varnothing\\), then \\(\\Prob(A \\cup B) = \\Prob(A) + \\Prob(B)\\)\n\n\n\n\nRandom variable/vector/function â€” rule that assigns a number (or vector) to each outcome, e.g., \\(Y = 1\\) if approve, \\(0\\) if not; \\(X = \\text{time to wait for taxi}\\); \\(\\vX = (\\text{height}, \\text{weight})\\)\nCumulative distribution function (CDF) \\(F\\) of random variable \\(X\\) (right-continuous)\n\n\\(F(x) : =  \\Prob(X \\le x)\\)\nSurvival function \\(S(x) := \\Prob(X &gt; x) = 1 - F(x)\\)\n\nQuantile function \\(Q\\) of a random variable \\(X\\) plays the role of the inverse of the CDF (in a left-continuous sense)\n\n\\(Q(p) : = \\inf\\{x \\in \\reals : F(x) \\ge p\\}, \\quad 0 &lt; p &lt; 1\\)\n\nProbability density function (PDF) \\(\\varrho\\) of an absolutely continuous random variable \\(X\\)\n\n\\(\\varrho(x) := F'(x)\\)\n\n\n\n\nExpectation â€” average value of a random variable, weighted by its probability\n\n\\(\\displaystyle \\Ex[f(X)] : = \\sum_{x} f(x) \\varrho(x)\\) for discrete random variables\n\\(\\displaystyle \\Ex[f(X)] : =  \\int f(x) \\varrho(x) \\, \\dif x\\) for continuous random variables\n\\(\\displaystyle \\Ex[f(X)] : =  \\int f(x)  \\, \\dif F(x)\\) in general (see Lebesgueâ€“Stieltjes integral)\n\nMean of \\(X\\) is \\(\\mu : = \\Ex(X) \\exeq \\Argmin{m\\in\\reals} \\Ex[(X-m)^2]\\)\n\n\\(\\Ex(a X + Y) \\exeq a \\Ex(X) + \\Ex(Y)\\)\n\nVariance of \\(X\\) is \\(\\sigma^2 : = \\var(X) := \\Ex[(X - \\mu)^2] \\exeq \\Ex(X^2) - \\mu^2\\)\n\nStandard deviation of \\(X\\) is \\(\\std(X) := \\sigma = \\sqrt{\\var(X)}\\)\n\nMedian of \\(X\\) is \\(\\med(X) : = Q(1/2) \\in\n\\Argmin{m\\in\\reals} \\Ex (\\lvert X-m \\rvert)\\)\n\n\n\n\\((X,Y)\\) are independent iff \\(F_{X,Y}(x,y) = F_X(x) F_Y(y)\\) for all \\(x,y\\)\n\nor equivalently \\(\\varrho_{X,Y}(x,y) = \\varrho_X(x) \\varrho_Y(y)\\) for all \\(x,y\\)\n\nCovariance \\(\\cov(X,Y) : = \\Ex[(X- \\mu_X)(Y - \\mu_Y)]\\)\n\n\\(\\var(aX + Y) \\exeq a^2 \\var(X) + \\var(Y)\\) if \\(\\cov(X,Y) = 0\\)\n\nCorrelation \\(\\displaystyle \\corr(X,Y) : = \\frac{\\cov(X,Y)}{\\std(X) \\std(Y)}\\)\nIndependence \\(\\implies\\) zero correlation, but zero correlation \\(\\notimplies\\) independence\n\n\nE.g., \\(\\Prob[(X,Y) = (x,y)] = 1/4\\) for \\((x,y) \\in \\{(\\pm 1, 0), (0, \\pm 1)\\}\\) \\[\\begin{gather*}\n\\text{PMF }\\varrho_X(x) = \\varrho_Y(x) = \\begin{cases}\n\\frac 14 & x =  \\pm 1 \\\\\n\\frac 12 & x = 0\n\\end{cases}\n\\qquad \\Ex(X) = \\Ex(Y) = 0 \\\\\n\\cov(X,Y) = \\Ex(XY) = 0 \\quad \\text{so }\\class{alert}{\\text{uncorrelated}} \\\\\n\\text{BUT } \\varrho_{XY}(x,y) \\ne \\varrho_X(x)\\varrho_Y(y) \\quad \\text{so }\\class{alert}{\\text{dependent}}\n\\end{gather*}\\]\n\n\n\n\n\n\n\nBinomial (free-throw shooting, discrete) \\(\\Bin(n,p)\\) \\[\n\\varrho(x) : = \\Prob(X=x) = \\binom{n}{x} p^x (1-p)^{n-x}, \\ x =0, \\ldots, n; \\quad \\Ex(X) = np, \\; \\var(X) = np(1-p)\n\\]\nExponential (taxi waiting, continuous) \\(\\Exp(\\lambda)\\) \\[\nF(x): = 1 - \\exp(-\\lambda x),\\ \\varrho(x) : = \\lambda \\exp(-\\lambda x), \\ x \\ge 0; \\quad \\Ex(X) = 1/\\lambda, \\quad \\var(X) = 1/\\lambda^2\n\\]\nUniform (random number generation, continuous) \\(\\Unif(a,b)\\) \\[\\begin{gather*}\nF(x) : = \\begin{cases} 0, & -\\infty &lt; x &lt; a, \\\\\n\\displaystyle \\frac{x - a}{b - a}, & a \\le x &lt; b, \\\\\n1, & b \\le x &lt; \\infty,\n\\end{cases} \\qquad \\qquad\n\\varrho(x): = \\frac{1}{b-a}, \\ x \\in [a,b]; \\\\\n\\Ex(X) = \\frac{a+b}{2}, \\quad  \\var(X) = \\frac{(b-a)^2}{12}  \n\\end{gather*}\\]\n\n\n\nNormal (Gaussian) (limiting distribution, continuous) \\(\\Norm(\\mu, \\sigma^2)\\) \\[\n\\varrho(x): = \\frac{\\exp\\bigl(-(x-\\mu)^2/(2\\sigma^2)\\bigr)}{\\sqrt{2 \\pi} \\sigma}, \\ x \\in \\reals;\\quad \\Ex(X) = \\mu, \\quad \\var(X) = \\sigma^2\n\\]\nMultivariate Normal (Gaussian) (limiting distribution, continuous) \\(\\Norm(\\vmu, \\mSigma)\\) \\[\\begin{gather*}\n\\varrho(\\vx): = \\frac{\\exp\\bigl(-(\\vx-\\vmu)^\\top \\mSigma^{-1} (\\vx-\\vmu) /2\\bigr)}{\\sqrt{(2 \\pi)^{d} \\det(\\mSigma)}};\\; \\Ex(\\vX) = \\vmu, \\\\\n\\cov(\\vX) = \\mSigma\n\\class{alert}{\\text{ symmetric, positive definite covariance matrix}}\n\\end{gather*}\\]"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/probability/probability-review-distrib-part-1.html#prob-concepts",
    "href": "classlib/classlib/quarto/slides/shared/probability/probability-review-distrib-part-1.html#prob-concepts",
    "title": "Probability Review",
    "section": "",
    "text": "Outcome â€” a single possible result of a random process, e.g., yes, \\(0.5\\), \\((1,-2)\\).\nSample space â€” the set of all possible outcomes, e.g., \\(\\{\\text{yes},\\text{no}\\}\\), \\(\\reals\\).\nEvent â€” a subset of the sample space to which we may assign a probability, e.g., intervals like \\([0,\\infty)\\) or regions such as \\(\\{(x,y) : x^2 + y^2 \\le 1\\}\\).\nEvent space â€” a collection of events (subsets of the sample space) that we are allowed to assign probabilities to.\nProbability â€” a function\n[ :] satisfying:\n\n\\(0 \\le \\Prob(A) \\le 1\\) for every event \\(A\\)\n\\(\\Prob(\\text{sample space}) = 1\\)\nIf \\(A \\cap B = \\varnothing\\), then \\(\\Prob(A \\cup B) = \\Prob(A) + \\Prob(B)\\)\n\n\n\n\nRandom variable/vector/function â€” rule that assigns a number (or vector) to each outcome, e.g., \\(Y = 1\\) if approve, \\(0\\) if not; \\(X = \\text{time to wait for taxi}\\); \\(\\vX = (\\text{height}, \\text{weight})\\)\nCumulative distribution function (CDF) \\(F\\) of random variable \\(X\\) (right-continuous)\n\n\\(F(x) : =  \\Prob(X \\le x)\\)\nSurvival function \\(S(x) := \\Prob(X &gt; x) = 1 - F(x)\\)\n\nQuantile function \\(Q\\) of a random variable \\(X\\) plays the role of the inverse of the CDF (in a left-continuous sense)\n\n\\(Q(p) : = \\inf\\{x \\in \\reals : F(x) \\ge p\\}, \\quad 0 &lt; p &lt; 1\\)\n\nProbability density function (PDF) \\(\\varrho\\) of an absolutely continuous random variable \\(X\\)\n\n\\(\\varrho(x) := F'(x)\\)\n\n\n\n\nExpectation â€” average value of a random variable, weighted by its probability\n\n\\(\\displaystyle \\Ex[f(X)] : = \\sum_{x} f(x) \\varrho(x)\\) for discrete random variables\n\\(\\displaystyle \\Ex[f(X)] : =  \\int f(x) \\varrho(x) \\, \\dif x\\) for continuous random variables\n\\(\\displaystyle \\Ex[f(X)] : =  \\int f(x)  \\, \\dif F(x)\\) in general (see Lebesgueâ€“Stieltjes integral)\n\nMean of \\(X\\) is \\(\\mu : = \\Ex(X) \\exeq \\Argmin{m\\in\\reals} \\Ex[(X-m)^2]\\)\n\n\\(\\Ex(a X + Y) \\exeq a \\Ex(X) + \\Ex(Y)\\)\n\nVariance of \\(X\\) is \\(\\sigma^2 : = \\var(X) := \\Ex[(X - \\mu)^2] \\exeq \\Ex(X^2) - \\mu^2\\)\n\nStandard deviation of \\(X\\) is \\(\\std(X) := \\sigma = \\sqrt{\\var(X)}\\)\n\nMedian of \\(X\\) is \\(\\med(X) : = Q(1/2) \\in\n\\Argmin{m\\in\\reals} \\Ex (\\lvert X-m \\rvert)\\)\n\n\n\n\\((X,Y)\\) are independent iff \\(F_{X,Y}(x,y) = F_X(x) F_Y(y)\\) for all \\(x,y\\)\n\nor equivalently \\(\\varrho_{X,Y}(x,y) = \\varrho_X(x) \\varrho_Y(y)\\) for all \\(x,y\\)\n\nCovariance \\(\\cov(X,Y) : = \\Ex[(X- \\mu_X)(Y - \\mu_Y)]\\)\n\n\\(\\var(aX + Y) \\exeq a^2 \\var(X) + \\var(Y)\\) if \\(\\cov(X,Y) = 0\\)\n\nCorrelation \\(\\displaystyle \\corr(X,Y) : = \\frac{\\cov(X,Y)}{\\std(X) \\std(Y)}\\)\nIndependence \\(\\implies\\) zero correlation, but zero correlation \\(\\notimplies\\) independence\n\n\nE.g., \\(\\Prob[(X,Y) = (x,y)] = 1/4\\) for \\((x,y) \\in \\{(\\pm 1, 0), (0, \\pm 1)\\}\\) \\[\\begin{gather*}\n\\text{PMF }\\varrho_X(x) = \\varrho_Y(x) = \\begin{cases}\n\\frac 14 & x =  \\pm 1 \\\\\n\\frac 12 & x = 0\n\\end{cases}\n\\qquad \\Ex(X) = \\Ex(Y) = 0 \\\\\n\\cov(X,Y) = \\Ex(XY) = 0 \\quad \\text{so }\\class{alert}{\\text{uncorrelated}} \\\\\n\\text{BUT } \\varrho_{XY}(x,y) \\ne \\varrho_X(x)\\varrho_Y(y) \\quad \\text{so }\\class{alert}{\\text{dependent}}\n\\end{gather*}\\]"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/probability/probability-review-distrib-part-1.html#prob-distrib",
    "href": "classlib/classlib/quarto/slides/shared/probability/probability-review-distrib-part-1.html#prob-distrib",
    "title": "Probability Review",
    "section": "",
    "text": "Binomial (free-throw shooting, discrete) \\(\\Bin(n,p)\\) \\[\n\\varrho(x) : = \\Prob(X=x) = \\binom{n}{x} p^x (1-p)^{n-x}, \\ x =0, \\ldots, n; \\quad \\Ex(X) = np, \\; \\var(X) = np(1-p)\n\\]\nExponential (taxi waiting, continuous) \\(\\Exp(\\lambda)\\) \\[\nF(x): = 1 - \\exp(-\\lambda x),\\ \\varrho(x) : = \\lambda \\exp(-\\lambda x), \\ x \\ge 0; \\quad \\Ex(X) = 1/\\lambda, \\quad \\var(X) = 1/\\lambda^2\n\\]\nUniform (random number generation, continuous) \\(\\Unif(a,b)\\) \\[\\begin{gather*}\nF(x) : = \\begin{cases} 0, & -\\infty &lt; x &lt; a, \\\\\n\\displaystyle \\frac{x - a}{b - a}, & a \\le x &lt; b, \\\\\n1, & b \\le x &lt; \\infty,\n\\end{cases} \\qquad \\qquad\n\\varrho(x): = \\frac{1}{b-a}, \\ x \\in [a,b]; \\\\\n\\Ex(X) = \\frac{a+b}{2}, \\quad  \\var(X) = \\frac{(b-a)^2}{12}  \n\\end{gather*}\\]\n\n\n\nNormal (Gaussian) (limiting distribution, continuous) \\(\\Norm(\\mu, \\sigma^2)\\) \\[\n\\varrho(x): = \\frac{\\exp\\bigl(-(x-\\mu)^2/(2\\sigma^2)\\bigr)}{\\sqrt{2 \\pi} \\sigma}, \\ x \\in \\reals;\\quad \\Ex(X) = \\mu, \\quad \\var(X) = \\sigma^2\n\\]\nMultivariate Normal (Gaussian) (limiting distribution, continuous) \\(\\Norm(\\vmu, \\mSigma)\\) \\[\\begin{gather*}\n\\varrho(\\vx): = \\frac{\\exp\\bigl(-(\\vx-\\vmu)^\\top \\mSigma^{-1} (\\vx-\\vmu) /2\\bigr)}{\\sqrt{(2 \\pi)^{d} \\det(\\mSigma)}};\\; \\Ex(\\vX) = \\vmu, \\\\\n\\cov(\\vX) = \\mSigma\n\\class{alert}{\\text{ symmetric, positive definite covariance matrix}}\n\\end{gather*}\\]"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/statistics/estimation/confidence.html",
    "href": "classlib/classlib/quarto/slides/shared/statistics/estimation/confidence.html",
    "title": "Confidence Intervals",
    "section": "",
    "text": "If\nthen we try to construct random quantities \\(\\Theta_L\\) and/or \\(\\Theta_U\\), depending only on the data (and not on \\(\\theta\\)), that give intervals which capture \\(\\theta\\) with high probability \\(1-\\alpha\\). Depending on the situation, this means constructing\nThe bounds \\(\\Theta_L\\) and \\(\\Theta_U\\) are random because they depend on random data. Here \\(\\alpha\\) is our willingness to be wrong, typically \\(\\alpha = 5\\%\\)."
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/statistics/estimation/confidence.html#large-sample-confidence-intervals-for-means",
    "href": "classlib/classlib/quarto/slides/shared/statistics/estimation/confidence.html#large-sample-confidence-intervals-for-means",
    "title": "Confidence Intervals",
    "section": "Large sample size confidence intervals for means",
    "text": "Large sample size confidence intervals for means\nIf \\(X_1, \\ldots, X_n\\) are IID with mean \\(\\mu\\) and variance \\(\\sigma^2 &lt; \\infty\\), and \\(\\barX_n\\) is the sample mean, then by the Central Limit Theorem \\[\n\\frac{\\barX_n - \\mu}{\\sigma/\\sqrt{n}} \\appxsim \\Norm(0,1) \\quad \\text{for large } n\n\\] Letting \\(z_{\\alpha/2}\\) be the upper \\(\\alpha/2\\) quantile of \\(\\Norm(0,1)\\), i.e., \\(z_{\\alpha/2} = Q_{\\Norm(0,1)}(1 - \\alpha/2)\\), then \\[\\begin{align*}\n1 - \\alpha & \\approx\n\\Prob \\biggl( -z_{\\alpha/2} \\le \\frac{\\barX_n - \\mu}{\\sigma/\\sqrt{n}} \\le z_{\\alpha/2} \\biggr) \\\\\n& \\approx \\Prob \\biggl( \\barX_n - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\le \\mu \\le \\barX_n + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\biggr) \\\\\n& \\approx \\Prob \\biggl( \\underbrace{\\barX_n - z_{\\alpha/2} \\frac{S}{\\sqrt{n}}}_{\\Theta_L} \\le \\mu \\le \\underbrace{\\barX_n + z_{\\alpha/2} \\frac{S}{\\sqrt{n}}}_{\\Theta_U} \\biggr)\n\\end{align*}\\] where \\(S^2\\) is some estimate of the unknown population variance \\(\\sigma^2\\) (e.g., unbiased or MLE)\n\nSee the Approval Ratings example for an illustration of this construction for a Bernoulli mean\nExample: You observe \\(\\barX_n = 12.0\\) minutes for taxis to arrive.You construct a \\(95\\%\\) confidence interval for the mean arrival time, \\(\\mu\\), assuming that the arrival times are distributed \\(\\Exp(1/\\mu)\\). Recall that \\(\\mu = \\sigma = 1/\\lambda\\).\n\n\n\n\\(n\\)\nCLT CI for \\(\\mu\\) (\\(S=\\barX_{n}\\))\nCI width\n\n\n\n\n16\n\\([6.12,\\,17.88]\\)\n11.760\n\n\n400\n\\([10.82,\\,13.18]\\)\n2.352"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/statistics/estimation/confidence.html#small-sample-confidence-intervals-for-means-when-the-distribution-is-known",
    "href": "classlib/classlib/quarto/slides/shared/statistics/estimation/confidence.html#small-sample-confidence-intervals-for-means-when-the-distribution-is-known",
    "title": "Confidence Intervals",
    "section": "Small sample size confidence intervals for means when the distribution is known",
    "text": "Small sample size confidence intervals for means when the distribution is known\nIf the sample size is not large enough for the Central Limit Theorem to apply, but the sample mean has a known distribution, then exact confidence intervals can sometimes be constructed\nExample: You observe \\(\\barX_n = 12.0\\) minutes for taxis to arrive.based on \\(n\\). You construct a \\(95\\%\\) confidence interval for the mean arrival time, \\(\\mu\\), assuming that the arrival times are distributed \\(\\Exp(1/\\mu)\\). Recall that \\(\\mu = \\sigma = 1/\\lambda\\).\nBut now we have the true distribution of \\(\\barX_n\\):\n\\[\\begin{align*}\n2 \\lambda n \\barX_n \\sim \\chi^2_{2n} \\implies 1 - \\alpha &= \\Prob \\biggl( \\chi^2_{2n, \\alpha/2} \\le 2 \\lambda n \\barX_n \\le \\chi^2_{2n, 1-\\alpha/2} \\biggr) \\\\\n& = \\Prob \\biggl( \\frac{\\chi^2_{2n, \\alpha/2}}{2 n \\barX_n} \\le \\lambda \\le \\frac{\\chi^2_{2n, 1-\\alpha/2}}{2 n \\barX_n} \\biggr) \\\\\n& = \\Prob \\biggl( \\frac{2 n \\barX_n}{\\chi^2_{2n, 1-\\alpha/2}} \\le \\mu \\le \\frac{2 n \\barX_n}{\\chi^2_{2n, \\alpha/2}} \\biggr)\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\(n\\)\nExact \\(\\chi^2\\) CI\nExact CI width\nCLT CI\nCLT CI width\n(CLT width/Exact width)\n\n\n\n\n16\n\\([7.76,\\,20.99]\\)\n13.234\n\\([6.12,\\,17.88]\\)\n11.760\n0.889\n\n\n400\n\\([10.91,\\,13.27]\\)\n2.363\n\\([10.82,\\,13.18]\\)\n2.352\n0.995\n\n\n\n\nFor small \\(n\\)\n\nExact confidence interval can be substantially different from the CLT-based interval\nCLT is symmetric about \\(\\barX_n\\), while the exact interval is not\n\nAs \\(n\\) increases, CLT-based interval approaches the exact interval\n\nÂ \n â¬‡ Exact vs CLT Confidence Interval Coverage ðŸ““ \nÂ \n\n\\(\\exstar\\) You draw \\(n\\) IID samples of your product to test for failure. What is your confidence interval for \\(p\\), the probability of a satisfactory product, if none of the samples fail?"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/statistics/estimation/confidence.html#students-t-confidence-interval-for-normal-data-with-unknown-variance",
    "href": "classlib/classlib/quarto/slides/shared/statistics/estimation/confidence.html#students-t-confidence-interval-for-normal-data-with-unknown-variance",
    "title": "Confidence Intervals",
    "section": "Studentâ€™s \\(t\\) Confidence Interval for Normal Data with Unknown Variance",
    "text": "Studentâ€™s \\(t\\) Confidence Interval for Normal Data with Unknown Variance\nFor \\(X_1,\\dots,X_n \\IIDsim \\Norm(\\mu, \\sigma^2)\\)\n\nSample mean: \\(\\barX\\)\nSample standard deviation: \\(S\\)\n\nWe have an exact confidence interval for all \\(n\\): \\[\n\\Prob\\left[ \\barX - t_{1-\\alpha/2,n-1}\\frac{S}{\\sqrt{n}} \\le \\mu \\le \\barX + t_{1-\\alpha/2,n-1}\\frac{S}{\\sqrt{n}} \\right] = 1 - \\alpha\n\\] where \\(t_{1-\\alpha/2,n-1}\\) is the upper \\(\\alpha/2\\) quantile of the Studentâ€™s t distribution with \\(n-1\\) degrees of freedom\n\nFor large \\(n\\), this interval is close to the CLT-based interval"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/statistics/estimation/confidence.html#confidence-intervals-for-means-of-differences",
    "href": "classlib/classlib/quarto/slides/shared/statistics/estimation/confidence.html#confidence-intervals-for-means-of-differences",
    "title": "Confidence Intervals",
    "section": "Confidence Intervals for Means of Differences",
    "text": "Confidence Intervals for Means of Differences\nFor paired or matched data (before/after, twins, same subject measured twice) \\[\nD_i = X_i - Y_i, \\quad i = 1,\\dots,n\n\\]\nInference is about the mean difference \\(\\mu_D\\)\nNot difference of means \\(\\mu_X - \\mu_Y\\), even though \\(\\barD = \\barX - \\barY\\)\nIf \\(D_1,\\dots,D_n \\IIDsim \\Norm(\\mu_D, \\sigma_D^2)\\) \\[\n\\Prob\\left[ \\barD - t_{1-\\alpha/2,n-1}\\frac{S_D}{\\sqrt{n}} \\le \\mu_D \\le \\barD + t_{1-\\alpha/2,n-1}\\frac{S_D}{\\sqrt{n}} \\right] = 1 - \\alpha\n\\]\nIf \\(D_1,\\dots,D_n\\) are IID with finite variance, and \\(n\\) is large \\[\n\\Prob\\left[ \\barD - z_{1-\\alpha/2}\\frac{S_D}{\\sqrt{n}} \\le \\mu_D \\le \\barD + z_{1-\\alpha/2}\\frac{S_D}{\\sqrt{n}} \\right] \\approx 1 - \\alpha\n\\]"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/statistics/estimation/confidence.html#confidence-intervals-for-differences-of-means",
    "href": "classlib/classlib/quarto/slides/shared/statistics/estimation/confidence.html#confidence-intervals-for-differences-of-means",
    "title": "Confidence Intervals",
    "section": "Confidence Intervals for Differences of Means",
    "text": "Confidence Intervals for Differences of Means\nFor two independent samples (control/treatment, two groups)\n\\[\nX_1,\\dots,X_{n_X} \\sim \\text{population 1}, \\quad\nY_1,\\dots,Y_{n_Y} \\sim \\text{population 2}\n\\] with sample means \\(\\barX,\\barY\\) and sample variances \\(s_X^2,s_Y^2\\)\nIf the \\(X\\) and \\(Y\\) are Normal, we have an approximate confidence interval for \\(\\mu_X - \\mu_Y\\)\n\\[\n\\Prob\\left[\n(\\barX-\\barY)\n-\nt_{1-\\alpha/2,\\nu}\n\\sqrt{\\frac{S_X^2}{n_X}+\\frac{S_Y^2}{n_Y}} \\le \\mu_X - \\mu_Y \\le\n(\\barX-\\barY)\n+\nt_{1-\\alpha/2,\\nu}\n\\sqrt{\\frac{S_X^2}{n_X}+\\frac{S_Y^2}{n_Y}}  \n\\right] = 1 - \\alpha\n\\]\nwhere \\(\\nu\\) is an approximate degrees of freedom (value not critical here)\nOther variations exist, e.g., pooled t, Welch\n\nIf \\(n_X, n_Y\\) are large, we have a CLT-based interval\n\\[\n\\Prob\\left[\n(\\barX-\\barY)\n- z_{1-\\alpha/2}\n\\sqrt{\\frac{S_X^2}{n_X}+\\frac{S_Y^2}{n_Y}} \\le \\mu_X - \\mu_Y \\le\n(\\barX-\\barY)\n+ z_{1-\\alpha/2}\n\\sqrt{\\frac{S_X^2}{n_X}+\\frac{S_Y^2}{n_Y}}  \n\\right] \\approx 1 - \\alpha\n\\]"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/statistics/estimation/confidence.html#confidence-intervals-for-proportions",
    "href": "classlib/classlib/quarto/slides/shared/statistics/estimation/confidence.html#confidence-intervals-for-proportions",
    "title": "Confidence Intervals",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\nTypical setting: Bernoulli trials\nLet\n[ X (n,p), p = X/n ]\nParameter of interest: population proportion \\(p\\).\nCommon methods\n\nNormal approximation interval\n\nRequires \\(np\\) and \\(n(1-p)\\) large\nSimple but can undercover\n\nWilson / Agrestiâ€“Coull interval\n\nImproved accuracy over normal approximation\nNear-nominal coverage for moderate \\(n\\)\n\nExact (Clopperâ€“Pearson) interval\n\nNo sample size restriction\nGuaranteed coverage\nConservative (wider than necessary)\n\n\nKey tradeoff: simplicity vs coverage accuracy."
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/statistics/estimation/confidence.html#confidence-intervals-for-variances",
    "href": "classlib/classlib/quarto/slides/shared/statistics/estimation/confidence.html#confidence-intervals-for-variances",
    "title": "Confidence Intervals",
    "section": "Confidence Intervals for Variances",
    "text": "Confidence Intervals for Variances\nTypical setting: inference about variability\nParameter of interest: population variance \\(\\sigma^2\\).\nCommon methods\n\nChi-squared interval\n\nAssumes data are IID Normal\nExact for any \\(n\\)\nVery sensitive to non-normality\n\nAsymptotic (CLT-based) intervals\n\nAssume finite fourth moments\nRequire large \\(n\\)\nRarely used in introductory courses\n\n\nImportant: variance inference is far less robust than mean inference."
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/statistics/estimation/confidence.html#assumptions-behind-common-confidence-intervals",
    "href": "classlib/classlib/quarto/slides/shared/statistics/estimation/confidence.html#assumptions-behind-common-confidence-intervals",
    "title": "Confidence Intervals",
    "section": "Assumptions Behind Common Confidence Intervals",
    "text": "Assumptions Behind Common Confidence Intervals\nData are IID from a distribution with finite variance\n\n\n\n\n\n\n\n\n\n\nParameter\nDistributional Assumptions\nSample Size\nMethod\nNotes\n\n\n\n\n\\(\\mu\\)\nAny distribution\nLarge \\(n\\)\nCentral Limit Theorem (CLT)\nApproximate, accuracy improves as \\(n \\to \\infty\\)\n\n\n\\(\\mu\\)\nNormal data, \\(\\sigma\\) unknown\nAny \\(n\\)\nStudentâ€™s t\nExact\n\n\n\\(\\mu = p\\)\nBernoulli trials\nAny \\(n\\)\nBinomial(Clopperâ€“Pearson)\nExactConservative\n\n\n\\(\\mu = p\\)\nBernoulli trials\nLarge \\(np\\), \\(n(1-p)\\)\nCentral Limit Theorem (CLT)\nApproximate\n\n\n\\(\\mu\\)\nExponential data\nAny \\(n\\)\nGamma/Chi-squared\nExact\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nDistributional Assumptions\nSample Size\nMethod\nNotes\n\n\n\n\n\\(\\mu_D\\) (paired differences)\nDifferences are Normal\nAny \\(n\\)\nPaired t\nExact, sometimes confused with two-sample t\n\n\n\\(\\mu_1-\\mu_2\\)\nEach sample Normal; independent samples\nAny \\(n_1,n_2\\)\nTwo-sample t (Wackerly)\nApproximate, uses unpooled standard error\n\n\n\\(\\mu_1-\\mu_2\\)\nIndependent samples from any distributions with finite variances\nLarge \\(n_1,n_2\\)\nCLT (two-sample)\nApproximate\n\n\n\n\n\n\n\n\n\n\\(\\sigma^2\\)\nNormal data\nAny \\(n\\)\nChi-squared\nExact, sensitive to non-normality\n\n\n\\(\\med(X)\\)\nContinuous distribution\n\\(n\\) not too small\nOrder-statistics\nApproximate, Distribution-free\n\n\n\n\n\nSummary\n\nConfidence intervals provide a range of plausible values for parameters based on data\nThe validity of confidence intervals depends on assumptions about the data distribution and sample size(s)\n\nWith fewer data we need stronger distributional assumptions\nWith more data we can rely on asymptotic results like the CLT"
  },
  {
    "objectID": "classlib/classlib/quarto/slides/shared/statistics/intro/how-to-read-slides.html",
    "href": "classlib/classlib/quarto/slides/shared/statistics/intro/how-to-read-slides.html",
    "title": "How to read these slides",
    "section": "",
    "text": "How to read these slides\nThese slides are written in Quarto and the source code is available on the class repository, MATH563Spring2026 on GitHub\nÂ \n\nThis is a third level header\n\nÂ \nThis is a link to somewhere else, in this case the title slide\nÂ \nThis color means emphasis\nÂ \nMathematical formulas are written in LaTeX syntax, e.g., \\(\\me^{\\pi i} + 1 = 0\\) or \\[\n\\int_a^b f(x) \\, \\dif x \\approx \\sum_{i=1}^n w_i f(x_i)\n\\]"
  },
  {
    "objectID": "slides/01-intro.html#approval-ratings",
    "href": "slides/01-intro.html#approval-ratings",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Approval ratings",
    "text": "Approval ratings\nThe Gallup Poll tracks the approval ratings of US presidents according to a careful polling methodology\n\nEach week they telephone \\(n=1500\\) adults\nTheir sampling error for the approval ratings is about \\(4\\%\\)\n\nLet \\(Y \\sim \\Bern(\\mu)\\), a Bernoulli (free throw shooting) distribution with probability of success \\(\\mu\\), \\[\n\\Prob(Y =y) = \\begin{cases} \\mu, & y=1, \\text{ (yes, approve)}\\\\\n1-\\mu, & y = 0, \\text{ (no, do not approve)}\n\\end{cases}\n\\]\n\n\nIf \\(Y_1, \\ldots, Y_n \\IIDsim \\Bern(\\mu)\\), then \\[\\begin{align*}\nT &:= Y_1 + \\cdots + Y_n \\sim \\Bin(n,\\mu), \\; \\Prob(T = t) = \\binom{n}{t} \\mu^t (1-\\mu)^{n-t}\\\\\n\\barY &:= \\frac 1n (Y_1 + \\cdots + Y_n) \\\\\n& \\appxsim  \\Norm\\bigl(\\mu,\\mu(1-\\mu)/n\\bigr) \\quad \\text{by the }\\class{alert}{\\text{Central Limit Theorem}}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/01-intro.html#life-gdp",
    "href": "slides/01-intro.html#life-gdp",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Life expectancy vs gross domestic product (GDP) per capita",
    "text": "Life expectancy vs gross domestic product (GDP) per capita\nCountries differ dramatically in both life expectancy and economic output. A classic dataset (World Bank / Gapminder) records, for each country,\n\nLife \\(=\\) Life expectancy at birth (years)\nGDP \\(=\\) Gross domestic product per capita (USD, purchasing-power adjusted)\n\nEach point represents one country, not one individual. We are interested in the conditional behavior of life expectancy with given national income:\n\n\n\\[\n\\Ex(\\text{Life} | \\text{GDP} = x)\n\\]"
  },
  {
    "objectID": "slides/01-intro.html#trapezoidal",
    "href": "slides/01-intro.html#trapezoidal",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Numerical integration",
    "text": "Numerical integration\nComputational mathematics seems distinct from statistics, but there is an overlap.\nSuppose that \\(f\\) is a random function defined in \\([0,1]\\), in particular, a Gaussian process, i.e., \\(f \\sim \\GP(0,K)\\). This means that for any distinct \\(x_1, \\ldots, x_n \\in [0,1]\\), the random vector of function values, \\(\\vf := \\bigl(f(x_1), \\ldots, f(x_n) \\bigr)^\\top\\), has a multivariate Gaussian distribution with\n\nMean \\(\\vmu = \\Ex(\\vf) = \\vzero\\)\nCovariance matrix \\(\\mSigma := \\Ex(\\vf \\vf^\\top) = \\bigl(\\Ex[f(x_i)f(x_j)]\\bigr)_{i,j=1}^n  = \\bigl(K(x_i,x_j)\\bigr)_{i,j=1}^n =: \\mK\\)\n\n\nIf this is the Bayesian prior belief about \\(f\\), then the Bayesian posterior mean of the function conditioned on the data \\(\\vf = \\vy\\) is \\[\n\\Ex\\bigl[f(x) \\mid \\vf  = \\vy\\bigr] = \\vy^\\top \\mK^{-1} \\vk(x) , \\quad\n\\text{where } \\vk(x) = \\bigl( K(x,x_i) \\bigr)_{i=1}^n\n\\]\nThe Bayesian posterior mean integral of \\(f\\) conditioned on the data is\n\\[\n\\Ex\\biggl[\\int_0^1 f(x) \\, \\dif x \\mid \\vf  = \\vy\\biggr] = \\vy^\\top \\mK^{-1} \\int_0^1 \\vk(x) \\, \\dif x\n\\]\nFor \\(K(t,x) = 2 - \\lvert t - x\\rvert\\) this is the trapezoidal rule.  â¬‡ Bayesian Quadrature ðŸ““"
  },
  {
    "objectID": "slides/01-intro.html#prob-concepts",
    "href": "slides/01-intro.html#prob-concepts",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Key probability concepts",
    "text": "Key probability concepts\n\nOutcome â€” a single possible result of a random process, e.g., yes, \\(0.5\\), \\((1,-2)\\).\nSample space â€” the set of all possible outcomes, e.g., \\(\\{\\text{yes},\\text{no}\\}\\), \\(\\reals\\).\nEvent â€” a subset of the sample space to which we may assign a probability, e.g., intervals like \\([0,\\infty)\\) or regions such as \\(\\{(x,y) : x^2 + y^2 \\le 1\\}\\).\nEvent space â€” a collection of events (subsets of the sample space) that we are allowed to assign probabilities to.\nProbability â€” a function\n[ :] satisfying:\n\n\\(0 \\le \\Prob(A) \\le 1\\) for every event \\(A\\)\n\\(\\Prob(\\text{sample space}) = 1\\)\nIf \\(A \\cap B = \\varnothing\\), then \\(\\Prob(A \\cup B) = \\Prob(A) + \\Prob(B)\\)"
  },
  {
    "objectID": "slides/01-intro.html#prob-distrib",
    "href": "slides/01-intro.html#prob-distrib",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Important distributions",
    "text": "Important distributions\n\nBinomial (free-throw shooting, discrete) \\(\\Bin(n,p)\\) \\[\n\\varrho(x) : = \\Prob(X=x) = \\binom{n}{x} p^x (1-p)^{n-x}, \\ x =0, \\ldots, n; \\quad \\Ex(X) = np, \\; \\var(X) = np(1-p)\n\\]\nExponential (taxi waiting, continuous) \\(\\Exp(\\lambda)\\) \\[\nF(x): = 1 - \\exp(-\\lambda x),\\ \\varrho(x) : = \\lambda \\exp(-\\lambda x), \\ x \\ge 0; \\quad \\Ex(X) = 1/\\lambda, \\quad \\var(X) = 1/\\lambda^2\n\\]\nUniform (random number generation, continuous) \\(\\Unif(a,b)\\) \\[\\begin{gather*}\nF(x) : = \\begin{cases} 0, & -\\infty &lt; x &lt; a, \\\\\n\\displaystyle \\frac{x - a}{b - a}, & a \\le x &lt; b, \\\\\n1, & b \\le x &lt; \\infty,\n\\end{cases} \\qquad \\qquad\n\\varrho(x): = \\frac{1}{b-a}, \\ x \\in [a,b]; \\\\\n\\Ex(X) = \\frac{a+b}{2}, \\quad  \\var(X) = \\frac{(b-a)^2}{12}  \n\\end{gather*}\\]"
  },
  {
    "objectID": "slides/01-intro.html#exponential-families-of-distributions",
    "href": "slides/01-intro.html#exponential-families-of-distributions",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Exponential families of distributions",
    "text": "Exponential families of distributions\nIf a PMF or PDF can be expressed as \\[\n\\varrho(\\vx\\mid \\vtheta) = h(\\vx) \\, c(\\vtheta) \\, \\exp \\biggl(\\sum_k w_k(\\vtheta) t_k(\\vx) \\biggr) \\quad \\vx \\in \\reals^d,\n\\] then this is called an exponential family. E.g.,\n\nBinomial for fixed \\(n\\) and \\(0 &lt; p &lt; 1\\): \\(\\quad \\displaystyle \\varrho(x) = \\binom{n}{x} p^x (1-p)^{n-x} = \\indic(x \\in \\{0, \\ldots, n\\}) \\binom{n}{x} \\, (1-p)^n \\, \\exp\\biggl(\\log\\biggl(\\frac{p}{1-p} \\biggr ) x \\biggr)\\)\nExponential: \\(\\quad \\varrho(x) = \\indic(x \\ge 0) \\, \\exp(-\\lambda x)\\)\nNormal (Gaussian): \\(\\quad \\displaystyle \\varrho(x) = \\frac{\\exp\\bigl(-(x-\\mu)^2/(2\\sigma^2)\\bigr)}{\\sqrt{2 \\pi} \\sigma} = \\frac{\\exp\\bigl(-\\mu^2/(2\\sigma^2)\\bigr)}{\\sqrt{2 \\pi} \\sigma} \\, \\exp\\biggl(-\\frac{x^2}{2 \\sigma^2} + \\frac{ \\mu x }{\\sigma^2}\\biggr)\\)\nMultivariate Normal (Gaussian): \\[\n\\varrho(\\vx): = \\frac{\\exp\\bigl(-(\\vx-\\vmu)^\\top \\mSigma^{-1} (\\vx-\\vmu) /2\\bigr)}{\\sqrt{(2 \\pi)^{d} \\det(\\mSigma)}}=\n\\frac{\\exp(-\\vmu^ \\top\\mSigma^{-1} \\vmu/2 )}{\\sqrt{(2 \\pi)^{d} \\det(\\mSigma)}}\n\\, \\exp\\biggl(-\\frac{\\vx^\\top \\mSigma^{-1} \\vx}{2} + \\vmu^\\top \\mSigma^{-1} \\vx \\biggr)\n\\]"
  },
  {
    "objectID": "slides/01-intro.html#a-mixed-partially-discrete-partially-continuous-random-variable",
    "href": "slides/01-intro.html#a-mixed-partially-discrete-partially-continuous-random-variable",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "A mixed (partially discrete, partially continuous) random variable",
    "text": "A mixed (partially discrete, partially continuous) random variable\n\nDiscrete random variables have a PMF\nContinuous random variables have a PDF\nSome random variables are mixed\n\nZero-inflated exponential distribution â€” \\(X\\) is the waiting time for a taxi\n\nA taxi waiting for you with probability \\(p\\)\nBut if there is none, then the waiting time is exponential with mean \\(1/\\lambda\\) \\[\\begin{align*}\nF(x) &= \\begin{cases}\n0, &-\\infty &lt; x &lt; 0 \\\\\n1- (1-p)\\exp(-\\lambda x),  & 0 \\le x &lt; \\infty\n\\end{cases}\n\\end{align*}\\] Note: \\(F\\) has a jump of size \\(p\\) at \\(x=0\\), corresponding to \\(\\Prob(X=0)=p\\)"
  },
  {
    "objectID": "slides/01-intro.html#lebesguestieltjes-integral-integral-jumps",
    "href": "slides/01-intro.html#lebesguestieltjes-integral-integral-jumps",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Lebesgueâ€“Stieltjes integral = integral + jumps",
    "text": "Lebesgueâ€“Stieltjes integral = integral + jumps\nLet the CDF \\(F\\)\n\nBe a nondecreasing, right-continuous function on \\(\\mathbb{R}\\)\nHave (possible) discontinuities at \\(-\\infty &lt;  x_1 &lt; \\ldots &lt; x_N &lt; \\infty\\)\nBe differentiable between its jump points, with \\(\\varrho(x)=F'(x)\\) there\n\nand let \\(f\\) be a suitable function\nThe Lebesgueâ€“Stieltjes integral of \\(f\\) with respect to \\(F\\) can be written as \\[\\begin{align*}\n  \\int f(x)\\, dF(x) & = \\int_{-\\infty}^{x_{1}} f(x) \\,\\varrho(x) \\, \\dif x + \\sum_{k=1}^{N-1} \\int_{x_k}^{x_{k+1}} f(x) \\, \\varrho(x) \\, \\dif x + \\int_{x_N}^{\\infty} f(x) \\, \\varrho(x) \\, \\dif x \\\\\n  & \\qquad \\qquad + \\sum_{k=1}^N f(x_k)[F(x_k) - F(x_k^{-})]\n  \\end{align*}\\]\nIntegrate \\(f\\) where \\(F\\) is smooth, and add a weighted sum of \\(f\\) at the jump points of \\(F\\)"
  },
  {
    "objectID": "slides/01-intro.html#conditional-prob",
    "href": "slides/01-intro.html#conditional-prob",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Conditional probability",
    "text": "Conditional probability\nThe conditional probability means restricting attention to outcomes where \\(B\\) occurs, then measure how often \\(A\\) occurs within that restricted universe of \\(A\\) given \\(B\\) (with \\(\\Prob(B)&gt;0\\)): \\[\n\\Prob(A \\mid B)\n=\n\\frac{\\Prob(A \\cap B)}{\\Prob(B)},\n\\] which means restricting attention to outcomes where \\(B\\) occurs, then measuring how often \\(A\\) occurs within that restricted universe.\n\nMultiplication rule: \\(\\Prob(A \\cap B)\n=\n\\Prob(A \\mid B) \\Prob(B)\\)\n\\(A\\) and \\(B\\) are independent if \\(\\Prob(A \\mid B) = \\Prob(A)\\) or equivalently \\(\\Prob(A \\cap B)=\\Prob(A)\\Prob(B)\\)\n\nBayesâ€™ Rule\n\\[\n\\Prob(A \\mid B)\n=\n\\frac{\\Prob(B \\mid A) \\, \\Prob(A)}{\\Prob(B)},\n\\qquad \\Prob(B)&gt;0\n\\]"
  },
  {
    "objectID": "slides/01-intro.html#bayesian-statistics-to-approximate-and-integrate-functions",
    "href": "slides/01-intro.html#bayesian-statistics-to-approximate-and-integrate-functions",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Bayesian statistics to approximate and integrate functions",
    "text": "Bayesian statistics to approximate and integrate functions\nWe assume that \\(f\\), the function to approximate or integrate, is an instance of a Gaussian process, \\(\\GP(0,K)\\). This means that\n\\[\\begin{gather*}\n\\vf := \\bigl(f(x_1), \\ldots, f(x_n) \\bigr)^\\top \\sim \\Norm(\\vzero, \\mK), \\quad \\text{where }\\mK : = \\bigl(K(x_i,x_j)\\bigr)_{i,j=1}^n \\\\\n\\tvf := \\bigl(f(x_1), \\ldots, f(x_n), f(x) \\bigr)^\\top \\sim \\Norm(\\vzero, \\tmK) \\\\\n\\text{where }\n\\tmK : = \\left(\\begin{array}{c|c}\n\\mK & \\vk(x) \\\\\n\\hline\n\\vk^\\top(x) & K(x,x)\n\\end{array}\n\\right)\n, \\quad \\vk(x) = \\bigl( K(x,x_i) \\bigr)_{i=1}^n\n\\end{gather*}\\] and so the conditional density of \\(f(x)\\) given \\(\\vf = \\vy\\), where \\(\\vy\\) is the observed data, is\n\\[\\begin{align*}\n\\varrho_{f(x) | \\vf}(z | \\vy) = \\frac{\\varrho_{\\tvf}(\\tvy)}{\\varrho_{\\vf}(\\vy)} = \\frac{\\frac{\\exp(-\\tvy^\\top \\tmK^{-1} \\tvy/2)}{\\sqrt{(2 \\pi)^{n+1} \\det(\\tmK)}}}{\\frac{\\exp(-\\vy^\\top \\mK^{-1} \\vy/2)}{\\sqrt{(2 \\pi)^{n} \\det(\\mK)}}}\n= \\frac{\\exp([-\\tvy^\\top \\tmK^{-1} \\tvy + \\vy^\\top \\mK^{-1} \\vy]/2)}{\\sqrt{(2 \\pi) \\det(\\tmK)/\\det(\\mK)}}\n\\end{align*}\\] where \\(\\tvy = (\\vy^\\top, z)^\\top\\)"
  },
  {
    "objectID": "slides/01-intro.html#bayesian-inference",
    "href": "slides/01-intro.html#bayesian-inference",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Bayesian inference",
    "text": "Bayesian inference\nBayesâ€™ rule for densities\n\\[\n\\varrho_{X\\mid Y}(x\\mid y)\n=\n\\frac{\\varrho_{Y\\mid X}(y\\mid x)\\,\\varrho_X(x)}{\\varrho_Y(y)}\n\\]\nLet \\[\\begin{align*}\nX & = \\text{random unknown parameter} \\\\\nY & = \\text{random observed data} \\\\\n\\varrho_X & = \\text{prior density or belief about the parameter} \\\\\n\\varrho_{Y\\mid X} & = \\text{likelihood} \\\\\n\\varrho_{X\\mid Y} & = \\text{posterior density}\n\\end{align*}\\] Then \\[\\begin{align*}\n\\varrho_{X\\mid Y}( \\text{parameter} \\mid \\text{data})\n& =\n\\frac{\\varrho_{Y\\mid X}(\\text{data}\\mid \\text{parameter} ) \\,\\varrho_X(\\text{parameter})}{\\varrho_Y(\\text{data})}\n\\\\\n\\text{posterior density } &\n\\propto\n\\text{likelihood} \\times \\text{prior density}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/01-intro.html#conditional-exp",
    "href": "slides/01-intro.html#conditional-exp",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Conditional expectation",
    "text": "Conditional expectation\nConditional expectation of \\(X\\) given event \\(A\\) (with \\(\\Prob(A)&gt;0\\)) is \\[\n\\Ex[X \\mid A]\n=\n\\frac{\\Ex[X \\, \\indic(X \\in A)]}{\\Prob(A)} \\qquad \\text{and so} \\qquad \\Ex[X \\, \\indic(X \\in A)]\n=\n\\Prob(A)\\,\\Ex[X \\mid A]\n\\] The indicator function, \\(\\indic(\\cdot)\\), equals \\(1\\) if the statement inside the parentheses is true, and \\(0\\) otherwise.\n\nConditional expectation of \\(X\\) given \\(Y=y\\) is \\[\n\\Ex[X \\mid Y=y]\n=\n\\int x \\, \\varrho_{X\\mid Y}(x \\mid y)\\,\\dif x\n\\] If \\(g(y)=\\Ex[X \\mid Y=y]\\), then \\(\\Ex[X \\mid Y] = g(Y)\\) is a random variable depending only on \\(Y\\)\n\n\nIf \\(X\\) is independent of \\(Y\\), then \\(\\Ex[X \\mid Y] = \\Ex[X]\\)"
  },
  {
    "objectID": "slides/01-intro.html#type-conv",
    "href": "slides/01-intro.html#type-conv",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Types of convergence",
    "text": "Types of convergence\nLet \\(X_1, X_2, \\ldots\\) be random variables and \\(X\\) another random variable\nAlmost Sure Convergence\n\\[\nX_n \\convas X \\iff \\Prob \\left( \\lim_{n \\to \\infty} X_n = X \\right) = 1\n\\] For almost every outcome the sequence \\(X_n\\) converges to \\(X\\)\nConvergence in Probability\n\\[\nX_n \\convp X \\iff \\forall \\varepsilon &gt; 0, \\;\n\\Prob\\big( |X_n - X| &gt; \\varepsilon \\big) \\;\\longrightarrow\\; 0.\n\\] The probability that \\(X_n\\) differs significantly from \\(X\\) goes to zero\nConvergence in Distribution\n\\[\nX_n \\convd X \\iff\n\\lim_{n \\to \\infty} F_{X_n}(x) = F_X(x)\n\\quad \\text{for all continuity points of } F_X\n\\] The distributions of \\(X_n\\) approach the distribution of \\(X\\)"
  },
  {
    "objectID": "slides/01-intro.html#clt",
    "href": "slides/01-intro.html#clt",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nIf \\(X_1, X_2, \\ldots \\IIDsim\\) some distribution with finite moment generating function, \\(M(t) := \\Ex\\bigl(\\exp(tX_1)\\bigr)\\), that exists for \\(t\\) near \\(0\\), and \\(\\mu = \\Ex(X_1)\\) and \\(\\sigma^2 = \\var(X_1)\\), and \\[\n\\barX_n := \\frac 1n \\left(X_1 + \\cdots + X_n \\right),\n\\] then \\[\n\\frac{\\barX_n - \\mu}{\\sigma /\\sqrt{n}} \\convd \\Norm(0,1)\n\\] Note that IID means independent and identically distributed"
  },
  {
    "objectID": "slides/01-intro.html#proof-of-the-central-limit-theorem",
    "href": "slides/01-intro.html#proof-of-the-central-limit-theorem",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Proof of the Central Limit Theorem",
    "text": "Proof of the Central Limit Theorem\n\nDefine a standardized (mean \\(0\\), variance \\(1\\)) random variable, \\(Y_i := (X_i - \\mu)/\\sigma\\) and note that \\(Y_1, Y_2, \\dots\\) are IID with mean \\(0\\) and variance \\(1\\):\n\n\\[\\begin{align*}\nZ_n: &= \\frac{\\barX_n - \\mu}{\\sigma /\\sqrt{n}} = \\frac{Y_1 + \\cdots + Y_n}{\\sqrt{n}} \\\\\nM_{Z_n}(t) &= \\Ex[\\exp(tZ_n)] = \\Ex[\\exp(tY_1/\\sqrt{n} + \\cdots + tY_n/\\sqrt{n})]\\\\\n& = \\Ex[\\exp(tY_1/\\sqrt{n}) \\cdots \\exp(tY_n/\\sqrt{n})] \\\\\n& =  \\{\\Ex[\\exp(tY_1/\\sqrt{n})]\\}^n \\qquad \\text{by independence} \\\\\n& = [M_Y(t/\\sqrt{n})]^n \\\\\n\\lim_{n \\to \\infty}M_{Z_n}(t) &= [M_Y(0)]^\\infty = 1^\\infty \\quad \\class{alert}{\\text{(undetermined)}}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/01-intro.html#chebyshev-markov-inequality",
    "href": "slides/01-intro.html#chebyshev-markov-inequality",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Chebyshev (Markov) Inequality",
    "text": "Chebyshev (Markov) Inequality\n\\[\n\\Prob\\bigl( f(X) \\ge r \\bigr) \\le \\frac{\\Ex[f(X)]}{r} \\qquad f \\text{ is }\\class{alert}{\\text{non-negative}}\n\\]\nProof\n\\[\\begin{align*}\n\\Prob(f(X)\\ge r)\n&= \\Ex\\bigl[\\indic\\bigl(f(X)\\ge r\\bigr)\\bigr] \\\\\n&= \\Ex\\Bigl[\\Ex\\bigl[\\indic\\bigl(f(X)\\ge r\\bigr)\\mid X\\bigr]\\Bigr]\n\\qquad \\text{(total expectation)} \\\\\n&\\le \\Ex\\Bigl[\\Ex\\bigl[f(X)/r \\mid X\\bigr]\\Bigr]\n\\qquad \\text{since } \\indic\\bigl(f(X)\\ge r\\bigr)\\le f(X)/r \\text{ a.s.} \\\\\n&= \\Ex\\bigl[f(X)/r\\bigr] \\\\\n&= \\Ex[f(X)]/r\n\\end{align*}\\]\nImportant special cases\n\\[\\begin{gather*}\n\\Prob\\bigl( \\lvert X - \\mu \\rvert^2 \\ge r^2 \\bigr) \\le \\frac{\\sigma^2}{r^2} \\quad \\text{where } \\mu = \\Ex(X), \\ \\sigma^2 = \\var(X) \\\\\n\\Prob\\bigl( \\lvert X - \\med(X) \\rvert \\ge r \\bigr) \\le \\frac{\\Ex (\\lvert X - \\med(X)\\rvert)}{r}\n\\end{gather*}\\]"
  },
  {
    "objectID": "slides/01-intro.html#normal-tails-vs-chebyshev-bounds",
    "href": "slides/01-intro.html#normal-tails-vs-chebyshev-bounds",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Normal tails vs Chebyshev bounds",
    "text": "Normal tails vs Chebyshev bounds\nLet \\(X\\sim \\Norm(0,1)\\) and compare \\(\\Prob(|X|&gt;r)\\) with two bounds\nHere \\(\\Ex\\lvert X \\rvert \\exeq \\sqrt{2/\\pi} \\approx 0.798\\) and \\(\\Ex(X^2)=1\\)\n\n\n\n\n\n\n\n\n\n\\(r\\)\n\\(\\Prob(|X|&gt;r)\\) (exact)\n\\(\\le \\sqrt{2/\\pi}\\,/r\\) (Markov)\n\\(\\le 1/r^2\\) (Chebyshev)\n\n\n\n\n1\n0.317311\n0.797885 (2.5Ã—)\n1 (3.2Ã—)\n\n\n2\n0.0455003\n0.398942 (8.8Ã—)\n0.25 (5.5Ã—)\n\n\n6\n1.97318e-09\n0.132981 (6.7e+07Ã—)\n0.0277778 (1.4e+07Ã—)\n\n\n10\n1.52397e-23\n0.0797885 (5.2e+21Ã—)\n0.01 (6.6e+20Ã—)\n\n\n100\n0\n0.00797885 (âˆž)\n0.0001 (âˆž)\n\n\n\n\nThe Markov and Chebyshev bounds are much looser, but more general\nCLT bounds for means will resemble the exact; we are willing to live with approximate bounds that are tighter than absolute bounds\nThese guaranteed Markov and Chebyshev bounds still depend on knowing or approximating \\(\\sigma^2\\) or \\(\\Ex \\lvert X - \\med(X)\\rvert\\)"
  },
  {
    "objectID": "slides/01-intro.html#student-t-tails-vs-markovchebyshev-bounds",
    "href": "slides/01-intro.html#student-t-tails-vs-markovchebyshev-bounds",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Student \\(t\\) tails vs Markov/Chebyshev bounds",
    "text": "Student \\(t\\) tails vs Markov/Chebyshev bounds\nLet \\(T_\\nu \\sim t_\\nu\\) and compare \\(\\Prob(|T_\\nu|&gt;r)\\) with two bounds\n\n\n\n\n\n\n\n\n\n\n\\(\\nu\\)\n\\(r\\)\n\\(\\Prob(|T_\\nu|&gt;r)\\) (exact)\n\\(\\le \\Ex(|T_\\nu|)/r\\) (Markov)\n\\(\\le \\Ex(T_\\nu^2)/r^2\\) (Chebyshev)\n\n\n\n\n2\n1\n0.423\n1.41 (3Ã—)\ninf (âˆž)\n\n\n2\n2\n0.184\n0.707 (4Ã—)\ninf (âˆž)\n\n\n2\n6\n0.0267\n0.236 (9Ã—)\ninf (âˆž)\n\n\n3\n1\n0.391\n1.1 (3Ã—)\n3 (8Ã—)\n\n\n3\n2\n0.139\n0.551 (4Ã—)\n0.75 (5Ã—)\n\n\n3\n6\n0.00927\n0.184 (20Ã—)\n0.0833 (9Ã—)\n\n\n5\n1\n0.363\n0.949 (3Ã—)\n1.67 (5Ã—)\n\n\n5\n2\n0.102\n0.475 (5Ã—)\n0.417 (4Ã—)\n\n\n5\n6\n0.00185\n0.158 (86Ã—)\n0.0463 (25Ã—)"
  },
  {
    "objectID": "slides/01-intro.html#weak-law-of-large-numbers-via-chebyshev",
    "href": "slides/01-intro.html#weak-law-of-large-numbers-via-chebyshev",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Weak Law of Large Numbers (via Chebyshev)",
    "text": "Weak Law of Large Numbers (via Chebyshev)\nTheorem (WLLN)\nLet \\(X_1,X_2,\\dots\\) be IID with \\(\\Ex[X_1]=\\mu\\) and \\(\\var(X_1)=\\sigma^2&lt;\\infty\\). Define the sample mean as \\(\\displaystyle\\barX_n := \\frac1n\\sum_{i=1}^n X_i\\). Then \\(\\barX_n \\convp \\mu\\).\nProof\nBy Chebyshevâ€™s inequality, \\(\\displaystyle \\Prob\\bigl(|\\barX_n-\\mu|\\ge \\varepsilon\\bigr)\n\\le \\frac{\\var(\\barX_n)}{\\varepsilon^2}\\)\nUsing independence, \\[\n\\var(\\barX_n)\n=\n\\var\\!\\left(\\frac1n\\sum_{i=1}^n X_i\\right)\n=\n\\frac{1}{n^2}\\sum_{i=1}^n \\var(X_i)\n=\n\\frac{1}{n^2}\\cdot n\\sigma^2\n=\n\\frac{\\sigma^2}{n}\n\\] Therefore, \\[\n\\Prob\\bigl(|\\barX_n-\\mu|\\ge \\varepsilon\\bigr)\n\\le\n\\frac{\\sigma^2}{n\\varepsilon^2}\n\\to 0 \\quad \\text{as } n \\to \\infty \\qquad \\square\n\\]"
  },
  {
    "objectID": "slides/01-intro.html#slutskys-theorem",
    "href": "slides/01-intro.html#slutskys-theorem",
    "title": "MATH 563 â€” Mathematical Statistics",
    "section": "Slutskyâ€™s Theorem",
    "text": "Slutskyâ€™s Theorem\nIf \\[\\begin{align*}\nX_n &\\convd X  \\quad \\text{and} \\\\\nY_n &\\convp c \\quad \\text{for some constant } c\n\\end{align*}\\] Then \\[\\begin{align*}\nX_n + Y_n &\\convd X + c \\\\\nX_n Y_n &\\convd cX \\\\\n\\frac{X_n}{Y_n} &\\convd \\frac{X}{c} \\quad \\text{if } c \\neq 0\n\\end{align*}\\]\n\nRandom quantities converging to constants behave like constants asymptotically"
  }
]