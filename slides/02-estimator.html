<!DOCTYPE html>
<html lang="en"><head>
<script src="02-estimator_files/libs/clipboard/clipboard.min.js"></script>
<script src="02-estimator_files/libs/quarto-html/tabby.min.js"></script>
<script src="02-estimator_files/libs/quarto-html/popper.min.js"></script>
<script src="02-estimator_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="02-estimator_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="02-estimator_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="02-estimator_files/libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.27">

  <meta name="author" content="Fred J. Hickernell">
  <meta name="dcterms.date" content="2026-02-28">
  <title>MATH 563 â€” Mathematical Statistics</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="02-estimator_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="02-estimator_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="02-estimator_files/libs/revealjs/dist/theme/quarto-f40efc9c2addd22605eff515b14f1811.css">
  <link href="02-estimator_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="02-estimator_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="02-estimator_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="02-estimator_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script>
    window.MathJax = {
      startup: {
        typeset: false,
        pageReady: () => {
          const typesetAndLayout = (() => {
            let busy = false;
            let queued = false;

            const run = async () => {
              if (busy) {
                queued = true;
                return;
              }
              busy = true;
              queued = false;

              try {
                await MathJax.typesetPromise();
                if (window.Reveal && typeof Reveal.layout === "function") {
                  Reveal.layout();
                }
              } finally {
                busy = false;
                if (queued) {
                  requestAnimationFrame(run);
                }
              }
            };

            return () => requestAnimationFrame(run);
          })();

          return MathJax.startup.defaultPageReady().then(() => {
            if (window.Reveal) {
              Reveal.on("ready", typesetAndLayout);
              Reveal.on("slidechanged", typesetAndLayout);
              Reveal.on("fragmentshown", typesetAndLayout);
              Reveal.on("fragmenthidden", typesetAndLayout);
            }
            typesetAndLayout();
          });
        }
      },
      
      svg: {
        mtextInheritFont: true,
        fontCache: "global"
      },
      
      tex: {
        macros: {

          mathlink: ["\\href{#1}{\\text{\\color{##0f8b8d}{#2}}}", 2],
          frag: ["{\\class{fragment}{#2}}", 2],
          notimplies: "\\ \\mathrel{\\not\\!\\!\\!\\Longrightarrow}",
          convas: ["\\xrightarrow{\\mathsf{a.s.}}", 0],
          convp:  ["\\xrightarrow{\\Prob}", 0],
          convd:  ["\\xrightarrow{\\mathsf{d}}", 0],

          nconvas: ["\\mathrel{\\;\\not\\!\\!\\!\\xrightarrow{\\mathsf{a.s.}}}", 0],
          nconvp:  ["\\mathrel{\\;\\not\\!\\xrightarrow{\\Prob}}", 0],
          nconvd:  ["\\mathrel{\\;\\not\\!\\xrightarrow{\\mathsf{d}}}", 0],

          asto: ["\\xrightarrow{\\mathsf{a.s.}}", 0],
          pto:  ["\\xrightarrow{\\Prob}", 0],
          dto:  ["\\xrightarrow{\\mathsf{d}}", 0],

          success: "{\\operatorname{succ}}",
          sinc:    "{\\operatorname{sinc}}",
          sech:    "{\\operatorname{sech}}",
          csch:    "{\\operatorname{csch}}",

          Prob: "{\\mathbb{P}}",
          Ex:   "{\\mathbb{E}}",


          dist:  "{\\operatorname{dist}}",
          spn:   "{\\operatorname{span}}",
          sgn:   "{\\operatorname{sgn}}",
          mse:   "{\\operatorname{mse}}",
          rmse:  "{\\operatorname{rmse}}",
          rank:  "{\\operatorname{rank}}",
          erfc:  "{\\operatorname{erfc}}",
          erf:   "{\\operatorname{erf}}",
          cov:   "{\\operatorname{cov}}",
          cost:  "{\\operatorname{cost}}",
          comp:  "{\\operatorname{comp}}",
          corr:  "{\\operatorname{corr}}",
          diag:  "{\\operatorname{diag}}",
          power: "{\\operatorname{power}}",
          var:   "{\\operatorname{var}}",
          opt:   "{\\operatorname{opt}}",
          brandnew: "{\\operatorname{new}}",
          std:   "{\\operatorname{std}}",
          se:    "{\\operatorname{se}}",
          kurt:  "{\\operatorname{kurt}}",
          med:   "{\\operatorname{med}}",
          vol:   "{\\operatorname{vol}}",
          bias:  "{\\operatorname{bias}}",
          RR:   "{\\mathrm{RR}}",

          Bern:  "{\\operatorname{Bern}}",
          Bin:   "{\\operatorname{Bin}}",
          Unif:  "{\\operatorname{Unif}}",
          Norm:  "{\\operatorname{N}}",
          Exp:   "{\\operatorname{Exp}}",
          Gam:   "{\\operatorname{Gamma}}",
          Pois:  "{\\operatorname{Pois}}",
          Geom:  "{\\operatorname{Geom}}",
          Cauchy:"{\\operatorname{Cauchy}}",
          Laplace:"{\\operatorname{Laplace}}",
          Beta:  "{\\operatorname{Beta}}",
          Weibull:"{\\operatorname{Weibull}}",
          Lognorm:"{\\operatorname{Lognormal}}",
          GP:     "{\\operatorname{GP}}",

          argmin: ["\\operatorname*{argmin}", 0],
          argmax: ["\\operatorname*{argmax}", 0],
          Argmin: ["\\argmin\\limits_{#1}", 1],
          Argmax: ["\\argmax\\limits_{#1}", 1],

          sign:  "{\\operatorname{sign}}",
          spann: "{\\operatorname{span}}",
          cond:  "{\\operatorname{cond}}",
          trace: "{\\operatorname{trace}}",
          Si:    "{\\operatorname{Si}}",
          col:   "{\\operatorname{col}}",
          nullspace: "{\\operatorname{null}}",
          Order: "{\\mathcal{O}}",

          IIDsim: "\\mathrel{\\stackrel{\\mathrm{IID}}{\\sim}}",
          LDsim:  "\\mathrel{\\stackrel{\\mathrm{LD}}{\\sim}}",
          appxsim: "\\mathrel{\\stackrel{\\cdot}{\\sim}}",

          naturals:  "{\\mathbb{N}}",
          natzero:   "{\\mathbb{N}_0}",
          integers:  "{\\mathbb{Z}}",
          rationals: "{\\mathbb{Q}}",
          reals:     "{\\mathbb{R}}",
          complex:   "{\\mathbb{C}}",
          bbone:     "{\\mathbb{1}}",
          indic:     "{\\mathop{\\mathchoice{\\large\\mathbb{1}}{\\large\\mathbb{1}}{\\mathbb{1}}{\\mathbb{1}}}}",  // indic: enlarged blackboard-bold indicator (uses mathchoice so subscripts scale)

          abs:  ["{\\left\\lvert #1 \\right\\rvert}", 1],
          norm: ["{\\left\\lVert #1 \\right\\rVert}", 1],
          ip:   ["{\\left\\langle #1, #2 \\right\\rangle}", 2],
          dim:   "{\\operatorname{dim}}",
          df:   "{\\mathrm{df}}",


          bvec: ["{\\boldsymbol{#1}}", 1],
          avec: ["{\\vec{#1}}", 1],
          vecsym: ["{\\boldsymbol{#1}}", 1],

          vf:  "{\\boldsymbol{f}}",
          tvf: "{\\widetilde{\\boldsymbol{f}}}",
          vk:  "{\\boldsymbol{k}}",
          vt:  "{\\boldsymbol{t}}",
          vT:  "{\\boldsymbol{T}}",
          vx:  "{\\boldsymbol{x}}",
          vX:  "{\\boldsymbol{X}}",
          vy:  "{\\boldsymbol{y}}",
          tvy: "{\\widetilde{\\boldsymbol{y}}}",
          vY:  "{\\boldsymbol{Y}}",
          vz:  "{\\boldsymbol{z}}",
          vZ:  "{\\boldsymbol{Z}}",

          tg:  "{\\widetilde{g}}",
          th:  "{\\widetilde{h}}",

          valpha: "{\\boldsymbol{\\alpha}}",
          vbeta:  "{\\boldsymbol{\\beta}}",
          vgamma: "{\\boldsymbol{\\gamma}}",
          vdelta: "{\\boldsymbol{\\delta}}",
          vepsilon: "{\\boldsymbol{\\epsilon}}",
          vlambda:  "{\\boldsymbol{\\lambda}}",
          vsigma:   "{\\boldsymbol{\\sigma}}",
          vtheta:   "{\\boldsymbol{\\theta}}",
          vTheta:   "{\\boldsymbol{\\Theta}}",
          vomega:   "{\\boldsymbol{\\omega}}",
          vpi:      "{\\boldsymbol{\\pi}}",
          vphi:     "{\\boldsymbol{\\phi}}",
          vPhi:     "{\\boldsymbol{\\Phi}}",
          vmu:      "{\\boldsymbol{\\mu}}",
          vnu:     "{\\boldsymbol{\\nu}}",

          htheta: "{\\widehat{\\theta}}",
          hTheta: "{\\widehat{\\Theta}}",
          hbeta:  "{\\widehat{\\beta}}",
          hlambda: "{\\widehat{\\lambda}}",
          hmu:    "{\\widehat{\\mu}}",
          hsigma: "{\\widehat{\\sigma}}",
          hSigma: "{\\widehat{\\Sigma}}",
          hP:     "{\\widehat{P}}",
          hX:     "{\\widehat{X}}",
          hY:     "{\\widehat{Y}}",
          hZ:     "{\\widehat{Z}}",

          ct: "{\\mathcal{T}}",
          cx: "{\\mathcal{X}}",

          vzero: "{\\boldsymbol{0}}",
          vone:  "{\\boldsymbol{1}}",
          vinf:  "{\\boldsymbol{\\infty}}",
          
          barD: "{\\overline{D}}",
          barx: "{\\overline{x}}",
          barX: "{\\overline{X}}",
          barY: "{\\overline{Y}}",
          bary: "{\\overline{y}}",
          barZ: "{\\overline{Z}}",

          me:  "{\\mathrm{e}}",
          mi:  "{\\mathrm{i}}",
          mpi: "{\\mathrm{\\pi}}",
          mK: "{\\mathsf{K}}",
          tmK: "{\\widetilde{\\mathsf{K}}}",
          mSigma: "{\\mathsf{\\Sigma}}",

          dif: "{\\mathrm{d}}",
          IID: "{\\mathrm{IID}}",
          MLE: "{\\mathrm{MLE}}",

          exstar: "{\\mathop{\\mathchoice{\\color{gold}{\\Large\\star}}{\\color{gold}{\\Large\\star}}{\\color{gold}{\\large\\star}}{\\color{gold}{\\star}}}}",
          exeq: "{\\mathrel{\\,\\overset{\\exstar}{=}\\,}}",
          exsim: "{\\mathrel{\\,\\overset{\\exstar}{\\sim}\\,}}",
          
        } 
      }
    };
  </script>

  <script id="course-website-meta" type="application/json">
  {
    "course_website": "https://fjhickernell.github.io/MATH563Spring2026/",
    "deck_title": ""
  }
  </script>

  <script>
  (function () {

    function readMeta() {
      const el = document.getElementById("course-website-meta");
      if (!el) return {};
      try {
        return JSON.parse(el.textContent || "{}");
      } catch (e) {
        return {};
      }
    }

    function clean(v) {
      if (!v || typeof v !== "string") return "";
      // Quarto leaves unresolved meta as strings containing "meta "
      if (v.includes("meta ")) return "";
      return v.trim();
    }

    function setFooterCourseWebsite() {
      const meta = readMeta();

      const url = clean(meta.course_website);
      const deckTitle = clean(meta.deck_title);

      const a = document.getElementById("fh-course-website");
      if (!a) return;

      if (url) a.setAttribute("href", url);

      if (deckTitle) {
        a.textContent = deckTitle + " Â· Course Website";
      }
    }

    function insertCourseWebsiteOnTitleSlide() {
      const meta = readMeta();
      const url = clean(meta.course_website);
      if (!url) return;

      const titleSlide = document.querySelector(".reveal #title-slide");
      if (!titleSlide) return;

      if (titleSlide.querySelector(".course-website")) return;

      const p = document.createElement("p");
      p.className = "course-website";
      p.innerHTML = `<a href="${url}">${url}</a>`;

      const authorBlock =
        titleSlide.querySelector(".quarto-title-authors") ||
        titleSlide.querySelector(".quarto-title-author") ||
        titleSlide.querySelector(".quarto-title-author-name");

      const subtitle = titleSlide.querySelector("p.subtitle");

      if (authorBlock && authorBlock.parentNode) {
        authorBlock.parentNode.insertBefore(p, authorBlock);
      } else if (subtitle && subtitle.parentNode) {
        subtitle.parentNode.insertBefore(p, subtitle.nextSibling);
      } else {
        titleSlide.appendChild(p);
      }
    }

    function applyAll() {
      setFooterCourseWebsite();
      insertCourseWebsiteOnTitleSlide();
    }

    function hookReveal() {
      if (window.Reveal && typeof window.Reveal.on === "function") {
        window.Reveal.on("ready", applyAll);
        window.Reveal.on("slidechanged", applyAll);
        applyAll();
        return;
      }

      let tries = 0;
      const t = setInterval(function () {
        applyAll();
        tries += 1;
        if (tries >= 20) clearInterval(t);
      }, 250);
    }

    if (document.readyState === "loading") {
      document.addEventListener("DOMContentLoaded", hookReveal);
    } else {
      hookReveal();
    }

  })();
  </script>

</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-color="#f5f7fb" data-background-image="https://fjhickernell.github.io/MATH563Spring2026/classlib/classlib/quarto/assets/images/normal-scatter.png" data-background-position="top left" data-background-size="55%" class="quarto-title-block center">
  <h1 class="title">MATH 563 â€” Mathematical Statistics</h1>
  <p class="subtitle"></p><p><span class="deck-title">Estimators and Confidence Intervals</span><br> <span class="deck-meta">Casella &amp; Berger Ch. 5â€“7 (parts) <br> Assignment 3 due 2/13 <br> Test 1 on 2/26 on Probability Review, Estimators, &amp; Confidence Intervals </span></p><p></p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Fred J. Hickernell 
</div>
</div>
</div>

  <p class="date">February 28, 2026</p>
</section>
<section id="how-to-learn-this-subject" class="title-slide slide level1 center">
<h1>How to Learn This Subject</h1>
<ul>
<li>Make sure you understand the slides</li>
<li>Ask questions for clarification</li>
<li>Do the assignments</li>
<li>Work out problems in the text that are not assigned, or convince yourself that you could do them</li>
<li>Practice on old tests</li>
<li>Do the <span class="math inline">\(\exstar\)</span> exercises</li>
<li>Make up questions and quiz your friends</li>
</ul>
</section>

<section>
<section id="estimatorsestimates" class="title-slide slide level1 center">
<h1>Estimators/Estimates</h1>
<p><span class="alert">Estimator:</span> a random variable (or function of the sample) used to approximate an unknown parameter <span class="alert">Estimate:</span> the realized numerical value of an estimator after observing the data</p>
<ul>
<li><a href="#/summary-statistics">Summary statistics</a></li>
<li><a href="#/maximum-likelihood-estimators">Maximum likelihood estimators (MLE)</a></li>
<li><a href="#/plug-in-estimators">Plug-in estimators</a></li>
</ul>
</section>
<section id="summary-statistics" class="slide level2">
<h2>Summary statistics</h2>
<p><span class="small">(CB Â§5.3, Â§5.4, Â§6.1; WMS Â§6.7, Â§8.1, Â§9.6)</span></p>
<p>Given IID data, <span class="math inline">\(X_1, \ldots, X_n\)</span>, we often compute</p>
<ul>
<li><p><span class="alert">Empirical Distribution</span> <span class="math inline">\(F_{\{X_i\}}(x) := \frac 1n \sum_{i=1}^n \indic(X_i \le x)\)</span></p></li>
<li><p><span class="alert">Sample Mean</span> <span class="math inline">\(\displaystyle \barX = \barX_n := \frac{1}{n} \sum_{i=1}^n X_i = \int x \, \dif F_{\{X_i\}}(x) = \Ex_{F_{\{X_i\}}}(X)\)</span> to approximate the population mean <span class="math inline">\(\mu := \Ex[X_1]\)</span></p></li>
<li><p><span class="alert">Sample Variance</span> <span class="math inline">\(S^2 =S^2_n := \displaystyle \frac{1}{n-1} \sum_{i=1}^n (X_i - \barX_n)^2\)</span> to approximate the population variance <span class="math inline">\(\sigma^2 := \var(X_1) := \Ex[(X_1-\mu)^2]\)</span></p>
<ul>
<li>Sometimes <span class="math inline">\(\hsigma^2 = \hsigma^2_n := \displaystyle \frac{1}{\class{alert}{n}} \sum_{i=1}^n (X_i - \barX_n)^2\)</span></li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><span class="alert">Order Statistics</span> <span class="math inline">\(X_{(1)}, X_{(2)}, \ldots\)</span>, reorder the data so that <span class="math display">\[ X_{(1)} \le X_{(2)} \le \cdots \le X_{(n)}, \qquad \text{i.e., } X_{(i)} = Q_{\{X_i\}}(i/n)
\]</span> where <span class="math inline">\(Q_{\{X_i\}}\)</span> is the <a href="../slides/01-intro.html#quantile-function">quantile function</a> corresponding to the empirical distribution
<ul>
<li><span class="math inline">\(X_{(1)}\)</span> is the minimum and <span class="math inline">\(X_{(n)}\)</span> is the maximum of the data</li>
<li><span class="math inline">\(X_{(i)}\)</span> is often an estimator of the population quantile <span class="math inline">\(Q_X(p)\)</span> for <span class="math inline">\(p \approx i/(n+1)\)</span> or <span class="math inline">\((i-1/2)/n\)</span></li>
</ul></li>
</ul>
<p>Given IID data, <span class="math inline">\((X_1, Y_1), \ldots, (X_n,Y_n)\)</span>, with sample mean <span class="math inline">\((\barX_n, \barY_n)\)</span>, we often compute</p>
<ul>
<li><p><span class="alert">Sample Covariance</span> <span class="math inline">\(\displaystyle  S_{XY} := \frac{1}{n-1} \sum_{i=1}^n (X_i - \barX_n)(Y_i - \barY_n)\)</span> to approximate the population covariance <span class="math inline">\(\cov(X_1,Y_1) := \Ex[(X_1 - \mu_X)(Y_1 - \mu_Y)]\)</span></p></li>
<li><p><span class="alert">Sample Correlation</span> <span class="math inline">\(\displaystyle  R_{XY} := \frac{S_{XY}}{\sqrt{S^2_X S^2_Y}}\)</span> to approximate the population correlation <span class="math inline">\(\displaystyle \corr(X_1,Y_1) := \frac{\cov(X_1,Y_1)}{\sigma_X \sigma_Y}\)</span></p></li>
</ul>
</section>
<section id="maximum-likelihood-estimators" class="slide level2">
<h2>Maximum likelihood estimators</h2>
<p><span class="small">(CB Â§7.2.2; WMS Â§9.7)</span></p>
<p>The joint density of <span class="alert">data</span>, <span class="math inline">\(\vX = (X_1, \ldots, X_n)^\top\)</span> given a parameter, <span class="math inline">\(\vtheta\)</span>, is <span class="math inline">\(\varrho_{\vX \mid  \vtheta}\)</span>. The <span class="alert">likelihood</span>, <span class="math inline">\(L\)</span> turns that around to make the parameter the variable, so <span class="math display">\[
L(\vtheta  \mid  \vx) := \varrho_{\vX  \mid  \vtheta}(\vx); \qquad L(\vtheta  \mid  \vx)  = \prod_{i=1}^n \varrho_{X_1  \mid \vtheta}(x_i) \quad \text{if } X_1, \ldots, X_n \text{ are } \IID
\]</span></p>
<p>The <span class="alert">maximum likelihood estimator (MLE)</span> of <span class="math inline">\(\vtheta\)</span> is the one that fits the observed data best in terms of <span class="math display">\[
\vTheta_{\MLE}  = \Argmax{\vtheta} L(\vtheta  \mid  \vX)
\]</span></p>
<p>It may be easier to work with the <span class="alert">log-likelihood</span> <span class="math inline">\(\ell(\vtheta \mid \vX) := \log(L(\vtheta \mid \vX))\)</span> since the logarithm is a monotone transformation, so <span class="math display">\[
\vTheta_{\MLE}  = \Argmax{\vtheta} \ell(\vtheta  \mid  \vX)
\]</span></p>
</section>
<section class="slide level2">

<p><span class="math inline">\(\exstar\)</span> What is the MLE of <span class="math inline">\(p\)</span> for the distribution <span class="math inline">\(\Bern(p)\)</span>?</p>
<div class="exitem">
<p><span class="exbullet"><span class="math inline">\(\exstar\)</span></span><span>What is the MLE of <span class="math inline">\(\lambda\)</span> for <span class="math inline">\(\Exp(\lambda)\)</span>? What are the MLE of <span class="math inline">\(\mu=\Ex(X)\)</span> and <span class="math inline">\(\sigma^2=\var(X)\)</span> for <span class="math inline">\(X\sim\Exp(\lambda)\)</span>?</span></p>
</div>
<p><span class="math inline">\(\exstar\)</span> What are the MLE of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> for <span class="math inline">\(X \sim \Norm(\mu,\sigma^2)\)</span>?</p>
</section>
<section id="plug-in-estimators" class="slide level2">
<h2>Plug-in estimators</h2>
<ul>
<li>If <span class="math inline">\(\hTheta_1\)</span> is an estimator of <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2 = g(\theta_1)\)</span>, then <span class="math inline">\(\hTheta_2 : = g(\hTheta_1)\)</span> is a <span class="alert">plug-in estimator</span> of <span class="math inline">\(\theta_2\)</span></li>
<li>If <span class="math inline">\(\hTheta_1\)</span> is MLE of <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2 = g(\theta_1)\)</span>, then <span class="math inline">\(\hTheta_2 : = g(\hTheta_1)\)</span> is an MLE of <span class="math inline">\(\theta_2\)</span></li>
</ul>
</section></section>
<section>
<section id="properties-of-estimators" class="title-slide slide level1 center">
<h1>Properties of Estimators</h1>
<ul>
<li><a href="#/biasvar">Bias and variance</a></li>
<li><a href="#/distribest">Distributions of estimators</a></li>
<li><a href="#/consistency">Consistency</a></li>
</ul>
</section>
<section id="biasvar" class="slide level2">
<h2>Bias, variance, and mean squared error of estimators</h2>
<p><span class="small">(CB Â§7.3.1; WMS Â§Â§8.2â€“8.4)</span></p>
<p>Suppose that <span class="math inline">\(\Theta\)</span> is an estimator of a parameter, <span class="math inline">\(\theta\)</span>, of a population</p>
<ul>
<li><p><span class="alert">Bias</span> <span class="math inline">\(\bias(\Theta) = \Ex(\Theta) - \theta\)</span></p>
<ul>
<li>Asymptotic bias is <span class="math inline">\(\displaystyle \lim_{n \to \infty} \bias(\Theta_n)\)</span>, where <span class="math inline">\(n\)</span> is the size of the sample on which the estimator is based</li>
<li>An estimator is <span class="alert">unbiased</span> if <span class="math inline">\(\bias(\Theta) = 0\)</span></li>
<li><span class="math inline">\(\barX_n\)</span> is an unbiased estimator of <span class="math inline">\(\mu = \Ex(X_1)\)</span> for identically distributed data</li>
</ul></li>
<li><p><span class="alert">Variance</span> we already know this definition</p>
<ul>
<li><span class="math inline">\(\var(\barX_n) \exeq \var(X_1)/n\)</span> for uncorrelated, identically distributed data</li>
</ul></li>
<li><p><span class="alert">Mean squared error</span> <span class="math inline">\(\mse(\Theta) := \Ex[(\Theta - \theta)^2] \exeq [\bias(\Theta)]^2 + \var(\Theta)\)</span></p></li>
<li><p><span class="alert">Standard Error</span> <span class="math inline">\(\se(\Theta) := \sqrt{\var(\Theta)}\)</span> is the standard deviation of the sampling distribution of <span class="math inline">\(\Theta\)</span></p>
<ul>
<li><span class="math inline">\(\se(\barX_n) = \sqrt{\var(\barX_n)}\)</span></li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<p><span class="math inline">\(\exstar\)</span> Show that <span class="math inline">\(S^2 := \displaystyle \frac{1}{n-1} \sum_{i=1}^n (X_i - \barX_n)^2\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span></p>
<p><span class="math inline">\(\exstar\)</span> Show that <span class="math inline">\(S = \sqrt{S^2}\)</span> as an estimator of <span class="math inline">\(\sigma\)</span> has negative bias (see Jensenâ€™s inequality)</p>
<p><span class="math inline">\(\exstar\)</span> Is the MLE of <span class="math inline">\(\sigma=\std(X)\)</span> for <span class="math inline">\(X\sim\Exp(\lambda)\)</span> unbiased?</p>
<p><span class="math inline">\(\exstar\)</span> What is the MLE <span class="math inline">\(\theta\)</span> of <span class="math inline">\(\theta\)</span> for <span class="math inline">\(X \sim \Unif(0,\theta)\)</span>?</p>
</section>
<section id="distribest" class="slide level2">
<h2>Distributions of estimators (see <a href="../slides/01-intro.html#prob-distrib">Important Distributions</a>)</h2>
<p><span class="small">(CB Â§5.2â€“5.4; WMS Â§Â§7.2)</span></p>
<p>For the sample mean <span class="math inline">\(\barX_n\)</span>, based on IID data</p>
<ul>
<li><p><span class="math inline">\(n \barX_n \sim \Bin(n,p)\)</span> if <span class="math inline">\(X \sim \Bern(p)\)</span></p></li>
<li><p><span class="math inline">\(\barX_n \exsim \Gam(n, n \lambda)\)</span> if <span class="math inline">\(X \sim \Exp(\lambda)\)</span> where <span class="math inline">\(\displaystyle \varrho_{\Gam(\alpha, \beta)}(x) = \frac{\beta^{\alpha}}{\Gamma(\alpha)}\, x^{\alpha-1} \exp(-\beta x)
\quad x&gt;0\)</span></p>
<ul>
<li>Note: <span class="math inline">\(\Gamma(n) = (n-1)!\)</span> for integer <span class="math inline">\(n\)</span></li>
<li><span class="math inline">\(\Gam(\alpha, \beta)\)</span> is the generic gamma distribution with shape <span class="math inline">\(\alpha\)</span> and rate <span class="math inline">\(\beta\)</span></li>
<li><span class="math inline">\(\lambda n \barX_n \exsim \Gam(n, 1)\)</span></li>
<li><span class="math inline">\(2 \lambda n \barX_n \exsim \chi^2_{2n}\)</span>, where <span class="math inline">\(\displaystyle \varrho_{\chi^2_\nu}(x) = \frac{x^{\nu/2 - 1} \exp(-x/2)}{2^{\nu/2} \Gamma(\nu/2)} \quad x &gt; 0\)</span></li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<p>For the sample mean <span class="math inline">\(\barX_n\)</span>, based on IID data (contâ€™d)</p>
<ul>
<li><p><span class="math inline">\(\barX_n \exsim \Norm(\mu,\sigma^2/n)\)</span> if <span class="math inline">\(X \sim \Norm(\mu,\sigma^2)\)</span></p></li>
<li><p><span class="math inline">\(\barX_n \appxsim \Norm(\mu,\sigma^2/n)\)</span> for arbitrary distributions and large <span class="math inline">\(n\)</span> by the <span class="alert">Central Limit Theorem</span></p></li>
<li><p><span class="math inline">\(\displaystyle \frac{\barX_n - \mu}{S_n/\sqrt{n}} \sim t_{n-1}\)</span> if <span class="math inline">\(X \sim \Norm(\mu,\sigma^2)\)</span> where</p>
<ul>
<li><p><span class="math inline">\(\displaystyle S_n^2 := \frac{1}{n-1} \sum_{i=1}^n (X_i - \barX_n)^2\)</span></p></li>
<li><p><span class="math inline">\(t_\nu\)</span> is the <span class="alert">Studentâ€™s t distribution</span> with <span class="math inline">\(\nu\)</span> degrees of freedom</p>
<ul>
<li><span class="math inline">\(\displaystyle \varrho_{t_\nu}(x) = \frac{\Gamma\!\left(\frac{\nu+1}{2}\right)}{\sqrt{\nu\pi}\,\Gamma\!\left(\frac{\nu}{2}\right)}\left(1+\frac{x^2}{\nu}\right)^{-(\nu+1)/2}, \quad -\infty &lt; x &lt; \infty\)</span></li>
<li>Symmetric about <span class="math inline">\(0\)</span></li>
<li>Heavier tails than the standard normal</li>
<li><span class="math inline">\(\exstar\)</span> Converges to <span class="math inline">\(N(0,1)\)</span> as <span class="math inline">\(\nu \to \infty\)</span></li>
</ul></li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<p>For the unbiased sample variance, <span class="math inline">\(S_n^2\)</span>, for <span class="alert"><span class="math inline">\(\Norm(\mu,\sigma^2)\)</span></span> based on IID data</p>
<ul>
<li><span class="math inline">\(\displaystyle \frac{(n-1) S_n^2}{\sigma^2} \sim \chi^2_{n-1}\)</span></li>
</ul>
<p>&nbsp;</p>
<p>For order statistics, <span class="math inline">\(X_{(k)}\)</span>, <span class="math inline">\(\displaystyle F_{X_{(k)}}(x) = \sum_{j=k}^n \binom{n}{j} [F_X(x)]^j [1 - F_X(x)]^{n-j}\)</span> for IID data from CDF <span class="math inline">\(F_X\)</span></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><span class="math inline">\(\exstar\)</span> Is the MLE <span class="math inline">\(\theta\)</span> of <span class="math inline">\(\theta\)</span> for <span class="math inline">\(X \sim \Unif(0,\theta)\)</span> unbiased? Can you modify it to be unbiased?</p>
</section>
<section id="consistency" class="slide level2">
<h2>Consistency of estimators</h2>
<p><span class="small">(CB Â§10.1; WMS Â§9.3)</span></p>
<p>Let <span class="math inline">\(\Theta_n\)</span> be an estimate <span class="math inline">\(\theta\)</span> based on a sample of size <span class="math inline">\(n\)</span>. This estimator is <span class="alert">consistent</span> if <span class="math display">\[
\Theta_n \pto \theta \quad \text{as } n \to \infty
\]</span></p>
<p>This is automatic if <span class="math inline">\(\Theta_n\)</span> is (asymptotically) unbiased and its variance vanishes as <span class="math inline">\(n \to \infty\)</span></p>
<div class="key-point">
<p>The sample mean is a consistent estimator of the population mean if the variance of the data is finite</p>
</div>
</section>
<section id="exp-families-estimators" class="slide level2">
<h2>Estimators for <a href="01-intro.html#exp-families">exponential families</a> of distributions</h2>
<p>Recall that <a href="01-intro.html#exp-families">exponential families</a> of distributions have PMF or PDF of the form <span class="math display">\[
\varrho(x ; \vtheta) = h(\vx) \, c(\vtheta) \, \exp \biggl(\sum_k w_k(\vtheta) t_k(x) \biggr) \quad x \in \reals
\]</span> The binomial, exponential, and normal are all exponential families</p>
<h3 id="likelihood-equations-depend-on-sample-averages">1. Likelihood equations depend on sample averages</h3>
<p>The log-likelihood for IID data <span class="math inline">\(\vX = (X_1,\dots,X_n)\)</span> is <span class="math display">\[
\ell(\vtheta \mid \vX) : = \log(L(\vtheta \mid \vX))
= \sum_{i=1}^n \log \bigl(h(X_i) \bigr) +   n \log \bigl( c(\vtheta) \bigr) +\sum_{k} w_k(\vtheta)\Biggl[ \sum_{i=1}^{n} t_k(X_i) \Biggr]
\]</span> Solving for the MLE depends on the data only through the averages: <span class="math inline">\(\displaystyle \frac 1n \sum_{i=1}^{n} t_k(X_i)\)</span></p>
<ul>
<li><span class="math inline">\(\Bern(\mu)\)</span> and <span class="math inline">\(\Exp(\lambda)\)</span>: <span class="math inline">\(\displaystyle \frac 1n \sum_{i=1}^{n} X_i\)</span>, &nbsp; &nbsp; &nbsp; <span class="math inline">\(\Norm(\mu,\sigma^2)\)</span>: <span class="math inline">\(\displaystyle \frac 1n \sum_{i=1}^{n} X_i\)</span> and <span class="math inline">\(\displaystyle \frac 1n \sum_{i=1}^{n} X_i^2\)</span></li>
</ul>
</section>
<section class="slide level2">

<h3 id="sampling-distributions-are-often-known">2. Sampling distributions are often known</h3>
<p>Because sums or averages of exponential family data have known distributions, the distribution of the estimator is often explicit</p>
<ul>
<li><span class="math inline">\(\Bern(p)\)</span>: <span class="math inline">\(n\barX \sim \text{Binomial}(n,p)\)</span></li>
<li><span class="math inline">\(\Exp(\lambda)\)</span>: <span class="math inline">\(\barX \sim \Gam(n,\;n\lambda)\)</span> (shapeâ€“rate)</li>
<li><span class="math inline">\(\Norm(\mu,\sigma^2)\)</span>: <span class="math inline">\(\barX \sim \Norm(\mu,\sigma^2/n)\)</span></li>
</ul>
<p>This buys us</p>
<ul>
<li>Exact bias and variance, e.g.&nbsp;for <span class="math inline">\(X \sim \Exp(\lambda)\)</span>, <span class="math inline">\(\hlambda_{\MLE} \exeq 1/\barX\)</span> and <span class="math inline">\(\displaystyle \Ex(\hlambda_{\MLE}) \exeq \frac{n \lambda}{n-1}\)</span> for <span class="math inline">\(n &gt; 1\)</span></li>
<li>Exact or near-exact confidence intervals</li>
</ul>
</section>
<section class="slide level2">

<h3 id="bias-behavior-is-transparent">3. Bias behavior is transparent</h3>
<p>Although many MLEs are asymptotically unbiased, they are biased for finite sample size, but the bias is computable</p>
<p>&nbsp;</p>
<p>Note that</p>
<p><span class="math display">\[\theta_2 = g(\theta_1) \implies \Theta_{2,\MLE} = g(\Theta_{1,\MLE})\]</span></p>
<p>But</p>
<p><span class="math display">\[ \Theta_{1,\MLE} \text{ unbiased } \notimplies \Theta_{2,\MLE} \text{ unbiased}\]</span></p>
</section></section>
<section>
<section id="confidence-intervals" class="title-slide slide level1 center">
<h1>Confidence Intervals</h1>
<p><span class="small">(CB Â§9.1; WMS Â§8.5)</span></p>
<p>If</p>
<ul>
<li><p><span class="math inline">\(\theta\)</span> is a parameter of interest of a distribution, and</p></li>
<li><p><span class="math inline">\(X_1, \ldots, X_n\)</span> are data that we assume are collected from that distribution,</p></li>
</ul>
<p>then we try to construct random quantities <span class="math inline">\(\Theta_L\)</span> and/or <span class="math inline">\(\Theta_U\)</span>, depending only on the data (and not on <span class="math inline">\(\theta\)</span>), that give intervals which <span class="alert">capture</span> <span class="math inline">\(\theta\)</span> with high probability <span class="math inline">\(1-\alpha\)</span>. Depending on the situation, this means constructing</p>
<ul>
<li><p>a two-sided interval with <span class="math inline">\(\Prob(\Theta_L \le \theta \le \Theta_U) \ge 1-\alpha\)</span>, or</p></li>
<li><p>a one-sided lower interval with <span class="math inline">\(\Prob(\Theta_L \le \theta) \ge 1-\alpha\)</span>, or</p></li>
<li><p>a one-sided upper interval with <span class="math inline">\(\Prob(\theta \le \Theta_U) \ge 1-\alpha\)</span></p></li>
</ul>
<p>The bounds <span class="math inline">\(\Theta_L\)</span> and <span class="math inline">\(\Theta_U\)</span> are random because they depend on random data. Here <span class="math inline">\(\alpha\)</span> is our willingness to be wrong, typically <span class="math inline">\(\alpha = 5\%\)</span>.</p>
</section>
<section class="slide level2">

<p>More about confidence intervals:</p>
<ul>
<li>In many continuous cases, the probability is exactly <span class="math inline">\(1-\alpha\)</span></li>
<li>For discrete distributions, the probability is often slightly larger than <span class="math inline">\(1-\alpha\)</span></li>
</ul>
<p>This process often proceeds by</p>
<ul>
<li>Identifying a <span class="alert">estimator</span> <span class="math inline">\(\Theta\)</span> for <span class="math inline">\(\theta\)</span> that depends only on the data</li>
<li>Finding the the <span class="alert">sampling distribution</span> of the estimator <span class="math inline">\(\Theta\)</span></li>
<li>Using the sampling distribution to find <span class="alert">quantiles</span> that give the desired coverage probability</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><a href="#/critical-value-notation-important">Critical value notation</a></li>
<li><a href="#/large-sample-confidence-intervals-for-means">Large sample size confidence intervals for means</a></li>
<li><a href="#/small-sample-confidence-intervals-for-means-when-the-distribution-is-known">Small sample size confidence intervals for means when the distribution is known</a></li>
<li><a href="#/confidence-intervals-for-means-of-differences">Confidence intervals for means of differences</a></li>
<li><a href="#/confidence-intervals-for-proportions">Confidence intervals for proportions</a></li>
<li><a href="#/confidence-intervals-for-variances">Confidence intervals for variances</a></li>
<li><a href="#/bootstrap-confidence-intervals">Bootstrap confidence intervals</a></li>
<li><a href="#/ci-assumptions">Summary of common confidence intervals and their assumptions</a></li>
</ul>
</section>
<section id="critical-value-notation-important" class="slide level2">
<h2>Upper critical values</h2>
<p>For a distribution with CDF <span class="math inline">\(F\)</span> and quantile function <span class="math inline">\(Q\)</span>, define the upper critical value <span class="math display">\[
c_{\alpha} := Q(1-\alpha),  \quad \text{i.e., } F(c_{\alpha}) \ge 1-\alpha \text{ and } F(c_{\alpha} - \epsilon) &lt; 1-\alpha \; \forall \epsilon &gt; 0
\]</span></p>
<div class="columns">
<div class="column" style="width:60%;">
<p>Examples</p>
<ul>
<li><span class="math inline">\(z_{\alpha} = Q_{\Norm(0,1)}(1-\alpha)\)</span><br>
</li>
<li><span class="math inline">\(t_{\nu,\alpha} = Q_{t_\nu}(1-\alpha)\)</span><br>
</li>
<li><span class="math inline">\(\chi^2_{\nu,\alpha} = Q_{\chi^2_\nu}(1-\alpha)\)</span></li>
</ul>
<p>These <span class="alert">upper critical values</span> are <em>not</em> <span class="math inline">\(\alpha\)</span>-quantiles.</p>
</div><div class="column" style="width:40%;">
<div id="8cb3678e" class="cell" data-fig-height="3.6" data-fig-width="4.8" data-execution_count="1">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="02-estimator_files/figure-revealjs/cell-2-output-1.png" width="950" height="469"></p>
</figure>
</div>
</div>
</div>
</div></div>
</section>
<section id="large-sample-confidence-intervals-for-means" class="slide level2">
<h2>Large sample size confidence intervals for means</h2>
<p><span class="small">(CB Â§9.2; WMS Â§Â§8.6â€“8.7)</span></p>
<p>If <span class="math inline">\(X_1, \ldots, X_n\)</span> are IID with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2 &lt; \infty\)</span>, and</p>
<ul>
<li><p><span class="math inline">\(\barX_n\)</span> is the sample mean,</p></li>
<li><p><span class="math inline">\(S_n^2\)</span> is some estimate of the unknown population variance <span class="math inline">\(\sigma^2\)</span> (e.g., unbiased or MLE)</p></li>
</ul>
<p>then by the <a href="../slides/01-intro.html#clt">Central Limit Theorem</a> <span class="math display">\[
\frac{\barX_n - \mu}{\sigma/\sqrt{n}} \appxsim \Norm(0,1) \quad \text{for large } n
\]</span></p>
</section>
<section class="slide level2">

<p><span class="math display">\[
\frac{\barX_n - \mu}{\sigma/\sqrt{n}} \appxsim \Norm(0,1) \quad \text{for large } n
\]</span></p>
<p>Letting <span class="math inline">\(z_{\alpha/2}\)</span> be the upper <span class="math inline">\(\alpha/2\)</span> quantile of <span class="math inline">\(\Norm(0,1)\)</span>, i.e., <span class="math inline">\(z_{\alpha/2} = Q_{\Norm(0,1)}(1 - \alpha/2)\)</span>, then <span class="math display">\[\begin{align*}
1 - \alpha &amp; \approx
\Prob \biggl( -z_{\alpha/2} \le \frac{\barX_n - \mu}{\sigma/\sqrt{n}} \le z_{\alpha/2} \biggr) \\
&amp; \approx \Prob \biggl( \barX_n - z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \le \mu \le \barX_n + z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \biggr) \\
&amp; \approx \Prob \biggl( \underbrace{\barX_n - z_{\alpha/2} \frac{S_n}{\sqrt{n}}}_{\Theta_L} \le \mu \le \underbrace{\barX_n + z_{\alpha/2} \frac{S_n}{\sqrt{n}}}_{\Theta_U} \biggr)
\end{align*}\]</span></p>
<p>Thus, a large sample size confidence interval for <span class="math inline">\(\mu\)</span> is <span class="math display">\[\left[ \barX_n - z_{\alpha/2} \frac{S_n}{\sqrt{n}}, \; \barX_n + z_{\alpha/2} \frac{S_n}{\sqrt{n}} \right]\]</span></p>
</section>
<section class="slide level2">

<p>See the <a href="../slides/01-intro.html#approval-ratings">Approval Ratings</a> example for an illustration of this construction for a Bernoulli mean</p>
<p><em>Example</em>: You observe <span class="math inline">\(\barX_n = 12.0\)</span> minutes for taxis to arrive.You construct a 95% confidence interval for the mean arrival time, <span class="math inline">\(\mu\)</span>, assuming that the arrival times are distributed <span class="math inline">\(\Exp(1/\mu)\)</span>. Recall that <span class="math inline">\(\mu = \sigma = 1/\lambda\)</span>.</p>
<div class="columns">
<div class="column" style="width:58%;">
<div id="f15db4e4" class="cell" data-dpi="220" data-fig-height="4.6" data-fig-width="9" data-execution_count="3">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="02-estimator_files/figure-revealjs/cell-4-output-1.png" class="quarto-figure quarto-figure-center" width="763" height="392"></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:42%;">
<div id="37e80904" class="cell" data-dpi="220" data-fig-height="4.6" data-fig-width="6.5" data-execution_count="4">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="02-estimator_files/figure-revealjs/cell-5-output-1.png" class="quarto-figure quarto-figure-center" width="763" height="356"></p>
</figure>
</div>
</div>
</div>
</div></div>
</section>
<section id="small-sample-confidence-intervals-for-means-when-the-distribution-is-known" class="slide level2">
<h2>Small sample size confidence intervals for means when the distribution is known</h2>
<p><span class="small">(CB Â§9.2; WMS Â§8.8â€“8.9)</span></p>
<ul>
<li><p>If the sample sizem <span class="math inline">\(n\)</span>, is <span class="alert">not large enough</span> for the <a href="../slides/01-intro.html#clt">Central Limit Theorem</a> to apply,</p></li>
<li><p>But the sample mean has a <span class="alert">known distribution</span>, then exact confidence intervals can sometimes be constructed</p></li>
</ul>
</section>
<section class="slide level2">

<h3 id="confidence-interval-for-the-mean-of-an-exponential-distribution">Confidence interval for the mean of an exponential distribution</h3>
<p>You observe <span class="math inline">\(\barX_n = 12.0\)</span> minutes for taxis to arrive.based on <span class="math inline">\(n\)</span> observations. You construct a <span class="math inline">\(95\%\)</span> confidence interval for the mean arrival time, <span class="math inline">\(\mu\)</span>, assuming that the arrival times are distributed <span class="math inline">\(\Exp(1/\mu)\)</span>. Recall that <span class="math inline">\(\mu = \sigma = 1/\lambda\)</span>. Since we have the true distribution of <span class="math inline">\(\barX_n\)</span>:</p>
<div class="columns">
<div class="column" style="width:60%;">
<p><span class="math display">\[\begin{align*}
2\lambda n \barX_n &amp;\sim \chi^2_{2n} \\
\implies 1-\alpha
&amp;= \Prob \bigl( \chi^2_{2n,\,1-\alpha/2} \le 2\lambda n \barX_n \le \chi^2_{2n,\,\alpha/2} \bigr) \\[6pt]
&amp;= \Prob \biggl( \frac{\chi^2_{2n,\,1-\alpha/2}}{2 n \barX_n} \le \lambda \le \frac{\chi^2_{2n,\,\alpha/2}}{2 n \barX_n} \biggr) \\[10pt]
&amp;= \Prob \biggl( \frac{2 n \barX_n}{\chi^2_{2n,\,\alpha/2}} \le \mu \le \frac{2 n \barX_n}{\chi^2_{2n,\,1-\alpha/2}} \biggr),
\end{align*}\]</span></p>
</div><div class="column" style="width:40%;">
<div id="ab6f67c7" class="cell" data-fig-height="3.4" data-fig-width="6.8" data-execution_count="6">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="02-estimator_files/figure-revealjs/cell-7-output-1.png" width="950" height="467"></p>
</figure>
</div>
</div>
</div>
</div></div>
</section>
<section class="slide level2">

<h3 id="exact-vs-clt-sampling-distribution-of-overlinex-when-x_i-sim-exp1mu">Exact vs CLT sampling distribution of <span class="math inline">\(\overline{X}\)</span>, when <span class="math inline">\(X_i \sim \Exp(1/\mu)\)</span></h3>
<div id="ff9c5445" class="cell" data-fig-height="3.4" data-fig-width="6.8" data-execution_count="7">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="02-estimator_files/figure-revealjs/cell-8-output-1.png" width="950" height="466"></p>
</figure>
</div>
</div>
</div>
</section>
<section class="slide level2">

<h3 id="exact-vs-clt-confidence-intervals-for-mu-when-x_i-sim-exp1mu-two-sample-sizes">Exact vs CLT confidence intervals for <span class="math inline">\(\mu\)</span>, when <span class="math inline">\(X_i \sim \Exp(1/\mu)\)</span> (two sample sizes)</h3>
<div class="columns">
<div class="column" style="width:65%;">
<div id="770b371b" class="cell" data-fig-height="3.4" data-fig-width="6.8" data-execution_count="8">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="02-estimator_files/figure-revealjs/cell-9-output-1.png" width="950" height="469"></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:35%;">
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><a class="button" style="margin-left:0em;" href="../classlib/classlib/notebooks/confidence_interval_exact_vs_clt_vs_bootstrap.ipynb" download=""> â¬‡ Exact vs CLT vs Bootstrap Confidence Interval Coverage ðŸ““ </a></p>
</div></div>
<ul>
<li>For small <span class="math inline">\(n\)</span>
<ul>
<li>Exact confidence interval can be substantially different from the CLT-based interval</li>
<li>CLT interval is symmetric about <span class="math inline">\(\bar X_n\)</span>, while the exact interval is not</li>
</ul></li>
<li>As <span class="math inline">\(n\)</span> increases
<ul>
<li>CLT-based interval approaches the exact interval</li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<h3 id="ci-for-the-mean-of-normal-data-with-unknown-variance">CI for the mean of Normal data with unknown variance</h3>
<p>If <span class="math inline">\(X_1, \ldots, X_n\)</span> are IID <span class="math inline">\(\Norm(\mu, \sigma^2)\)</span>, then <span class="math inline">\(\displaystyle\frac{\barX_n - \mu}{S_n/\sqrt{n}} \sim t_{n-1}\)</span> for all <span class="math inline">\(n \ge 2\)</span>, where <span class="math inline">\(S_n^2\)</span> is the unbiased sample variance estimator. Letting <span class="math inline">\(t_{n-1,\alpha/2}\)</span> be the upper <span class="math inline">\(\alpha/2\)</span> quantile of <span class="math inline">\(t_{n-1}\)</span>, then<br>
<span class="math display">\[
\Prob \biggl( \barX_n - t_{n-1,\alpha/2} \frac{S_n}{\sqrt{n}} \le \mu \le \barX_n + t_{n-1,\alpha/2} \frac{S_n}{\sqrt{n}} \biggr) = 1 - \alpha
\]</span></p>
<div class="columns">
<div class="column" style="width:30%;">
<p>Studentâ€™s <span class="math inline">\(t\)</span> CIs are <span class="alert">wider</span> than CLT Normal CIs for small <span class="math inline">\(n\)</span> because <span class="math inline">\(t_{n-1,\alpha/2} &gt; z_{\alpha/2}\)</span></p>
<p>But they are <span class="alert">exact</span> and thus more accurate for all <span class="math inline">\(n \ge 2\)</span> when the data are Normal</p>
</div><div class="column" style="width:70%;">
<div id="84fab741" class="cell" data-fig-height="3.4" data-fig-width="6.8" data-execution_count="9">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="02-estimator_files/figure-revealjs/cell-10-output-1.png" width="1028" height="394"></p>
</figure>
</div>
</div>
</div>
</div></div>
</section>
<section class="slide level2">

<h3 id="ci-binomial-zero-failures">CI for a binomial proportion when no failures are observed</h3>
<p>You draw <span class="math inline">\(n\)</span> IID samples of your product to test for <em>failure</em>, and none of the samples fail. What is your confidence interval for <span class="math inline">\(p\)</span>, the probability that a product is satisfactory?</p>
<p>Let <span class="math inline">\(X_i = 1\)</span> if the <span class="math inline">\(i\)</span>th product is satisfactory and <span class="math inline">\(0\)</span> otherwise. Note that <span class="math display">\[\begin{gather*}
X_i =
\begin{cases}
1, &amp; \text{satisfactory},\\
0, &amp; \text{failure},
\end{cases}
\qquad
X_i \sim \Bern(p), \quad p=\Prob(\text{satisfactory}),
\\
T := \sum_{i=1}^n X_i \quad \text{(\# satisfactory)} \sim \Bin(n,p).
\end{gather*}\]</span></p>
<ul>
<li><p>Want a one-sided confidence interval for <span class="math inline">\(p\)</span> of the form <span class="math inline">\([P_L,1]\)</span>; confidence in our product quality</p></li>
<li><p><span class="math inline">\(P_L\)</span> is <em>a random variable</em>, defined as a function of <span class="math inline">\(T\)</span></p>
<ul>
<li>If true success probability <span class="math inline">\(&lt; P_L\)</span>, then observing <span class="math inline">\(\ge T\)</span> successes is quite unlikely</li>
</ul></li>
<li><p>We define a function <span class="math inline">\(p_{L,\alpha} : \{0,1,\ldots,n\} \to [0,1]\)</span> implicitly by requiring that <span class="math display">\[
\Prob_{\Bin(n,p_{L,\alpha}(t))}\bigl(T \ge t\bigr) = \alpha \qquad \forall t \in \{0,1,\ldots,n\}
\]</span> The random lower confidence limit is then <span class="math inline">\(P_L := p_{L,\alpha}(T)\)</span></p></li>
</ul>
</section>
<section class="slide level2">

<p><span class="math inline">\([P_L,1]\)</span> takes the form <span class="math inline">\(P_L := p_{L,\alpha}(T)\)</span>, with <span class="math display">\[
\Prob_{\Bin(n,p_{L,\alpha}(t))}\bigl(T \ge t\bigr) = \alpha \qquad \forall t \in \{0,1,\ldots,n\}.
\]</span> In our case the realized confidence interval based on <span class="math inline">\(n\)</span> successes is <span class="math inline">\([p_{L,\alpha}(n),1]\)</span>, so <span class="math display">\[
[p_{L,\alpha}(n)]^n = \Prob_{\Bin(n,p_{L,\alpha}(n))}\bigl(T \ge n\bigr) = \alpha \iff p_{L,\alpha}(n) = \alpha^{1/n}
\]</span></p>
<p>&nbsp;</p>
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(n\)</span></th>
<th>5</th>
<th>10</th>
<th>20</th>
<th>100</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(p_L = \alpha^{1/n}\)</span></td>
<td>0.5493</td>
<td>0.7411</td>
<td>0.8609</td>
<td>0.9705</td>
</tr>
</tbody>
</table>
</section>
<section id="why-was-the-exact-confidence-interval-for-exponential-and-normal-data-easy-but-not-bernoulli-data" class="slide level2">
<h2>Why was the exact confidence interval for exponential and normal data â€œeasyâ€, but not Bernoulli data?</h2>
<p>A <span class="alert">pivot</span> is a function of the data and the parameter whose distribution does <strong>not</strong> depend on unknown parameters:</p>
<p><span class="math display">\[
\begin{array}{rcrll}
X_1,\ldots,X_n \sim \Exp(1/\mu)
&amp;:&amp;
\displaystyle \frac{2 n \barX_n}{\mu}
&amp;\sim \chi^2_{2n}
&amp;\quad \text{âœ“ no } \mu
\\[1.2em]
X_1,\ldots,X_n \sim \Norm(\mu,\sigma^2)
&amp;:&amp;
\displaystyle \frac{\barX_n - \mu}{S_n/\sqrt{n}}
&amp;\sim t_{n-1}
&amp;\quad \text{âœ“ no } \mu,\sigma^2
\\[1.2em]
X_1,\ldots,X_n \sim \Bern(p)
&amp;:&amp;
\displaystyle n\barX_n
&amp;\sim \Bin(n,p)
&amp;\quad \text{âœ— depends on } p
\end{array}
\]</span></p>
<p>If we can find a pivot, we can invert probability statements to get a confidence interval more easily</p>
</section>
<section id="confidence-intervals-for-means-of-differences" class="slide level2">
<h2>Confidence Intervals for <em>Means</em> of Differences</h2>
<p>For paired or matched data (before/after, twins, same subject measured twice) <span class="math display">\[
D_i = X_i - Y_i, \quad i = 1,\dots,n
\]</span></p>
<p>Inference is about the <strong>mean difference</strong> <span class="math inline">\(\mu_D\)</span> (paired setting)</p>
<p><em>Not</em> difference of means <span class="math inline">\(\mu_X - \mu_Y\)</span> (unpaired), even though <span class="math inline">\(\barD_n= \barX_n - \barY_n\)</span></p>
<p>If <span class="math inline">\(D_1,\dots,D_n \IIDsim \Norm(\mu_D, \sigma_D^2)\)</span> <span class="math display">\[
\Prob\left[ \barD_n - t_{n-1,\alpha/2}\frac{S_{D,n}}{\sqrt{n}} \le \mu_D \le \barD_n  + t_{n-1,\alpha/2}\frac{S_{D,n}}{\sqrt{n}} \right] = 1 - \alpha
\]</span> where <span class="math inline">\(\displaystyle S_{D,n}^2 = \frac 1{n-1} \sum_{i=1}^n (D_i - \barD_n)^2\)</span></p>
<p>If <span class="math inline">\(D_1,\dots,D_n\)</span> are IID with finite variance, and <span class="math inline">\(n\)</span> is large <span class="math display">\[
\Prob\left[ \barD_n - z_{\alpha/2}\frac{S_{D,n}}{\sqrt{n}} \le \mu_D \le \barD_n + z_{\alpha/2}\frac{S_{D,n}}{\sqrt{n}} \right] \approx 1 - \alpha
\]</span></p>
</section>
<section id="confidence-intervals-for-differences-of-means" class="slide level2">
<h2>Confidence Intervals for <em>Differences</em> of Means</h2>
<p>For two <em>independent</em> samples (control/treatment, two groups)</p>
<p><span class="math display">\[
X_1,\dots,X_{n_X} \sim \text{population 1}, \quad
Y_1,\dots,Y_{n_Y} \sim \text{population 2}
\]</span></p>
<p>with sample means <span class="math inline">\(\barX_{n_X}, \barY_{n_Y}\)</span> and sample variances <span class="math inline">\(S_{X,n_X}^2, S_{Y,n_Y}^2\)</span></p>
</section>
<section class="slide level2">

<h3 id="pooled-t-confidence-interval-wackerly">Pooled-<span class="math inline">\(t\)</span> confidence interval (Wackerly)</h3>
<p>Assume that the two populations:</p>
<ul>
<li>Are sampled <span class="alert">independently</span></li>
<li>Are <span class="alert">Normal</span></li>
<li>Have a <span class="alert">common variance</span> <span class="math inline">\(\sigma^2\)</span></li>
</ul>
<p>Define the <em>pooled variance</em> estimator of <span class="math inline">\(\sigma^2\)</span> as <span class="math display">\[
S_p^2
=
\frac{(n_X-1)S_{X,n_X}^2 + (n_Y-1)S_{Y,n_Y}^2}{n_X + n_Y - 2}.
\]</span></p>
<p>Then a <span class="math inline">\(t\)</span>-based confidence interval for <span class="math inline">\(\mu_X - \mu_Y\)</span> is</p>
<p><span class="math display">\[\begin{multline*}
\Prob\left[
(\barX_{n_X}-\barY_{n_Y})
-
t_{n_X+n_Y-2,\alpha/2}
\, S_p
\sqrt{\frac{1}{n_X} + \frac{1}{n_Y}}
\le \mu_X - \mu_Y \right .  \\ \left .
\le
(\barX_{n_X}-\barY_{n_Y})
+
t_{n_X+n_Y-2,\alpha/2}
\, S_p
\sqrt{\frac{1}{n_X} + \frac{1}{n_Y}}
\right]
=
1 - \alpha.
\end{multline*}\]</span></p>
</section>
<section class="slide level2">

<p>Other variations exist (Welch two-sample <span class="math inline">\(t\)</span>, unequal variances).</p>
<h3 id="clt-based-interval-large-samples">CLT-based interval (large samples)</h3>
<p>If <span class="math inline">\(n_X\)</span> and <span class="math inline">\(n_Y\)</span> are large and the samples are independent, then a <em>CLT-based confidence interval</em> applies even if the two populations:</p>
<ul>
<li>need <em>not</em> be Normal</li>
<li>need <em>not</em> have a common variance</li>
</ul>
<p><span class="math display">\[\begin{multline*}
\Prob\left[
(\barX_{n_X}-\barY_{n_Y})
-
z_{\alpha/2}
\sqrt{\frac{S_{X,n_X}^2}{n_X}+\frac{S_{Y,n_Y}^2}{n_Y}}
\le \mu_X - \mu_Y \right . \\ \left .
\le
(\barX_{n_X}-\barY_{n_Y})
+
z_{\alpha/2}
\sqrt{\frac{S_{X,n_X}^2}{n_X}+\frac{S_{Y,n_Y}^2}{n_Y}}
\right]
\approx
1 - \alpha.
\end{multline*}\]</span></p>
</section>
<section class="slide level2">

<h3 id="exstar-exercises-regarding-confidence-intervals-for-different-kinds-of-means"><span class="math inline">\(\exstar\)</span> Exercises regarding confidence intervals for different kinds of means</h3>
<ul>
<li><span class="math inline">\(X_1,\dots,X_{100}\)</span> and <span class="math inline">\(Y_1,\dots,Y_{100}\)</span> are two samples with sample means <span class="math inline">\(\barX, \barY\)</span> and sample standard deviations <span class="math inline">\(S_X, S_Y\)</span>, respectively</li>
<li><span class="math inline">\(D_i = X_i - Y_i\)</span> and <span class="math inline">\(S_D\)</span> be the sample standard deviation of the <span class="math inline">\(D_i\)</span></li>
<li>You observe <span class="math inline">\(\barx = 85, \bary = 75, s_X = 10, s_Y = 12, s_D = 4\)</span></li>
</ul>
<p>Construct the appropriate 95% confidence intervals for the following scenarios:</p>
<ol type="1">
<li><p><span class="math inline">\(X_1,\dots,X_{100}\)</span> are IID test scores from a population of medical students. Construct a 95% confidence interval for the mean test score of the whole population and interpret the interval in context.</p></li>
<li><p><span class="math inline">\(X_1,\dots,X_{100}\)</span> and <span class="math inline">\(Y_1,\dots,Y_{100}\)</span> are two independent IID samples of test scores from two different populations of medical students. The first group was given a practice test beforehand, and the second group was not. Construct a 95% confidence interval for the difference in mean test scores between the two populations and interpret the interval in context.</p></li>
<li><p><span class="math inline">\(X_1,\dots,X_{100}\)</span> and <span class="math inline">\(Y_1,\dots,Y_{100}\)</span> are two IID samples of test scores from the same population of medical students. The <span class="math inline">\(X_i\)</span>â€™s are the studentsâ€™ scores on the real test, and the <span class="math inline">\(Y_i\)</span>â€™s are the studentsâ€™ scores on the practice test taken earlier. Construct a 95% confidence interval for the mean difference in test scores between the practice and real tests.</p></li>
</ol>
</section>
<section id="confidence-intervals-for-proportions" class="slide level2">
<h2>Confidence Interval for <em>Proportions</em> (CLT)</h2>
<h3 id="one-proportion">One proportion</h3>
<ul>
<li><span class="math inline">\(X_1,\dots,X_n \IIDsim \Bern(p)\)</span> (<span class="math inline">\(p =\)</span> probability of success shooting free throws, product quality control, etc.)</li>
<li><span class="math inline">\(P_n = \frac{1}{n}\sum_{i=1}^n X_i =\)</span> sample proportion of successes</li>
<li><span class="math inline">\(\Ex[P_n] = p\)</span> and <span class="math inline">\(\var(P_n) = p(1-p)/n\)</span></li>
</ul>
<p>If <span class="math inline">\(n\)</span> is large, an <span class="alert">approximate CLT-based</span> interval for <span class="math inline">\(p\)</span> is</p>
<p><span class="math display">\[\begin{equation*}
\Prob\left[
P_n
-
z_{\alpha/2}\sqrt{\frac{P_n(1-P_n)}{n}}
\le p
\le
P_n
+
z_{\alpha/2}\sqrt{\frac{P_n(1-P_n)}{n}}
\right]
\approx
1-\alpha
\end{equation*}\]</span></p>
</section>
<section class="slide level2">

<h3 id="difference-of-two-proportions">Difference of two proportions</h3>
<p><span class="alert">Independent</span> samples <span class="math display">\[\begin{gather*}
X_1,\dots,X_{n_X} \IIDsim \Bern(p_X), \qquad
Y_1,\dots,Y_{n_Y} \IIDsim \Bern(p_Y)
\\
P_X = \frac{1}{n_X}\sum X_i,
\qquad
P_Y = \frac{1}{n_Y}\sum Y_j
\end{gather*}\]</span></p>
<p>If <span class="math inline">\(n_X\)</span> and <span class="math inline">\(n_Y\)</span> are large, an <span class="alert">approximate CLT-based</span> confidence interval for <span class="math inline">\(p_X - p_Y\)</span> is</p>
<p><span class="math display">\[\begin{multline*}
\Prob\left[
(P_X-P_Y)
-
z_{\alpha/2}
\sqrt{\frac{P_X(1-P_X)}{n_X}
+
\frac{P_Y(1-P_Y)}{n_Y}}
\right . \\ \left .
\le p_X - p_Y \le
(P_X-P_Y)
+
z_{\alpha/2}
\sqrt{\frac{P_X(1-P_X)}{n_X}
+
\frac{P_Y(1-P_Y)}{n_Y}}
\right]
\approx
1-\alpha
\end{multline*}\]</span></p>
</section>
<section id="confidence-interval-for-a-variance" class="slide level2">
<h2>Confidence Interval for a <em>Variance</em></h2>
<p>Let <span class="math inline">\(X_1,\dots,X_n \IIDsim \Norm(\mu,\sigma^2)\)</span> with sample variance <span class="math inline">\(S_n^2\)</span></p>
<p>Then <span class="math display">\[
\frac{(n-1)S_n^2}{\sigma^2} \sim \chi^2_{n-1}
\]</span></p>
<p>and a <span class="math inline">\((1-\alpha)\)</span> confidence interval for <span class="math inline">\(\sigma^2\)</span> is</p>
<p><span class="math display">\[\begin{equation*}
\Prob\!\left[
\frac{(n-1)S_n^2}{\chi^2_{n-1,\alpha/2}}
\le
\sigma^2
\le
\frac{(n-1)S_n^2}{\chi^2_{n-1,1-\alpha/2}}
\right]
=
1-\alpha
\end{equation*}\]</span></p>
</section>
<section id="confidence-interval-for-ratio-of-variances" class="slide level2">
<h2>Confidence Interval for <em>Ratio</em> of Variances</h2>
<p>Let <span class="math display">\[
X_1,\dots,X_{n_X} \IIDsim \Norm(\mu_X,\sigma_X^2), \quad
Y_1,\dots,Y_{n_Y} \IIDsim \Norm(\mu_Y,\sigma_Y^2)
\]</span></p>
<p>be independent samples with sample variances <span class="math inline">\(S_{X,n_X}^2, S_{Y,n_Y}^2\)</span>, respectively</p>
<p>Then <span class="math display">\[
\frac{S_{X,n_X}^2 / \sigma_X^2}{S_{Y,n_Y}^2 / \sigma_Y^2}
\sim
F_{\,n_X-1,n_Y-1}
\]</span></p>
<p>and a <span class="math inline">\((1-\alpha)\)</span> confidence interval for <span class="math inline">\(\displaystyle \frac{\sigma_X^2}{\sigma_Y^2}\)</span> is</p>
<p><span class="math display">\[\begin{equation*}
\Prob\!\left[
\frac{S_{X,n_X}^2}{S_{Y,n_Y}^2}
\frac{1}{F_{n_X-1,n_Y-1,\alpha/2}}
\le
\frac{\sigma_X^2}{\sigma_Y^2}
\le
\frac{S_{X,n_X}^2}{S_{Y,n_Y}^2}
\frac{1}{F_{n_X-1,n_Y-1,1-\alpha/2}}
\right]
=
1-\alpha
\end{equation*}\]</span></p>
</section>
<section id="bootstrap-confidence-intervals" class="slide level2">
<h2>Bootstrap Confidence Intervals</h2>
<p><span class="small">(EH Ch. 11)</span></p>
<p>Classical confidence intervals rely on assumptions such as:</p>
<ul>
<li>Known or estimable variance</li>
<li>Normality of the sampling distribution</li>
<li>Large sample sizes (via CLT)</li>
</ul>
<p>But in practice we often have:</p>
<ul>
<li>Small samples</li>
<li>Skewed or heavy-tailed data</li>
<li>Complicated estimators (medians, quantiles, ratios)</li>
</ul>
<p><em>Bootstrap confidence intervals</em> replace distributional assumptions with <span class="alert">resampling from the observed data</span> to approximate the sampling distribution of an estimator</p>
</section>
<section id="the-bootstrap-idea" class="slide level2">
<h2>The Bootstrap Idea</h2>
<p>Given data <span class="math inline">\(X_1,\dots,X_n\)</span> and an estimator <span class="math inline">\(\Theta\)</span>:</p>
<ol type="1">
<li><p>Resample <em>with replacement</em> from the data to form <span class="math inline">\(B\)</span> bootstrap samples<br>
Each bootstrap sample has size <span class="math inline">\(n\)</span> and consists of draws from the original data <span class="math display">\[
X_1^{(b)},\dots,X_n^{(b)} \IIDsim \text{Uniform}\{X_1,\dots,X_n\}, \quad b=1,\dots,B
\]</span></p></li>
<li><p>Compute the bootstrap estimators <span class="math inline">\(\Theta^{(b)}\)</span> <span class="math display">\[
\Theta^{(b)} = \Theta(X_1^{(b)},\dots,X_n^{(b)}), \quad b=1,\dots,B
\]</span></p></li>
<li><p>Use the empirical distribution of <span class="math inline">\(\Theta^{(1)},\dots,\Theta^{(B)}\)</span> to construct confidence intervals</p></li>
</ol>
<p>A simple <em>bootstrap percentile CI</em> uses the <em>order statistics</em> of the bootstrap estimators: <span class="math display">\[
\left[
\Theta_{(\alpha/2)},
\Theta_{(1-\alpha/2)}
\right]
\]</span></p>
<div class="key-point">
<p>No normality Â· No variance formula Â· Works when classical assumptions fail</p>
</div>
</section>
<section id="example-of-vanilla-bootstrap" class="slide level2">
<h2>Example of vanilla bootstrap</h2>
<p>We draw a <em>single IID random sample of size</em> <span class="math inline">\(n=8\)</span> from a population: <span class="math display">\[
X_1,\dots,X_{8}, \qquad
\barX = \frac{1}{8}\sum_{i=1}^{8} X_i
\]</span> A <em>vanilla bootstrap sample</em> is obtained by sampling <strong>with replacement</strong> from the observed data <span class="math inline">\(\{X_1,\dots,X_{8}\}\)</span>. We repeat this independently to obtain <em>bootstrap</em> samples.</p>
<table style="width:100%; table-layout:fixed;">
<thead>
<tr>
<th style="width:20%;">
Sample
</th>
<th style="width:55%;">
Observations
</th>
<th style="width:25%;">
Sample mean
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<strong>Original</strong>
</td>
<td>
0.15, 0.82, 1.24, 1.18, 3.64, 0.88, 0.23, 1.01
</td>
<td>
1.14
</td>
</tr>
<tr>
<td>
Bootstrap 1
</td>
<td>
0.88, 0.88, 0.15, 0.23, 0.15, 0.15, 1.24, 0.23
</td>
<td>
0.49
</td>
</tr>
<tr>
<td>
Bootstrap 2
</td>
<td>
1.01, 0.15, 1.18, 1.24, 0.23, 1.01, 1.01, 0.15
</td>
<td>
0.75
</td>
</tr>
<tr>
<td>
Bootstrap 3
</td>
<td>
0.82, 1.01, 1.01, 1.24, 0.15, 1.18, 1.24, 1.24
</td>
<td>
0.99
</td>
</tr>
<tr>
<td>
Bootstrap 4
</td>
<td>
1.01, 3.64, 0.82, 1.18, 0.23, 3.64, 0.82, 0.15
</td>
<td>
1.44
</td>
</tr>
<tr>
<td>
<span class="math inline">\(\vdots\)</span>
</td>
<td>
<span class="math inline">\(\vdots\)</span>
</td>
<td>
<span class="math inline">\(\vdots\)</span>
</td>
</tr>
</tbody>
</table>
<p><a class="button" style="margin-left:0em;" href="../classlib/classlib/notebooks/confidence_interval_exact_vs_clt_vs_bootstrap.ipynb" download=""> â¬‡ Exact vs CLT vs Bootstrap Confidence Interval Coverage ðŸ““ </a></p>
</section>
<section id="ci-assumptions" class="slide level2">
<h2>Assumptions behind common confidence intervals</h2>
<p>Data are IID from a distribution with finite variance</p>
<table class="caption-top">
<colgroup>
<col style="width: 7%">
<col style="width: 32%">
<col style="width: 8%">
<col style="width: 23%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Distributional Assumptions</th>
<th>Sample Size</th>
<th>Method</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mu\)</span></td>
<td><strong>Any</strong> distribution</td>
<td><strong>Large</strong> <span class="math inline">\(n\)</span></td>
<td><strong>CLT</strong></td>
<td><strong>Approximate</strong>, accuracy improves as <span class="math inline">\(n \to \infty\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mu\)</span></td>
<td><strong>Normal</strong> data, <br><span class="math inline">\(\sigma\)</span> unknown</td>
<td>Any <span class="math inline">\(n\)</span></td>
<td>Studentâ€™s <em>t</em></td>
<td><strong>Exact</strong></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mu = p\)</span></td>
<td><strong>Bernoulli</strong> trials</td>
<td>Any <span class="math inline">\(n\)</span></td>
<td>Binomial<br>(Clopperâ€“Pearson)</td>
<td><strong>Exact</strong><br>Conservative</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mu = p\)</span></td>
<td><strong>Bernoulli</strong> trials</td>
<td>Large <span class="math inline">\(np\)</span>, <span class="math inline">\(n(1-p)\)</span></td>
<td><strong>CLT</strong></td>
<td><strong>Approximate</strong></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mu\)</span></td>
<td><strong>Exponential</strong> data</td>
<td>Any <span class="math inline">\(n\)</span></td>
<td>Gamma/<br>Chi-squared</td>
<td><strong>Exact</strong></td>
</tr>
</tbody>
</table>
</section>
<section class="slide level2">

<table class="caption-top">
<colgroup>
<col style="width: 7%">
<col style="width: 32%">
<col style="width: 8%">
<col style="width: 23%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Distributional Assumptions</th>
<th>Sample Size</th>
<th>Method</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mu_D\)</span> <br>(paired differences)</td>
<td>Differences are <strong>Normal</strong></td>
<td>Any <span class="math inline">\(n\)</span></td>
<td>Paired <em>t</em></td>
<td><strong>Exact</strong>, sometimes confused with two-sample <em>t</em></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mu_X-\mu_Y\)</span></td>
<td>Each sample <strong>Normal</strong>; independent samples; <strong>common variance</strong></td>
<td>Any <span class="math inline">\(n_X,n_Y\)</span></td>
<td>Two-sample <em>t</em> (pooled)</td>
<td><strong>Exact</strong></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mu_X-\mu_Y\)</span></td>
<td>Independent samples from <strong>any</strong> distributions with finite variances</td>
<td><strong>Large</strong> <span class="math inline">\(n_X,n_Y\)</span></td>
<td><strong>CLT</strong> (two-sample)</td>
<td><strong>Approximate</strong></td>
</tr>
<tr class="even">
<td><span class="math inline">\(p_X-p_Y\)</span></td>
<td>Independent samples of <strong>Bernoulli</strong> trials</td>
<td>Large <span class="math inline">\(n_Xp_X\)</span>, <span class="math inline">\(n_X(1-p_X)\)</span>, <span class="math inline">\(n_Yp_Y\)</span>, <span class="math inline">\(n_Y(1-p_Y)\)</span></td>
<td><strong>CLT</strong> (two-sample)</td>
<td><strong>Approximate</strong></td>
</tr>
</tbody>
</table>
</section>
<section class="slide level2">

<table class="caption-top">
<colgroup>
<col style="width: 7%">
<col style="width: 32%">
<col style="width: 8%">
<col style="width: 23%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Distributional Assumptions</th>
<th>Sample Size</th>
<th>Method</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\sigma^2\)</span></td>
<td><strong>Normal</strong> data</td>
<td>Any <span class="math inline">\(n\)</span></td>
<td>Chi-squared</td>
<td><strong>Exact</strong>, sensitive to non-normality</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\sigma_X^2/\sigma_Y^2\)</span></td>
<td><strong>Normal</strong> data; independent samples</td>
<td>Any <span class="math inline">\(n_X,n_Y\)</span></td>
<td>F-distribution</td>
<td><strong>Exact</strong>, sensitive to non-normality</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\med(F)\)</span></td>
<td>Continuous distribution</td>
<td><span class="math inline">\(n\)</span> not too small</td>
<td>Order-statistics</td>
<td><strong>Approximate</strong>, Distribution-free</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\theta(F)\)</span></td>
<td><strong>None</strong> (empirical distribution)</td>
<td>Moderate <span class="math inline">\(n\)</span></td>
<td>Bootstrap resampling</td>
<td><strong>Approximate</strong>, works when classical theory breaks</td>
</tr>
</tbody>
</table>
</section>
<section class="slide level2">

<h3 id="summary">Summary</h3>
<ul>
<li>Confidence intervals provide a <span class="alert">range of plausible values</span> for parameters based on data
<ul>
<li>They go beyond point estimation by quantifying uncertainty</li>
</ul></li>
<li>Expressed in terms of random variables, confidence intervals are <span class="alert">probabilistic</span> statements about the data-generating process</li>
<li>Realized by plugging in the observed data, confidence intervals are <em>not</em> probabilistic statements</li>
<li>Validity of confidence intervals depends on assumptions about the <span class="alert">data distribution</span> and <span class="alert">sample size(s)</span>
<ul>
<li>With fewer data we need stronger distributional assumptions</li>
<li>With more data we can rely on asymptotic results like the CLT</li>
</ul></li>
<li>Two-sided CLT confidence intervals take the form <span class="math display">\[\text{estimator} \pm z_{\alpha/2} \times \text{standard error}\]</span></li>
</ul>
</section></section>
<section>
<section id="do-we-have-the-right-estimators-to-summarize-our-data-and-construct-confidence-intervals" class="title-slide slide level1 center">
<h1>Do we have the right estimators to summarize our data and construct confidence intervals?</h1>
<ul>
<li><p><a href="#/sufficiency">Sufficiency</a></p>
<ul>
<li>Do we have a statistic that captures <em>all</em> of the information about <span class="math inline">\(\vtheta\)</span> in the data?</li>
<li>Do we have a <em>minimal</em> statistic that compresses the data as much as possible without losing information about <span class="math inline">\(\vtheta\)</span>?</li>
</ul></li>
<li><p><a href="#/rao-blackwell">Raoâ€“Blackwell</a> â€” better estimators through conditioning</p></li>
<li><p><a href="#/completeness">Completeness</a> â€” eliminating hidden unbiased estimators</p></li>
<li><p><a href="#/cramer-rao">CramÃ©râ€“Rao</a> â€” lower bounds on the variance of unbiased estimators</p></li>
</ul>
</section>
<section id="sufficiency" class="slide level2">
<h2>Sufficiency</h2>
<p>Let <span class="math inline">\(\vX =(X_1,\dots,X_n)\)</span> have joint PMF/PDF <span class="math inline">\(\varrho(\vx \mid \vtheta)\)</span>.</p>
<p>&nbsp;</p>
<p>A statistic <span class="math inline">\(T(\vX)\)</span> is <span class="alert">sufficient</span> for <span class="math inline">\(\vtheta\)</span> if</p>
<p>the conditional distribution of <span class="math inline">\(\vX\)</span> given <span class="math inline">\(T(\vX)\)</span></p>
<p>does <strong>NOT</strong> depend on <span class="math inline">\(\vtheta\)</span>.</p>
<p>&nbsp;</p>
<div class="key-point">
<p>If a sufficient statistic, <span class="math inline">\(T\)</span>, is known, then the sample contains no additional information about <span class="math inline">\(\vtheta\)</span></p>
</div>
</section>
<section id="factorization-theorem" class="slide level2">
<h2>Factorization Theorem</h2>
<p><span class="math inline">\(T(\vX)\)</span> is <span class="alert">sufficient</span> for <span class="math inline">\(\vtheta\)</span> iff</p>
<p><span class="math display">\[\begin{align*}
\varrho(\vx \mid \vtheta) = L(\vtheta \mid \vx)
&amp; =
g(T(\vx), \vtheta)\, h(\vx) \\
\text{or equivalently } \qquad \ell(\vtheta \mid \vx)  = \log(L(\vtheta \mid \vx)) &amp; = \tg(T(\vx), \vtheta) + \th(\vx)
\end{align*}\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(g\)</span> depends on data only through <span class="math inline">\(T\)</span></li>
<li><span class="math inline">\(h\)</span> does not depend on <span class="math inline">\(\vtheta\)</span></li>
</ul>
<p>This is the fundamental practical tool for proving sufficiency</p>
</section>
<section class="slide level2">

<p><em>Example</em>: For <span class="math inline">\(X_1, \ldots, X_n \IIDsim \Bern(p)\)</span>, the log likelihood is <span class="math display">\[
\ell(p \mid \vx) = \sum_{i=1}^n \left [ x_i \log(p) + (1-x_i)\log(1-p) \right]
= \left(\sum_{i=1}^n x_i \right) [\log(p) - \log(1-p)]  + n\log(1-p)
\]</span> so <span class="math inline">\(\displaystyle T(\vX) = \sum_{i=1}^n X_i\)</span> is sufficient for <span class="math inline">\(p\)</span>.</p>
<p>&nbsp;</p>
<div class="exitem">
<p><span class="exbullet"><span class="math inline">\(\exstar\)</span></span><span> For <span class="math inline">\(X_1, \ldots, X_n \IIDsim \Bern(p)\)</span> <span class="exsub">Is the statistic <span class="math inline">\(T(\vX)=X_1+X_2\)</span> sufficient for <span class="math inline">\(p\)</span>?</span> <span class="exsub">Is the statistic <span class="math inline">\(T(\vX)=\left(X_1+X_2, \sum_{i=3}^n X_i\right)\)</span> sufficient for <span class="math inline">\(p\)</span>?</span> </span></p>
</div>
</section>
<section class="slide level2">

<p><span class="math inline">\(\exstar\)</span> For <span class="math inline">\(X_1, \ldots, X_n \IIDsim \Exp(\lambda)\)</span>, the statistic <span class="math inline">\(\displaystyle T(\vX) = \sum_{i=1}^n X_i\)</span> is sufficient for <span class="math inline">\(\lambda\)</span></p>
<p><span class="math inline">\(\exstar\)</span> For <span class="math inline">\(X_1, \ldots, X_n \IIDsim \Norm(\mu,\sigma^2)\)</span>, the statistic <span class="math inline">\(\displaystyle T(\vX) = \sum_{i=1}^n X_i\)</span> is not sufficient for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
<p><span class="math inline">\(\exstar\)</span> Find a sufficient (vector) statistic for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> for <span class="math inline">\(X_1, \ldots, X_n \IIDsim \Norm(\mu,\sigma^2)\)</span></p>
</section>
<section id="minimal-sufficiency" class="slide level2">
<h2>Minimal Sufficiency</h2>
<p>A sufficient statistic <span class="math inline">\(T\)</span> is <em>minimal sufficient</em> if</p>
<ul>
<li><p>it is sufficient, and</p></li>
<li><p>it is a function of every other sufficient statistic.</p></li>
</ul>
<div class="key-point">
<p>A minimial sufficient statistic compresses the data as much as possible without losing information about <span class="math inline">\(\vtheta\)</span></p>
</div>
<p><strong>Theorem</strong>: A statistic <span class="math inline">\(T(\vX)\)</span> is minimal sufficient if for all sample points <span class="math inline">\(\vx\)</span>, <span class="math inline">\(\vy\)</span>,</p>
<p><span class="math display">\[
\left . \begin{array}{r} \displaystyle \frac{L(\vtheta \mid \vx)}{L(\vtheta \mid \vy)} \\
\text{ or equivalently} \quad \ell(\vtheta \mid \vx) - \ell(\vtheta \mid \vy) \end{array} \right \}
\text{ is independent of } \vtheta
\quad
\Longleftrightarrow
\quad
T(\vx) = T(\vy)
\]</span></p>
</section>
<section class="slide level2">

<p><strong>Theorem</strong>: A statistic <span class="math inline">\(T(\vX)\)</span> is minimal sufficient if for all sample points <span class="math inline">\(\vx\)</span>, <span class="math inline">\(\vy\)</span>,</p>
<p><span class="math display">\[
\ell(\vtheta \mid \vx) - \ell(\vtheta \mid \vy)
\text{ is independent of } \vtheta
\quad
\Longleftrightarrow
\quad
T(\vx) = T(\vy)
\]</span></p>
<p><em>Proof that <span class="math inline">\(T(\vX)\)</span> is sufficient</em>:</p>
<p>Suppose that <span class="math inline">\(\ell(\vtheta \mid \vx) - \ell(\vtheta \mid \vy)\)</span> is independent of <span class="math inline">\(\vtheta\)</span> iff <span class="math inline">\(T(\vx) = T(\vy)\)</span>. Let</p>
<ul>
<li><span class="math inline">\(\cx\)</span> be the sample space of the data, <span class="math inline">\(\vX\)</span></li>
<li><span class="math inline">\(\ct : = T(\vtheta \mid \cx)\)</span> be the image of <span class="math inline">\(T\)</span></li>
</ul>
<p>For each <span class="math inline">\(t \in \ct\)</span>, find one <span class="math inline">\(\vx_t \in \cx\)</span> such that <span class="math inline">\(T(\vx_t) = t\)</span>, and define</p>
<ul>
<li><p><span class="math inline">\(\th(x) : = \ell(\vtheta \mid \vx) - \ell(\vtheta \mid \vx_{T(\vx)})\quad \forall x\in \cx \qquad\)</span> (independent of <span class="math inline">\(\vtheta\)</span> by the hypothesis)</p></li>
<li><p><span class="math inline">\(\tg(t,\vtheta) : = \ell(\vtheta \mid \vx_t)\quad \forall \vtheta, \forall t \in \ct\)</span></p></li>
</ul>
<p>Then for any <span class="math inline">\(\vx \in \cx\)</span>,</p>
<p><span class="math display">\[
\ell(\vtheta \mid \vx) \underbrace{=}_{\text{def. of} \th} \th(\vx) + \ell(\vtheta \mid \vx_{T(\vx)}) \underbrace{=}_{\text{def. of} \tg}  \th(\vx) + \tg(T(\vx),\vtheta)  
\quad  \forall \vtheta, \ \forall \vx \in \cx
\]</span></p>
<p>so <span class="math inline">\(T\)</span> is sufficient</p>
</section>
<section class="slide level2">

<p><em>Proof that <span class="math inline">\(T(\vX)\)</span> is minimal sufficient</em>:</p>
<p>Let <span class="math inline">\(T'(\vX)\)</span> be any other sufficient statistic, so</p>
<p><span class="math display">\[\ell(\vtheta \mid \vx) = \tg'(T'(\vx),\vtheta) + \th'(\vx) \quad \text{by the Factorization Theorem}\]</span></p>
<p>For any <span class="math inline">\(\vx, \vy \in \cx\)</span> with <span class="math inline">\(T'(\vx) = T'(\vy)\)</span>, we have <span class="math display">\[
\ell(\vtheta \mid \vx) - \ell(\vtheta \mid \vy) = \tg'(T'(\vx),\vtheta) + \th'(\vx) - \tg'(T'(\vy),\vtheta) - \th'(\vy) \underbrace{=}_{\text{since } T'(\vx) = T'(\vy)} \th'(\vx) - \th'(\vy)\]</span> which does not depend on <span class="math inline">\(\vtheta\)</span>. Thus, <span class="math inline">\(T(\vx) = T(\vy)\)</span> by the hypothesis of the theorem.</p>
<p>&nbsp;</p>
<p>Since <span class="math inline">\(T'(\vx) = T'(\vy) \implies T(\vx) = T(\vy)\)</span>, it follows that <span class="math inline">\(T\)</span> is a function of <span class="math inline">\(T'\)</span> <span class="math inline">\(\qquad \square\)</span></p>
</section>
<section class="slide level2">

<p><em>Example</em>: For <span class="math inline">\(X_1, \ldots, X_n \IIDsim \Bern(p)\)</span>, <span class="math inline">\(\displaystyle T(\vX) = \sum_{i=1}^n X_i\)</span> is a sufficient statistic for <span class="math inline">\(p\)</span>. To show that it is minimal sufficient, note that for any <span class="math inline">\(\vx\)</span>, <span class="math inline">\(\vy\)</span>, <span class="math display">\[
\ell(p \mid \vx) - \ell(p \mid \vy) = \bigl(T(\vx) - T(\vy) \bigr) [\log(p) - \log(1-p)]  = 0 \iff T(\vx) = T(\vy)
\]</span> so <span class="math inline">\(T(\vX)\)</span> is minimal sufficient for <span class="math inline">\(p\)</span></p>
<p>&nbsp;</p>
<div class="exitem">
<p><span class="exbullet"><span class="math inline">\(\exstar\)</span></span><span>For <span class="math inline">\(X_1, \ldots, X_n \IIDsim \Bern(p)\)</span>, is the statistic <span class="math inline">\(T'(\vX)=\left(X_1+X_2, \sum_{i=3}^n X_i\right)\)</span> minimal sufficient for <span class="math inline">\(p\)</span>?</span></p>
</div>
</section>
<section id="rao-blackwell" class="slide level2">
<h2>Raoâ€“Blackwell Theorem</h2>
<p>Let <span class="math inline">\(\vTheta\)</span> be unbiased and let <span class="math inline">\(T\)</span> be sufficient</p>
<p>Define</p>
<p><span class="math display">\[
\widetilde{\vTheta} := \Ex(\vTheta \mid T)
\]</span></p>
<p>Then</p>
<ul>
<li><span class="math inline">\(\widetilde{\vTheta}\)</span> is unbiased<br>
</li>
<li><span class="math inline">\(\var(\widetilde{\vTheta}) \le \var(\vTheta)\)</span></li>
</ul>
<p>&nbsp;</p>
<div class="key-point">
<p>Conditioning on sufficient statistics improves estimators</p>
</div>
</section>
<section class="slide level2">

<p><em>Example</em>: For <span class="math inline">\(X_1, \ldots, X_n \IIDsim \Bern(p)\)</span>, the statistic <span class="math inline">\(\displaystyle T(\vX) = \sum_{i=1}^n X_i\)</span> is sufficient for <span class="math inline">\(p\)</span>. The estimator <span class="math inline">\(\Theta := X_1\)</span> is unbiased for <span class="math inline">\(p\)</span>, but it is not a function of <span class="math inline">\(T\)</span>. The Raoâ€“Blackwell improvement of <span class="math inline">\(\Theta\)</span> is <span class="math display">\[
\widetilde{\Theta} = \Ex(X_1 \mid T) = \frac{T}{n} =   \frac 1n \sum_{i=1}^n X_i = \barX
\]</span></p>
</section>
<section id="completeness" class="slide level2">
<h2>Completeness</h2>
<p>A statistic <span class="math inline">\(T\)</span> is <em>complete</em> if</p>
<p><span class="math display">\[
\Ex_\vtheta[g(T)] = 0 \quad \forall \vtheta
\quad \implies \quad
g(T)=0 \text{ a.s.}
\]</span></p>
<p>Completeness eliminates hidden unbiased estimators</p>
<p>It is the key hypothesis in Lehmannâ€“ScheffÃ©</p>
</section>
<section class="slide level2">

<p><em>Example</em>: For <span class="math inline">\(X_1, \ldots, X_n \IIDsim \Bern(p)\)</span>, the statistic <span class="math inline">\(\displaystyle T(\vX) = \sum_{i=1}^n X_i\)</span> is sufficient for <span class="math inline">\(p\)</span>. To show that it is complete, note that if <span class="math inline">\(\Ex_p[g(T)] = 0\)</span> <span class="math display">\[\begin{align*}
0 &amp; =\sum_{t=0}^n g(t) \binom{n}{t} p^t (1-p)^{n-t} \\
\implies &amp; 0 =  \sum_{t=0}^n g(t) \binom{n}{t} \left( \frac{p}{1-p} \right)^t
\end{align*}\]</span> Since the right hand side is a polynomial in <span class="math inline">\(p/(1-p)\)</span> with coefficients <span class="math inline">\(g(t) \binom{n}{t}\)</span> that is zero for all <span class="math inline">\(p \in (0,1)\)</span>, we must have <span class="math inline">\(g(t) = 0\)</span> for all <span class="math inline">\(t \in \{0, \ldots, n\}\)</span>, so <span class="math inline">\(g(T) = 0\)</span> a.s.</p>
<p>&nbsp;</p>
<p>Thus, <span class="math inline">\(T(\vX)\)</span> is complete for <span class="math inline">\(p\in (0,1)\)</span></p>
</section>
<section class="slide level2">

<h3 id="minimal-sufficient-notimplies-complete">Minimal sufficient <span class="math inline">\(\notimplies\)</span> complete</h3>
<p>Let <span class="math inline">\(X_1, \ldots, X_n \IIDsim \Norm(\mu,\sigma^2)\)</span>. We show later that the statistic <span class="math inline">\(T(\vX) = (\barX, S^2)\)</span> is minimial sufficient for <span class="math inline">\((\mu,\sigma^2)\)</span></p>
<p>&nbsp;</p>
<p>We know <span class="math inline">\(\barX \sim \Norm(\mu,\sigma^2/n)\)</span>. Define</p>
<p><span class="math display">\[
g(T) = \barX^2 - \frac{\sigma^2}{n} - \mu^2
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
\mathbb{E}[g(T)] = 0
\quad \text{for all } (\mu,\sigma^2)
\]</span></p>
<p>but <span class="math inline">\(g(T)\)</span> is not almost surely zero. Therefore,</p>
<p><span class="math display">\[
T \text{ is not complete}
\]</span></p>
</section>
<section class="slide level2">

<h3 id="complete-notimplies-minimal-sufficient">Complete <span class="math inline">\(\notimplies\)</span> minimal sufficient</h3>
<p>Let <span class="math inline">\(X_1, \ldots, X_n \IIDsim \Bern(p)\)</span> with <span class="math inline">\(n&gt;1\)</span>, and <span class="math inline">\(U=X_1\)</span>.</p>
<p>&nbsp;</p>
<p>We know that <span class="math inline">\(U\)</span> is not sufficient for <span class="math inline">\(p\)</span></p>
<p>&nbsp;</p>
<p>Let <span class="math inline">\(g:\{0,1\}\to\reals\)</span>. Then <span class="math display">\[
\Ex_p[g(U)] = (1-p)g(0) + p g(1) = g(0) + p(g(1)-g(0))
\]</span> If <span class="math inline">\(\Ex_p[g(U)]=0\)</span> for all <span class="math inline">\(p\in(0,1)\)</span>, then this linear polynomial in <span class="math inline">\(p\)</span> must be identically zero, so <span class="math inline">\(g(0)= g(1)=0\)</span></p>
<p>&nbsp;</p>
<p>Hence <span class="math inline">\(g(U)=0\)</span> a.s., and <span class="math inline">\(U\)</span> is complete</p>
</section>
<section id="lehmannscheffÃ©-theorem" class="slide level2">
<h2>Lehmannâ€“ScheffÃ© Theorem</h2>
<p>If</p>
<ul>
<li><span class="math inline">\(T\)</span> is complete and sufficient<br>
</li>
<li><span class="math inline">\(\vTheta = g(T)\)</span> is unbiased estimator of <span class="math inline">\(\theta\)</span></li>
</ul>
<p>then <span class="math inline">\(\vTheta\)</span> is the unique <span class="alert">minimum variance unbiased estimator (MVUE)</span> of <span class="math inline">\(\theta\)</span></p>
<p>This is the main structural route to minimum variance unbiased estimators</p>
</section>
<section id="mvue-for-theta-in-textunif0theta" class="slide level2">
<h2>MVUE for <span class="math inline">\(\theta\)</span> in <span class="math inline">\(\text{Unif}(0,\theta)\)</span></h2>
<p>Let <span class="math inline">\(X_1,\ldots,X_n \IIDsim \Unif(0,\theta)\)</span> with <span class="math inline">\(\theta&gt;0\)</span>. Let <span class="math inline">\(T = X_{(n)} = \max_i X_i\)</span>.</p>
<h3 id="sufficiency-1">1. Sufficiency</h3>
<p>The joint density of the data <span class="math inline">\(\vX = (X_1,\ldots,X_n)\)</span> is</p>
<p><span class="math display">\[
\varrho(\vx\mid\theta)
=
\frac{1}{\theta^n}
\indic(0 &lt; x_{(n)} &lt; \theta)
\]</span></p>
<p>and depends on the sample only through <span class="math inline">\(X_{(n)}\)</span></p>
<p>&nbsp;</p>
<p>Thus <span class="math inline">\(T\)</span> is sufficient (indeed minimal sufficient) for <span class="math inline">\(\theta\)</span></p>
</section>
<section class="slide level2">

<h3 id="completeness-1">2. Completeness</h3>
<p>The pdf of <span class="math inline">\(T = X_{(n)}\)</span> is</p>
<p><span class="math display">\[
\varrho_T(t\mid\theta)
=
\frac{n t^{n-1}}{\theta^n},
\qquad 0&lt;t&lt;\theta
\]</span></p>
<p>Then</p>
<p><span class="math display">\[\begin{align*}
\Ex_\theta[g(T)] = 0 \quad \forall \theta &amp; \implies \int_0^\theta g(t)\frac{n t^{n-1}}{\theta^n}\, \dif t = 0 \quad \forall \theta &gt; 0 \\
&amp; \implies \int_0^\theta g(t) t^{n-1}\, \dif t = 0 \quad \forall \theta &gt; 0 \\
&amp; \implies g(\theta)\theta^{n-1} = 0 \quad \forall \theta &gt; 0 \\
&amp; \implies g(\theta) = 0 \quad \forall \theta &gt; 0
\end{align*}\]</span></p>
<p>Hence <span class="math inline">\(T\)</span> is complete</p>
</section>
<section class="slide level2">

<h3 id="unbiased-estimator">3. Unbiased Estimator</h3>
<p><span class="math display">\[
Ex_\theta[T] = \frac{n}{n+1}\theta, \qquad \text{so } \Theta : =\frac{n+1}{n}T \text{ is unbiased for } \theta
\]</span></p>
<p>&nbsp;</p>
<h3 id="conclusion-lehmannscheffÃ©">4. Conclusion (Lehmannâ€“ScheffÃ©)</h3>
<p>Since <span class="math inline">\(X_{(n)}\)</span> is complete sufficient,</p>
<p><span class="math display">\[
\hat\theta_{\text{MVUE}} = \frac{n+1}{n} X_{(n)}
\]</span></p>
</section>
<section id="cramer-rao" class="slide level2">
<h2>CramÃ©râ€“Rao Lower Bound</h2>
<p>Under regularity conditions,</p>
<p><span class="math display">\[
\var(\Theta)
\ge
\frac{1}{I(\theta)}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
I(\theta)
=
\Ex\!\left[
\left(
\frac{\partial \ell(\theta \mid \vX)}{\partial \theta}
\right)^2
\right]
=
-
\Ex\!
\left(
\frac{\partial^2 \ell(\theta \mid \vX)}{\partial \theta^2}
\right)
\]</span></p>
<p>This gives a universal lower bound on variance of unbiased estimators</p>
</section>
<section class="slide level2">

<h3 id="bernoulli-example">Bernoulli example</h3>
<p>Let <span class="math inline">\(X_1, \dots, X_n \IIDsim \Bern(p)\)</span></p>
<p><span class="math display">\[\begin{align*}
\text{log likelihood} \quad \ell(p)
&amp; =
\sum_{i=1}^n
\left[
X_i \log p
+
(1 - X_i)\log(1-p)
\right] \\
\text{score function} \quad \ell'(p) &amp;=
\sum_{i=1}^n
\left(
\frac{X_i}{p}
-
\frac{1 - X_i}{1-p}
\right)
=
\frac{1}{p(1-p)}
\sum_{i=1}^n (X_i - p) \\
\text{second derivative} \quad \ell''(p) &amp; =
-
\sum_{i=1}^n
\left(
\frac{X_i}{p^2}
+
\frac{1 - X_i}{(1-p)^2}
\right) \\
\text{Fisher information} \quad I_n(p) &amp; =
- \Ex[\ell''(p)] =
\left(
\frac{p}{p^2}
+
\frac{1-p}{(1-p)^2}
\right)
=
\frac{n}{p(1-p)} \\
\text{CRLB} \quad \var(P) &amp; \ge \frac{1}{I_n(p)} = \frac{p(1-p)}{n} = \var(\barX)
\end{align*}\]</span></p>
</section>
<section id="why-are-the-two-definitions-of-fisher-information-equal" class="slide level2">
<h2>Why are the two definitions of Fisher information equal?</h2>
<h3 id="set-up">Set-up</h3>
<ul>
<li><p><span class="math inline">\(\ell(\theta \mid \vX) = \log \bigl( \varrho(\vX \mid \theta) \bigr)\)</span> is the <span class="alert">log-likelihood function</span></p></li>
<li><p><span class="math inline">\(\displaystyle U(\theta \mid \vX) =
\frac{\partial}{\partial \theta}
\ell(\theta \mid X)\)</span> is the <span class="alert">score function</span></p></li>
</ul>
<p>Assume:</p>
<ul>
<li>the support of <span class="math inline">\(\varrho(\vx\mid\theta)\)</span> does not depend on <span class="math inline">\(\theta\)</span></li>
<li>differentiation and integration may be interchanged</li>
</ul>
</section>
<section class="slide level2">

<h3 id="score-has-mean-zero-for-all-theta">Score has mean zero for all <span class="math inline">\(\theta\)</span></h3>
<p><span class="math display">\[\begin{align*}
0 &amp; = \frac{\partial}{\partial \theta}
\int \varrho(\vx \mid \theta)\, \dif \vx \qquad \text{because densities always integrate to 1} \\
&amp; = \int
\frac{\partial}{\partial \theta}
\varrho(\vx \mid \theta)
\, \dif \vx \qquad \text{since differentiation and integration may be interchanged} \\
&amp; = \int
\varrho(\vx \mid \theta) \,
\frac{\partial}{\partial \theta}
\log \varrho(\vx \mid \theta)
\, \dif \vx \qquad \text{since }\frac{\partial}{\partial \theta}\varrho(\vx \mid \theta) = \varrho(\vx \mid \theta) \frac{\partial}{\partial \theta} \log \varrho(\vx \mid \theta) \\
&amp; = \int
\varrho(\vx \mid \theta)
\frac{\partial}{\partial \theta}
\ell(\theta \mid \vx) \, \dif \vx = \Ex[U(\theta \mid \vX)]
\end{align*}\]</span></p>
</section>
<section class="slide level2">

<h3 id="differentiate-again">Differentiate again</h3>
<p><span class="math display">\[\begin{align*}
0 &amp; = \frac{\partial}{\partial \theta} \Ex[U(\theta \mid \vX)] = \frac{\partial}{\partial \theta}
\int \varrho(\vx \mid \theta)\, U(\theta \mid \vx) \, \dif \vx \\
&amp; = \int
\left[
\frac{\partial \varrho(\theta \mid \vx)}{\partial \theta}\, U(\theta \mid \vx)
+
\varrho (\theta \mid \vx) \frac{\partial U(\theta \mid \vx)}{\partial \theta}
\right]
\, \dif \vx  \\
&amp; \qquad \qquad \text{since differentiation and integration may be interchanged} \\
&amp; = \int
\left[ \varrho (\theta \mid \vx)
\{U(\theta \mid \vx)\}^2
+
\varrho (\theta \mid \vx) \frac{\partial U(\theta \mid \vx)}{\partial \theta}
\right]
\, \dif \vx \\
&amp; = \Ex\bigl[U^2(\theta \mid \vX)\bigr]
+
\Ex\!\left[\frac{\partial U(\theta \mid \vX)}{\partial \theta}\right]
\end{align*}\]</span></p>
<p>That is,</p>
<p><span class="math display">\[
I_n(\theta) = \Ex\!\left[
\left(
\frac{\partial \ell(\theta \mid X)}{\partial \theta}
\right)^2
\right] = \Ex\bigl[U^2(\theta \mid \vX)\bigr] = - \Ex\!\left[\frac{\partial U(\theta \mid \vX)}{\partial \theta}\right] = - \Ex\!\left[\frac{\partial^2 \ell(\theta \mid \vX)}{\partial \theta^2}\right]
\]</span></p>
</section>
<section id="conceptual-pipeline" class="slide level2">
<h2>Conceptual Pipeline</h2>
<p>Data<br>
<span class="math inline">\(\longrightarrow\)</span> Sufficient statistic<br>
<span class="math inline">\(\longrightarrow\)</span> Complete sufficient statistic<br>
<span class="math inline">\(\longrightarrow\)</span> MVUE (Lehmannâ€“ScheffÃ©)</p>
<p>This is the classical parametric estimation framework.</p>
</section>
<section id="connection-to-exponential-families" class="slide level2">
<h2>Connection to <a href="#/exp-families">Exponential Families</a></h2>
<p>Recall that for IID data, the <a href="#/exp-families-estimators">log-likelihood for exponential families</a> is <span class="math display">\[
\ell(\vtheta \mid \vX) : = \log(L(\vtheta \mid \vX))
= \sum_{i=1}^n \log \bigl(h(X_i) \bigr) +   n \log \bigl(c(\vtheta) \bigr) +\sum_{k} w_k(\vtheta)\Biggl[ \sum_{i=1}^{n} t_k(X_i) \Biggr]
\]</span></p>
<p>Under regularity conditions <span class="math display">\[
T(\vX) := \left(\sum_{i=1}^n t_1(X_i),  \sum_{i=1}^n t_2(X_i), \ldots \right) \text{ is minimal sufficient for } \vtheta
\]</span></p>
<p>&nbsp;</p>
<p>For one-parameter full exponential families, <span class="math inline">\(T(\vX) := \sum_{i=1}^n t(X_i)\)</span> is typically complete, which makes Lehmann-ScheffÃ© especially powerful</p>
<p>&nbsp;</p>
<p>The Fisher information is often tractable, which makes the CramÃ©râ€“Rao lower bound especially powerful</p>
</section></section>
<section>
<section id="beyond-confidence-intervals" class="title-slide slide level1 center">
<h1>Beyond Confidence Intervals</h1>
<p>So far, our intervals have targeted a <span class="alert">population parameter</span> such as mean, <span class="math inline">\(\mu\)</span>, or variance, <span class="math inline">\(\sigma^2\)</span></p>
<p>But in practice, we may want to answer different questions:</p>
<ol type="1">
<li>Where is the true mean? <a href="#/ci_review">confidence interval</a></li>
<li>Where will the next observation fall? <a href="#/prediction_interval">prediction interval</a></li>
<li>Does most of the population lie within specifications? <a href="#/tolerance_interval">tolerance interval</a></li>
</ol>
<p>These require different kinds of intervals, and different calculations</p>
</section>
<section id="ci_review" class="slide level2">
<h2>Large sample <em>confidence</em> interval (review)</h2>
<p>Under the CLT, a <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval for <span class="math inline">\(\mu\)</span> is</p>
<p><span class="math display">\[
\Prob \left[ \bar X - z_{\alpha/2}\frac{S}{\sqrt{n}} \le \mu \le \bar X + z_{\alpha/2}\frac{S}{\sqrt{n}} \right] = 1-\alpha
\]</span></p>
<ul>
<li>Targets the parameter <span class="math inline">\(\mu\)</span><br>
</li>
<li>Shrinks to zero width as <span class="math inline">\(n\)</span> increases<br>
</li>
<li>Says <em>nothing</em> directly about future observations</li>
</ul>
</section>
<section id="prediction_interval" class="slide level2">
<h2>Large sample <em>prediction</em> interval (Normal data)</h2>
<p>Suppose <span class="math inline">\(X_1,\dots,X_n,X_{n+1}\)</span> are IID <span class="math inline">\(N(\mu,\sigma^2)\)</span>, and we want to predict a <em>new</em> observation:</p>
<p><span class="math display">\[X_{n+1}
=
(X_{n+1} - \mu)
-
(\barX_n - \mu) + \barX_n\]</span></p>
<p>in relation to <span class="math inline">\(\barX_n\)</span>. Note that</p>
<p><span class="math display">\[
\var(X_{n+1} - \barX_n)
=
\sigma^2 + \frac{\sigma^2}{n}
=
\sigma^2\left(1 + \frac{1}{n}\right)
\]</span></p>
<p>since <span class="math inline">\(X_{n+1}\)</span> and <span class="math inline">\(\barX_n\)</span> are independent. Thus, a <span class="math inline">\(100(1-\alpha)\%\)</span> prediction interval for <span class="math inline">\(X_{n+1}\)</span> is</p>
<p><span class="math display">\[
\Prob\left( \barX_n - z_{\alpha/2}\, S \sqrt{1 + \frac{1}{n}} \le X_{n+1} \le \barX_n + z_{\alpha/2}\, S \sqrt{1 + \frac{1}{n}} \right) = 1-\alpha
\]</span></p>
<ul>
<li><p>Targets a future observation <span class="math inline">\(X_{n+1}\)</span></p></li>
<li><p>Does <em>not</em> shrink to zero width as <span class="math inline">\(n\)</span> increases</p></li>
</ul>
</section>
<section id="tolerance_interval" class="slide level2">
<h2>Large sample <em>tolerance</em> interval for <span class="math inline">\(\Norm(\mu,\sigma^2)\)</span> data (different again)</h2>
<p>Assume <span class="math inline">\(X \sim N(\mu,\sigma^2)\)</span>. Sometimes the question is:</p>
<div class="key-point">
<p>Can we say that at least 95% of the population lies within an interval?</p>
</div>
<p>A <span class="math inline">\((1-\alpha,\gamma)\)</span> <strong>tolerance interval</strong> consists of random endpoints <span class="math inline">\(X_-\)</span> and <span class="math inline">\(X_+\)</span> such that</p>
<p><span class="math display">\[
\Prob\!\left[
\Prob(X_- \le X \le X_+) \ge 1-\alpha
\right]
=
\gamma
\]</span></p>
<ul>
<li>The inner probability is taken over the population distribution of <span class="math inline">\(X\)</span></li>
<li>The outer probability is taken over repeated samples used to compute <span class="math inline">\(X_-\)</span> and <span class="math inline">\(X_+\)</span></li>
</ul>
</section>
<section class="slide level2">

<p>For <span class="math inline">\(X \sim N(\mu,\sigma^2)\)</span>, the central <span class="math inline">\(1-\alpha\)</span> proportion of the population is <span class="math inline">\(\mu \pm z_{\alpha/2}\sigma\)</span></p>
<p>So a <span class="math inline">\((1-\alpha,\gamma)\)</span> tolerance interval is approximately</p>
<p><span class="math display">\[
\Prob \left [\Prob \left( \barX_n - z_{\alpha/2} S
\sqrt{
1+\frac{z_\gamma^2}{2n}} \le X \le \barX_n + z_{\alpha/2} S \sqrt{1 + \frac{z_\gamma^2}{2n}} \right) \ge 1-\alpha  \right ] = \gamma
\]</span></p>
<ul>
<li>Targets population coverage</li>
<li>Does not shrink to zero width</li>
<li>Is typically wider than a prediction interval</li>
</ul>
</section>
<section id="comparison-of-interval-half-widths" class="slide level2">
<h2>Comparison of interval half-widths</h2>
<table class="caption-top">
<colgroup>
<col style="width: 66%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Interval type</th>
<th>Half-width</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Confidence interval for <span class="math inline">\(\mu\)</span></td>
<td><span class="math inline">\(\displaystyle z_{\alpha/2} \frac{S}{\sqrt{n}}\)</span></td>
</tr>
<tr class="even">
<td>Prediction interval for <span class="math inline">\(X_{n+1}\)</span></td>
<td><span class="math inline">\(\displaystyle z_{\alpha/2} S \sqrt{1 + \frac{1}{n}}\)</span></td>
</tr>
<tr class="odd">
<td>Tolerance interval for <span class="math inline">\(1-\alpha\)</span> coverage</td>
<td><span class="math inline">\(\displaystyle z_{\alpha/2} S \sqrt{1 + \frac{z_\gamma^2}{2n}}\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="manufacturing-example-parameterized" class="slide level2">
<h2>Manufacturing example (parameterized)</h2>
<div id="f6de13ac" class="cell" data-message="false" data-execution_count="12">
<div class="cell-output cell-output-display cell-output-markdown">
<ul>
<li><span class="math inline">\(n=25\)</span>, <span class="math inline">\(\barx=10.0200\)</span>, <span class="math inline">\(s=0.2000\)</span></li>
<li>Specs: <span class="math inline">\([9.90,\ 10.10]\)</span></li>
<li>CI/PI level : <span class="math inline">\(1-\alpha=95\%\)</span> (<span class="math inline">\(\alpha=0.05\)</span>)</li>
<li>TI: coverage <span class="math inline">\(1-\alpha_\mathrm{cov}=95\%\)</span>, confidence <span class="math inline">\(\gamma=99\%\)</span></li>
</ul>
<p>Computed (large-sample) intervals:</p>
<ul>
<li>CI for <span class="math inline">\(\mu\)</span>: <span class="math inline">\((9.9416,\ 10.0984)\)</span> âœ… inside specs</li>
<li>PI for <span class="math inline">\(X_{n+1}\)</span>: <span class="math inline">\((9.6202,\ 10.4198)\)</span> âŒ not inside specs</li>
<li>TI (approx): <span class="math inline">\((9.6073,\ 10.4327)\)</span> âŒ not inside specs</li>
</ul>
</div>
</div>
</section>
<section class="slide level2">

<div id="1feaeffb" class="cell" data-message="false" data-execution_count="13">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="02-estimator_files/figure-revealjs/cell-14-output-1.png" width="1142" height="568"></p>
</figure>
</div>
</div>
</div>
<p>&nbsp;</p>
<div class="deck-nav">
  <a class="deck-prev" href="01-intro.html">â† Introduction and Probability Review</a>
  <a class="deck-next" href="03-hypothesis.html">Hypothesis Testing â†’</a>
</div>

</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<div class="fh-footer">
<p><span class="fh-footer-main"> Â© 2026 Fred J. Hickernell Â· Illinois Institute of Technology <span class="fh-footer-sep">Â·</span> <span class="fh-footer-deck"> Estimators &amp; CIs </span> <span class="fh-footer-sep">Â·</span> <a id="fh-course-website" href="#" target="_blank" rel="noopener" onmouseenter="(function(a){var el=document.getElementById('course-website-meta'); if(!el) return; var url=(JSON.parse(el.textContent||'{}').course_website||'').trim(); if(url) a.setAttribute('href',url);})(this)" onfocus="(function(a){var el=document.getElementById('course-website-meta'); if(!el) return; var url=(JSON.parse(el.textContent||'{}').course_website||'').trim(); if(url) a.setAttribute('href',url);})(this)">MATH 563 â€” Mathematical Statistics Course Website</a> <span class="fh-footer-sep">Â·</span> <span class="fh-footer-ex"> <span class="math inline">\(\exstar\)</span> = exercise for the reader </span> </span></p>
</div>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="02-estimator_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="02-estimator_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="02-estimator_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="02-estimator_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="02-estimator_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="02-estimator_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="02-estimator_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="02-estimator_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="02-estimator_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="02-estimator_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'grid',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1600,

        height: 1000,

        // Factor of the display size that should remain empty around the content
        margin: 0,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script>
    (function () {
      function setScale() {
        if (window.Reveal && typeof window.Reveal.getScale === "function") {
          const s = window.Reveal.getScale();
          document.documentElement.style.setProperty("--reveal-scale", String(s));
          window.__fh_reveal_scale = s; // debug: lets us confirm script ran
          return true;
        }
        return false;
      }

      function hookEvents() {
        if (window.Reveal && typeof window.Reveal.on === "function") {
          window.Reveal.on("ready", setScale);
          window.Reveal.on("resize", setScale);
          window.Reveal.on("slidechanged", setScale);
        }
        window.addEventListener("resize", setScale);
      }

      function startPolling() {
        let tries = 0;
        const timer = setInterval(function () {
          tries += 1;
          const ok = setScale();
          if (ok) {
            hookEvents();
            clearInterval(timer);
          }
          if (tries > 200) { // ~10s at 50ms
            clearInterval(timer);
          }
        }, 50);
      }

      startPolling();
      document.addEventListener("fullscreenchange", setScale);
      document.addEventListener("webkitfullscreenchange", setScale);
      document.addEventListener("visibilitychange", setScale);
      setScale();
    })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>