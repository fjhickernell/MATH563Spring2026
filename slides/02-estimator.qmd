---
title: "MATH 563 — Mathematical Statistics"
subtitle: |
  <span class="deck-title">Estimators and Confidence Intervals</span><br>
  <span class="deck-meta">{{< meta texts.cb.short >}} Ch. 5–7 (parts) <br>
  Assignment 3 due 2/13
  </span>
author: "Fred J. Hickernell"
date: last-modified
date-format: "MMMM D, YYYY"
reveal-options:
  disableLayout: false
---


{{< include ../classlib/classlib/quarto/slides/shared/general/how_to_learn.qmd >}}


{{< include ../classlib/classlib/quarto/slides/shared/statistics/estimation/estimators.qmd >}}

## Estimators for exponential families of distributions

Recall that exponential families of distributions have PMF or PDF that can be expressed as
$$
\varrho(\vx\mid \vtheta) = h(\vx) \, c(\vtheta) \, \exp \biggl(\sum_k w_k(\vtheta) t_k(\vx) \biggr) \quad \vx \in \reals^d
$$
The binomial, exponential, and normal are all exponential families

### 1. Likelihood equations depend on sample averages

The log-likelihood for IID $X_1,\dots,X_n$ is
$$
\ell(\vtheta) : = \log(L(\vtheta))
= \sum_{i=1}^n \log \bigl(h(\vX_i)) + 	n \log \bigl( c(\vtheta) \bigr) +\sum_{k} w_k(\vtheta)\Biggl[ \sum_{i=1}^{n} t_k(\vX_i) \Biggr]
$$
Solving for the MLE just depends on the data via the averages: $\displaystyle \frac 1n \sum_{i=1}^{n} t_k(\vX_i)$

- $\Bern(\mu)$ and $\Exp(1/\mu)$: $\displaystyle \frac 1n \sum_{i=1}^{n} X_i$, &nbsp; &nbsp; &nbsp;  $\Norm(\mu,\sigma^2)$: $\displaystyle \frac 1n \sum_{i=1}^{n} X_i$ and $\displaystyle \frac 1n \sum_{i=1}^{n} X_i^2$

---

### 2. Sampling distributions are often known
Because sums or averages of exponential family data have known
distributions, the distribution of the estimator is often explicit

- $\Bern(p)$: $n\barX \sim \text{Binomial}(n,p)$
- $\Exp(\lambda)$: $\barX \sim \Gam(n,\;n\lambda)$
- $\Norm(\mu,\sigma^2)$: $\barX \sim \Norm(\mu,\sigma^2/n)$


This buys us

- Exact bias and variance, e.g. for $X \sim \Exp(\lambda)$, $\hlambda_{\MLE} \exeq 1/\barX$ and $\displaystyle \Ex(\hlambda_{\MLE}) \exeq \frac{n \lambda}{n-1}$ for $n > 1$
- Exact or near-exact confidence intervals

---

### 3. Bias behavior is transparent
Although they may be [asymptotically]{.alert} unbiased many MLEs are biased for finite sample size but the bias is computable (see previous slide).  Thus, the bias can be analyzed
exactly and typically decreases as $n$ grows.

Although 

$$\theta_2 = g(\theta_1) \implies \Theta_{2,\MLE} = g(\Theta_{1,\MLE})$$

one also has

$$ \Theta_{1,\MLE} \text{ unbiased } \notimplies \Theta_{2,\MLE} \text{ unbiased}$$



{{< include ../classlib/classlib/quarto/slides/shared/statistics/estimation/confidence.qmd >}}