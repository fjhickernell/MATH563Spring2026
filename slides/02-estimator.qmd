---
title: "MATH 563 — Mathematical Statistics"
subtitle: |
  <span class="deck-title">Estimators and Confidence Intervals</span><br>
  <span class="deck-meta">{{< meta texts.cb.short >}} Ch. 5–7 (parts) <br>
  Assignment 3 due 2/13 <br>
  Test 1 on 2/26
  </span>
author: "Fred J. Hickernell"
date: today
date-format: "MMMM D, YYYY"
reveal-options:
  disableLayout: false
---


{{< include ../classlib/classlib/quarto/slides/shared/general/how_to_learn.qmd >}}


{{< include ../classlib/classlib/quarto/slides/shared/statistics/estimation/estimators.qmd >}}

## Estimators for exponential families of distributions

Recall that exponential families of distributions have PMF or PDF that can be expressed as
$$
\varrho(\vx ; \vtheta) = h(\vx) \, c(\vtheta) \, \exp \biggl(\sum_k w_k(\vtheta) t_k(\vx) \biggr) \quad \vx \in \reals^d
$$
The binomial, exponential, and normal are all exponential families

### 1. Likelihood equations depend on sample averages

The log-likelihood for IID $X_1,\dots,X_n$ is
$$
\ell(\vtheta) : = \log(L(\vtheta))
= \sum_{i=1}^n \log \bigl(h(\vX_i) \bigr) + 	n \log \bigl( c(\vtheta) \bigr) +\sum_{k} w_k(\vtheta)\Biggl[ \sum_{i=1}^{n} t_k(\vX_i) \Biggr]
$$
Solving for the MLE depends on the data only through the averages: $\displaystyle \frac 1n \sum_{i=1}^{n} t_k(\vX_i)$

- $\Bern(\mu)$ and $\Exp(\lambda)$: $\displaystyle \frac 1n \sum_{i=1}^{n} X_i$, &nbsp; &nbsp; &nbsp;  $\Norm(\mu,\sigma^2)$: $\displaystyle \frac 1n \sum_{i=1}^{n} X_i$ and $\displaystyle \frac 1n \sum_{i=1}^{n} X_i^2$

---

### 2. Sampling distributions are often known
Because sums or averages of exponential family data have known
distributions, the distribution of the estimator is often explicit

- $\Bern(p)$: $n\barX \sim \text{Binomial}(n,p)$
- $\Exp(\lambda)$: $\barX \sim \Gam(n,\;n\lambda)$ (shape–rate)
- $\Norm(\mu,\sigma^2)$: $\barX \sim \Norm(\mu,\sigma^2/n)$


This buys us

- Exact bias and variance, e.g. for $X \sim \Exp(\lambda)$, $\hlambda_{\MLE} \exeq 1/\barX$ and $\displaystyle \Ex(\hlambda_{\MLE}) \exeq \frac{n \lambda}{n-1}$ for $n > 1$
- Exact or near-exact confidence intervals

---

### 3. Bias behavior is transparent
Although many MLEs are asymptotically unbiased, they are biased for finite sample size, but the bias is computable

Although 

$$\theta_2 = g(\theta_1) \implies \Theta_{2,\MLE} = g(\Theta_{1,\MLE})$$

one also has

$$ \Theta_{1,\MLE} \text{ unbiased } \notimplies \Theta_{2,\MLE} \text{ unbiased}$$



{{< include ../classlib/classlib/quarto/slides/shared/statistics/estimation/confidence.qmd >}}


# Do we have the right estimators to summarize our data and construct confidence intervals?

- [Sufficiency](#sufficiency)


## Sufficiency {#sufficiency}

Let $\vX =(X_1,\dots,X_n)$ have joint PMF/PDF $\varrho(\vx \mid \vtheta)$.

&nbsp;

A statistic $T(\vX)$ is [sufficient]{.alert} for $\vtheta$ if

the conditional distribution of $\vX$ given $T(\vX)$

does **NOT** depend on $\vtheta$.

&nbsp;

> If a sufficient statistic, $T$, is known, then the sample contains no additional information about $\vtheta$

## Factorization Theorem

$T(\vX)$ is [sufficient]{.alert} for $\vtheta$ iff

\begin{align*}
\varrho(\vx \mid \vtheta) = L(\vtheta \mid \vx)
& =
g(T(\vx), \vtheta)\, h(\vx) \\
\text{or equivalently } \qquad \ell(\vtheta \mid \vx)  = \log(L(\vtheta \mid \vx)) & = \tg(T(\vx), \vtheta) + \th(\vx)
\end{align*}

where

-   $g$ depends on data only through $T$
-   $h$ does not depend on $\vtheta$

This is the fundamental practical tool for proving sufficiency

_Example_: For $X_1, \ldots, X_n \IIDsim \Bern(p)$, the log likelihood is
$$
\ell(p \mid \vx) = \sum_{i=1}^n \left [ x_i \log(p) + (1-x_i)\log(1-p) \right]
= \left(\sum_{i=1}^n x_i \right) [\log(p) - \log(1-p)]  + n\log(1-p)
$$
so $\displaystyle T(\vX) = \sum_{i=1}^n X_i$ is sufficient for $p$

---

$\exstar$ For $X_1, \ldots, X_n \IIDsim \Exp(\lambda)$, the statistic $\displaystyle T(\vX) = \sum_{i=1}^n X_i$ is sufficient for $\lambda$

$\exstar$ For $X_1, \ldots, X_n \IIDsim \Norm(\mu,\sigma^2)$, the statistic $\displaystyle T(\vX) = \sum_{i=1}^n X_i$ is not sufficient for $\mu$ and $\sigma^2$.  

$\exstar$ Find a sufficient (vector) statistic for $\mu$ and $\sigma^2$ for $X_1, \ldots, X_n \IIDsim \Norm(\mu,\sigma^2)$


## Minimal Sufficiency

A sufficient statistic $T$ is *minimal sufficient* if

-   it is sufficient, and

-   it is a function of every other sufficient statistic.

> A minimial sufficient statistic compresses the data as much as possible\
> without losing information about $\vtheta$

**Theorem**: A statistic $T(\vX)$ is minimal sufficient if for all sample points $\vx$, $\vy$,

$$
\left . \begin{array}{r} \displaystyle \frac{L(\vtheta \mid \vx)}{L(\vtheta \mid \vy)} \\
\text{ or equivalently} \quad \ell(\vtheta \mid \vx) - \ell(\vtheta \mid \vy) \end{array} \right \}
\text{ is independent of } \vtheta
\quad
\Longleftrightarrow
\quad
T(\vx) = T(\vy)
$$

---

_Proof_: Suppose that $\ell(\vtheta \mid \vx) - \ell(\vtheta \mid \vy)$ is independent of $\vtheta$ iff $T(\vx) = T(\vy)$. 

Let 

- $\cx$ be the sample space of the data, $\vX$
- $\ct : = T(\vtheta \mid \cx)$ be the image of $T$

For each $t \in \ct$, find one $\vx_t \in \cx$ such that $T(\vx_t) = t$, and define 

- $\th(x) : = \ell(\vtheta \mid \vx) - \ell(\vtheta \mid \vx_{T(\vx)})$, and
- $\tg(t,\vtheta) : = \ell(\vtheta \mid \vx_t)$

Then for any $\vx \in \cx$,

$$
\ell(\vtheta \mid \vx) = \th(\vx) + \ell(\vtheta \mid \vx_{T(\vx)}) = \th(\vx) + \tg(T(\vx),\vtheta)  
$$

so $T$ is sufficient

---

To show that $T(\vX)$ is minimal

-   Let $T'(\vX)$ be any other sufficient statistic, so

-  $\ell(\vtheta \mid \vx) = \tg'(T'(\vx),\vtheta) + \th'(\vx)$

For any $\vx, \vy \in \cx$ with $T'(\vx) = T'(\vy)$, we have
$$
\ell(\vtheta \mid \vx) - \ell(\vtheta \mid \vy) = \tg'(T'(\vx),\vtheta) + \th'(\vx) - \tg'(T'(\vy),\vtheta) - \th'(\vy) = \th'(\vx) - \th'(\vy)$$
which does not depend on $\vtheta$.  Thus, $T(\vx) = T(\vy)$.

Since $T'(\vx) = T'(\vy)$ implies $T(\vx) = T(\vy)$, it follows that  $T$ is a function of $T'$  $\qquad \square$

_Example_: For $X_1, \ldots, X_n \IIDsim \Bern(p)$, $\displaystyle T(\vX) = \sum_{i=1}^n X_i$ is a sufficient statistic for $p$.  To show that it is minimal sufficient, note that for any $\vx$, $\vy$,
$$
\ell(p \mid \vx) - \ell(p \mid \vy) = \bigl(T(\vx) - T(\vy) \bigr) [\log(p) - \log(1-p)]  = 0 \iff T(\vx) = T(\vy)
$$
so $T(\vX)$ is minimal sufficient for $p$

## Completeness

A statistic $T$ is *complete* if

$$
\Ex_\vtheta[g(T)] = 0 \quad \forall \vtheta
\quad \implies \quad
g(T)=0 \text{ a.s.}
$$

Completeness eliminates hidden unbiased estimators.

It is the key hypothesis in Lehmann--Scheffé.

------------------------------------------------------------------------

## Rao--Blackwell Theorem

Let $\hat{\vtheta}$ be unbiased and let $T$ be sufficient.

Define

$$
\tilde{\vtheta} = \E[\hat{\vtheta} \mid T].
$$

Then

-   $\tilde{\vtheta}$ is unbiased\
-   $\Var(\tilde{\vtheta}) \le \Var(\hat{\vtheta})$

Conditioning on sufficient statistics improves estimators.

------------------------------------------------------------------------

## Lehmann--Scheffé Theorem

If

-   $T$ is complete and sufficient\
-   $\hat{\vtheta} = g(T)$ is unbiased

then $\hat{\vtheta}$ is the unique MVUE.

This is the main structural route to minimum variance unbiased
estimators.

------------------------------------------------------------------------

## Cramér--Rao Lower Bound

Under regularity conditions,

$$
\Var(\hat{\vtheta})
\ge
\frac{1}{I(\vtheta)}
$$

where

$$
I(\vtheta)
=
\E\!\left[
\left(
\frac{\partial}{\partial \vtheta}
\log \varrho(\vX \mid \vtheta)
\right)^2
\right].
$$

This gives a universal lower bound on variance of unbiased estimators.

------------------------------------------------------------------------

## Conceptual Pipeline

Data\
$\longrightarrow$ Sufficient statistic\
$\longrightarrow$ Complete sufficient statistic\
$\longrightarrow$ MVUE (Lehmann--Scheffé)

This is the classical parametric estimation framework.

------------------------------------------------------------------------

## Connection to Exponential Families

We previously defined exponential families as

$$
\varrho(\vx\mid \vtheta)
=
h(\vx)\, c(\vtheta)\,
\exp\!\left(
\sum_k w_k(\vtheta) t_k(\vx)
\right).
$$

For IID data,

$$
\sum_{i=1}^n t_k(\vX_i)
$$

is automatically sufficient for $\vtheta$.

In regular cases, this statistic is also

-   minimal sufficient\
-   complete

which makes Lehmann--Scheffé especially powerful.

------------------------------------------------------------------------

## Why Exponential Families Make Everything Work

Because of their structure:

-   finite-dimensional sufficient statistics\
-   minimal sufficiency automatically\
-   completeness (in full regular families)\
-   tractable Fisher information\
-   explicit sampling distributions

This is why classical parametric inference is so clean for

Bernoulli, Poisson, Exponential, Normal, Gamma, etc.
