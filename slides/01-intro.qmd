---
title: "MATH 563 â€” Mathematical Statistics"
subtitle: |
  <span class="deck-title">Introduction and Probability Review</span><br>
  <span class="deck-meta">Casella &amp; Berger Ch. 1â€“4 (parts) <br> 
  Diagnostic Quiz due 1/16 <br>
  Assignment 2 due 1/30</span>
author: "Fred J. Hickernell"
date: last-modified
date-format: "MMMM D, YYYY"
reveal-options:
  disableLayout: false
---

# Statistics{data-state="goldborder"}

<div class="h3">Principled</div>
- Clear assumptions
- Probability as a basis
- Efficient and accurate computation

<div class="h3">Inference about a population</div>
- Want to know parameters that describe a large number of individuals or the relationships among them
- Should provide a level of uncertainty

<div class="h3">From Data</div> 
- Carefully sampled
- Observations, surveys, experiments, or computer simulations

&nbsp;

See [Big Statistics Questions](/pages/stats-qs.html)

# Examples

Let's look at 

- [Approval ratings](#approval-ratings)
- [Life expectancy explained by gross domestic (GDP) product per capita](#life-gdp)
- [Numerical integration (quadrature)](#trapezoidal)

## Approval ratings {#approval-ratings}

The Gallup Poll tracks the [approval ratings of US presidents](https://news.gallup.com/poll/203198/presidential-approval-ratings-donald-trump.aspx) according to a [careful polling methodology](https://www.gallup.com/224855/gallup-poll-work.aspx)

- Each week they telephone $n=1500$ adults 
- Their sampling error for the approval ratings is about $4\%$

Let $Y \sim \Bern(\mu)$, a [Bernoulli]{.alert} (free throw shooting) distribution with probability of success $\mu$, 
$$
\Prob(Y =y) = \begin{cases} \mu, & y=1, \text{ (yes, approve)}\\
1-\mu, & y = 0, \text{ (no, do not approve)}
\end{cases}
$$

:::: {.columns}
::: {.column width="70%"}

If $Y_1, \ldots, Y_n \IIDsim \Bern(\mu)$, then
\begin{align*}
T &:= Y_1 + \cdots + Y_n \sim \Bin(n,\mu), \; \Prob(T = t) = \binom{n}{t} \mu^t (1-\mu)^{n-t}\\
\barY &:= \frac 1n (Y_1 + \cdots + Y_n) \\
& \appxsim  \Norm\bigl(\mu,\mu(1-\mu)/n\bigr) \quad \text{by the }\class{alert}{\text{Central Limit Theorem}}
\end{align*}
:::

::: {.column width="27%"}

![](/classlib/classlib/notebooks/figures/approval_rating/ybar_histogram_approval.png){fig-align="center" width="100%"}
:::

::::

## Approval ratings (cont'd)

If $Y_1, \ldots, Y_n \IIDsim \Bern(\mu)$, then we can construct a [confidence interval]{.alert} that [captures]{.alert} the true approval rating with high probability:
\begin{align*}
T &:= Y_1 + \cdots + Y_n \sim \Bin(n,\mu), \quad \Prob(T = y) = \binom{n}{t} \mu^t (1-\mu)^{n-t}\\
\barY &:= \frac 1n (Y_1 + \cdots + Y_n) \appxsim  \Norm\bigl(\mu,\mu(1-\mu)/n\bigr) \quad \text{by the }\class{alert}{\text{Central Limit Theorem}}
\end{align*}

:::: {.columns}

::: {.column width="75%"}

<div class="fragment" data-fragment-index="1" style="width:100%; margin:0;" markdown="1">
\begin{align*}
95\% & \approx \Prob\Bigl[\mu - 1.96\sqrt{\mu(1-\mu)/n} \le \class{alert}{\barY} \le \mu + 1.96\sqrt{\mu(1-\mu)/n}\Bigr] \\
& = \Prob\Bigl[\barY - 1.96\sqrt{\mu(1-\mu)/n} \le \class{alert}{\mu} \le \barY + 1.96\sqrt{\mu(1-\mu)/n}\Bigr] \\
& \approx \Prob\Bigl[\barY - 1.96\sqrt{\barY(1-\barY)/n} \le \class{alert}{\mu} \le \barY + 1.96\sqrt{\barY(1-\barY)/n}\Bigr] \\
& \le \Prob\Bigl[\barY - 1/\sqrt{n} \le \class{alert}{\mu} \le \barY + 1/\sqrt{n} \Bigr] \quad \text{since } \sqrt{\barY(1-\barY)} \le 1/2
\end{align*}
For $n = 1000$ we get $1/\sqrt{n} \approx 3\%$.
<a class="button" style="margin-left:2em;" href="../classlib/classlib/notebooks/approval_rating_bernoulli_binomial_clt_ci.ipynb" download>
â¬‡ Approval Rating Jupyter ðŸ““
</a>
</div>

:::

::: {.column width="23%"}

<span class="fragment" data-fragment-index="1">
<img src="/classlib/classlib/notebooks/figures/approval_rating/ci_failure_approval.png" style="width:100%; display:block; margin:0 auto;">
</span>

:::

:::: 

## Approval ratings (cont'd)

- [Population]{.alert} is US adults
- [Model]{.alert} 
  - Bernoulli distribution
  - Ignores demographic factors that might explain approvals
- [Uncertainty]{.alert} of the estimate of the proportion, $\mu$
  - Provided by a [confidence interval (CI)]{.alert}
  - Facilitated by the [Central Limit Theorem]{.alert}
  - This is the CI capturing $\mu$, _not_ probability of $\mu$ in the CI

## Life expectancy vs gross domestic product (GDP) per capita {#life-gdp}

Countries differ dramatically in both [life expectancy]{.alert} and
[economic output]{.alert}. A classic dataset (World Bank / Gapminder) records, for each country,

- Life $=$ Life expectancy at birth (years)
- GDP $=$ Gross domestic product per capita (USD, purchasing-power adjusted)

Each point represents **one country**, not one individual.  We are interested in the conditional behavior of life expectancy with given national income:

:::: {.columns}
::: {.column width="50%"}

$$
\Ex(\text{Life} | \text{GDP} = x)
$$
:::

::: {.column width="47%"}

![](/classlib/classlib/notebooks/figures/health_wealth_gapminder/health_wealth_linear_scale.png){fig-align="center" width="100%"}
:::

::::


## Life expectancy vs GDP (cont'd)

The scatterplot of life expectancy versus GDP per capita reveals two important statistical features.

### Nonlinearity
Increases in GDP have a [much larger effect]{.alert} on life expectancy at low
income levels than at high income levels

### Heteroscedasticity
Countries with low GDP exhibit [much greater variability]{.alert} in life
expectancy

:::: {.columns}
::: {.column width="65%"}

Take logarithm to treat this
$$
\Ex(\text{Life}  \mid \text{GDP} = x) \approx \alpha + \beta \log(x)
$$

- $\beta$ measures years of life gained per [multiplicative]{.alert} increase in GDP
- The log scale naturally reflects [diminishing returns]{.alert}

<a class="button" style="margin-left:2em;" href="../classlib/classlib/notebooks/health_wealth_gapminder_regression.ipynb" download>
â¬‡ Health vs. Wealth ðŸ““
</a>
:::

::: {.column width="35%"}

![](/classlib/classlib/notebooks/figures/health_wealth_gapminder/health_wealth_semilogx_scale.png){fig-align="center" width="100%"}


:::

::::


## Life expectancy vs GDP (cont'd)

- [Population]{.alert} is all hypothetical countries
- [Model]{.alert} 
  - Linear regression 
  - Incorporates transformation of the explanatory variable
- [Uncertainty]{.alert}
  - $R^2$ measures the proportion of variance in $Y$ captured by the model
- [Interpretation]{.alert}
  - This describes relationship, not cause and effect
  - The variation in the residuals point to other explanatory factors that have been left out of the model

## Numerical integration {#trapezoidal}

Computational mathematics seems distinct from statistics, but there is an overlap.

Suppose that $f$ is a random function defined in $[0,1]$, in particular, a [Gaussian process]{.alert}, i.e., $f \sim \GP(0,K)$.  This means that for any distinct $x_1, \ldots, x_n \in [0,1]$, the random vector of function values, $\vf := \bigl(f(x_1), \ldots, f(x_n) \bigr)^\top$, has a [multivariate Gaussian distribution]{.alert} with 

- [Mean]{.alert} $\vmu = \Ex(\vf) = \vzero$
- [Covariance matrix]{.alert} $\mSigma := \Ex(\vf \vf^\top) = \bigl(\Ex[f(x_i)f(x_j)]\bigr)_{i,j=1}^n  = \bigl(K(x_i,x_j)\bigr)_{i,j=1}^n =: \mK$

<div class="fragment" data-fragment-index="1" style="width:100%; margin:0;" markdown="1">
If this is the Bayesian prior belief about $f$, then the [Bayesian posterior]{.alert} mean of the function conditioned on the data $\vf = \vy$ is
$$
\Ex\bigl[f(x) \mid \vf  = \vy\bigr] = \vy^\top \mK^{-1} \vk(x) , \quad 
\text{where } \vk(x) = \bigl( K(x,x_i) \bigr)_{i=1}^n
$$

The Bayesian posterior mean integral of $f$ conditioned on the data is

$$
\Ex\biggl[\int_0^1 f(x) \, \dif x \mid \vf  = \vy\biggr] = \vy^\top \mK^{-1} \int_0^1 \vk(x) \, \dif x
$$

For $K(t,x) = 2 - \lvert t - x\rvert$ this is the [trapezoidal rule]{.alert}.  <a class="button" style="margin-left:2em;" href="../classlib/classlib/notebooks/Bayesian_quadrature.ipynb" download>
â¬‡ Bayesian Quadrature ðŸ““
</a>
</div>

## Numerical integration (cont'd)

- [Population]{.alert} all (continuous) functions
- [Model]{.alert} 
  - Stochastic processes
  - Bayesian inference
  - Often one would the parameters in $K$ from data
- [Uncertainty]{.alert}
  - You may construct Bayesian credible intervals, but they depend on your confidence in choosing a reasonable $K$
- [Interpretation]{.alert}
  - Provides a statistical interpretation of computational mathematics

# Class Complexion

| Familiarity | Statistics | Probability | Matrix algebra | Multivariate calculus | Computer programming / coding |
|-------------|------------|-------------|----------------|-----------------------|--------------------------------|
| Not at all	| 1| 	0	| 0	| 0	| 0|
| A bit	| 2	| 1	| 0	| 2	| 2| 
| Somewhat	| 5	| 4	| 4	| 2	| 5| 
| Rather	| 6	| 8	| 5	| 6	| 4| 
| Very	| 0	| 1 | 5	| 4| 	3| 

## Class complexion (cont'd)

|                    | Programming environments |
|--------------------|--------------------------|
Python | 14
R  | 6
MATLAB | 	4
C/C++ | 	5
Java | 1

# Probability Review

- [Key concepts](#prob-concepts)
- [Important distributions](#prob-distrib)
- [Conditional probability](#conditional-prob)
- [Types of convergence](#type-conv)
- [Central Limit Theorem](#clt)

## Key probability concepts{#prob-concepts}

- [Outcome]{.alert} â€” single possible result of a random process, e.g., yes, $0.5$, $(1,-2)$

- [Sample space]{.alert} â€“ set of all possible outcomes, e.g., $\{\text{yes}, \text{no} \}$, $\reals$

- [Event]{.alert} â€“ set of outcomes (subset of the sample space), e.g., $[0,\infty)$

- [Event space]{.alert} â€“ collection of all possible events 

- [Probability]{.alert} â€“ function  $\Prob:\text{event space}\to[0,1]$ satisfying
  - $0 \le \Prob(\text{event}) \le 1$
  - $\mathbb{P}(\text{sample space})=1$
  - If $A \cap B=\varnothing$, then $\Prob(A\cup B)=\Prob(A)+\Prob(B)$


## Key probability concepts (cont'd)

- [Random variable/vector/function]{.alert} â€” mathematical quantity representing an outcome

- [Cumulative distribution function (CDF)]{.alert} $F$ of random variable $X$ 
  -  $F(x) : =  \Prob(X \le x)$

- [Probability mass function (PMF)]{.alert} $\varrho$ of discrete random variable $X$ 
  - $\varrho(x):= \Prob(X = x)$

- [Probability density function (PMF)]{.alert} $\varrho$ of continuous random variable $X$ 
  - $\varrho(x):= F'(x)$

- [Expectation]{.alert} â€” average value of a random variable, weighted by its probability
  - $\displaystyle \Ex[f(X)] : = \sum_{x} f(x) \varrho(x)$  for discrete random variables
  - $\displaystyle \Ex[f(X)] : =  \int f(x) \varrho(x) \; \dif x$  for continuous random variables

- [Mean]{.alert} of $X$ is $\mu : = \Ex(X)$

- [Variance]{.alert} of $X$ is $\sigma^2 : = \var(X) := \Ex[(X - \mu)^2] = E(X^2) - \mu^2$
  - [Standard deviation]{.alert} of $X$ is  $\std(X) := \sigma = \sqrt{\var(X)}$

## Key probability concepts (cont'd)

- $(X,Y)$ are [independent]{.alert} iff $F_{X,Y}(x,y) = F_X(x) F_Y(y)$ or equivalently $\varrho_{X,Y}(x,y) = \varrho_X(x) \varrho_Y(y)$

- [Covariance]{.alert} $\cov(X,Y) : = \Ex[(X- \mu_X)(Y - \mu_Y)]$

- [Correlation]{.alert} $\displaystyle \corr(X,Y) : = \frac{\cov(X,Y)}{\std(X) \std(Y)}$

- Independence $\implies$ zero correlation, but zero correlation $\notimplies$ independence
<div class="fragment" data-fragment-index="1" style="width:100%; margin:0;" markdown="1">
  - E.g., $X = \cos(2 \pi T)$, $Y = \sin(2 \pi T)$, $T \sim \Unif[0,1]$
  \begin{align*}
  F_X(x) & = \Prob(X \le x) = \Prob[\cos(2 \pi T) \le x] \\
  & = 2\Prob[\cos^{-1}(x)/(2\pi) \le T \le 1/2] \quad \text{since $\cos^{-1}$ is a decreasing function} \\
  & = 2[1/2 - \cos^{-1}(x)/(2\pi)] = 1 - \cos^{-1}(x)/\pi \quad \text{since $T$ is uniform} \\
  &  = F_Y(x) \quad \text{by symmetry}, \quad \varrho_X(x) = F' _X(x) = \frac{1}{\pi\sqrt{1 - x^2}}, \;  \Ex(X) = \Ex(Y) = 0 \\
  \cov(X,Y) & = \Ex(XY) = \Ex[\cos(2 \pi T) \sin( 2 \pi T)] = \int_0^1 \cos(2 \pi t) \sin( 2 \pi t) \, \dif t = \class{alert}{0} \\
  \text{BUT } 
  F_{X,Y}(-0.8,- 0.8) &= 0 \ne [1 - \cos^{-1}(0.8)/\pi ]^2 = F_X(0.8) F_Y(0.8) \quad \text{so $X$ and $Y$ are }\class{alert}{\text{dependent}}
  \end{align*}
</div>

## Important distributions{#prob-distrib}

- [Binomial]{.alert} (free-throw shooting, discrete) $\Bin(n,p)$
$$
\varrho(x) : = \Prob(X=x) = \binom{n}{x} p^x (1-p)^{n-x}, \ x =0, \ldots, n; \quad \Ex(X) = np, \quad \var(X) = np(1-p)
$$

- [Exponential]{.alert} (taxi waiting, continuous) $\Exp(\lambda)$
$$
F(x): = 1 - \exp(-\lambda x),\ \varrho(x) : = \lambda \exp(-\lambda x), \ x \ge 0; \quad \Ex(X) = 1/\lambda, \quad \var(X) = 1/\lambda^2
$$

- [Normal (Gaussian)]{.alert} (limiting distribution, continuous) $\Norm(\mu, \sigma^2)$
$$
\varrho(x): = \frac{\exp\bigl(-(x-\mu)^2/(2\sigma^2)\bigr)}{\sqrt{2 \pi} \sigma}, \ x \in \reals;\quad \Ex(X) = \mu, \quad \var(X) = \sigma^2
$$

- [Multivariate Normal (Gaussian)]{.alert} (limiting distribution, continuous) $\Norm(\vmu, \mSigma)$
$$
\varrho(\vx): = \frac{\exp\bigl(-(\vx-\vmu)^\top \mSigma^{-1} (\vx-\vmu) /2\bigr)}{\sqrt{(2 \pi)^{d} \det(\mSigma)}};\quad \Ex(\vX) = \vmu, \quad \cov(\vX) = \mSigma
$$

## Exponential families of distributions

If a PMF or PDF can be expressed as
$$
\varrho(\vx\mid \vtheta) = h(\vx) \, c(\vtheta) \, \exp \biggl(\sum_k w_k(\vtheta) t_k(\vx) \biggr) \quad \vx \in \reals^d,
$$
then this is called an [exponential family]{.alert}. E.g., 


- [Binomial]{.alert} for _fixed_ $n$ and $0 < p < 1$: $\quad \displaystyle \varrho(x) = \binom{n}{x} p^x (1-p)^{n-x} = \indic(x \in \{0, \ldots, n\}) \binom{n}{x} \, (1-p)^n \, \exp\biggl(\log\biggl(\frac{p}{1-p} \biggr ) x \biggr)$

- [Exponential]{.alert}: $\quad \varrho(x) = \indic(x \ge 0) \, \exp(-\lambda x)$

- [Normal (Gaussian)]{.alert}:
$\quad \displaystyle \varrho(x) = \frac{\exp\bigl(-(x-\mu)^2/(2\sigma^2)\bigr)}{\sqrt{2 \pi} \sigma} = \frac{\exp\bigl(-\mu^2/(2\sigma^2)\bigr)}{\sqrt{2 \pi} \sigma} \, \exp\biggl(-\frac{x^2}{2 \sigma^2} + \frac{ \mu x }{\sigma^2}\biggr)$

- [Multivariate Normal (Gaussian)]{.alert}: 
$$
\varrho(\vx): = \frac{\exp\bigl(-(\vx-\vmu)^\top \mSigma^{-1} (\vx-\vmu) /2\bigr)}{\sqrt{(2 \pi)^{d} \det(\mSigma)}}= 
\frac{\exp(-\vmu^ \top\mSigma^{-1} \vmu/2 )}{\sqrt{(2 \pi)^{d} \det(\mSigma)}}
 \, \exp\biggl(-\frac{\vx^\top \mSigma^{-1} \vx}{2} + \vmu^\top \mSigma^{-1} \vx \biggr)
$$

## A mixed (partially discrete, partially continuous) random variable

Discrete random variables have a PMF.  Continuous random variables have a PDF.  Some random variables are [mixed]{.alert}.

[Zero-inflated exponential distribution]{.alert} â€” $X$ is the waiting time for a taxi

- A taxi waiting for you with probability $p$
- But if there is none, then you have to wait time $1/\lambda$ on average 
\begin{align*}
F(x) &= \begin{cases} 
0, &-\infty < x < 0 \\
1- (1-p)\exp(-\lambda x),  & 0 \le x < \infty
\end{cases}
\\
\mu & = \Ex(X) = \int_{-\infty}^{\infty} x \, \dif F(x) \qquad   
     \mathlink{https://en.wikipedia.org/wiki/Lebesgue-Stieltjes_integration}{(Lebesgueâ€“Stieltjes integral)}\\
& = 0 \times p + \int_0^{\infty} x \times (1-p)\exp(-\lambda x) \, \dif x = (1-p)/ \lambda
\\
\sigma^2 & = \var(X) = \Ex[(X-\mu)^2] = \frac{1-p^2}{\lambda^2}
\end{align*} 

## Lebesgueâ€“Stieltjes Integral = Integral + Jumps

Let 

- $F$ be a nondecreasing, right-continuous function on $\mathbb{R}$
- with (possible) discontinuities at $-\infty <  x_1 < \ldots < x_N < \infty$, 
- $F$ be differentiable between its jump points with $\varrho(x) = F'(x)$, and 
- $f$ be a suitable function

The Lebesgueâ€“Stieltjes integral of $f$ with respect to $F$ can be written as
  \begin{align*}
  \int f(x)\, dF(x) & = \int_{-\infty}^{x_{1}} f(x) \,\varrho(x) \, \dif x + \sum_{k=1}^{N-1} \int_{x_k}^{x_{k+1}} f(x) \, \varrho(x) \, \dif x + \int_{x_N}^{\infty} f(x) \, \varrho(x) \, \dif x \\
  & \qquad \qquad + \sum_{k=1}^N f(x_k)[F(x_k) - F(x_k^{-})]
  \end{align*}

[Integrate $f$ where $F$ is smooth, and add a weighted sum of $f$ at the jump points of $F$]{.alert}

## Conditional probability{#conditional-prob}

The [conditional probability]{.alert} means restricting attention to outcomes where $B$ occurs, then
measure how often $A$ occurs within that restricted universe of $A$ given $B$ (with $\Prob(B)>0$):
$$
\Prob(A \mid B)
=
\frac{\Prob(A \cap B)}{\Prob(B)},
$$
which means restricting attention to outcomes where $B$ occurs, then
measuring how often $A$ occurs within that restricted universe.

- [Multiplication rule]{.alert}:
$\Prob(A \cap B)
=
\Prob(A \mid B),\Prob(B)$

- $A$ and $B$ are [independent]{.alert} if
$\Prob(A \mid B) = \Prob(A)$ or equivalently 
$\Prob(A \cap B)=\Prob(A)\Prob(B)$

### Bayes' Rule
$$
\Prob(A \mid B)
=
\frac{\Prob(B \mid A) \, \Prob(A)}{\Prob(B)},
\qquad \Prob(B)>0
$$

> If you meet an introvert, is she more likely to be a librarian or a salesman?

## Conditional Density
Let $X$ and $Y$ have [joint density]{.alert}
$\varrho_{X,Y}(x,y)$.  The [conditional density]{.alert} of $X$ given $Y=y$ is
$$
\varrho_{X\mid Y}(x \mid y)
=
\frac{\varrho_{X,Y}(x,y)}{\varrho_Y(y)},
\qquad
\varrho_Y(y)=\int \varrho_{X,Y}(x,y) \, \dif x \text{ is the }\class{alert}{\text{marginal density}} \text{ of } Y
$$
	
### Bayesâ€™ rule for densities
$$
\varrho_{X\mid Y}(x\mid y)
=
\frac{\varrho_{Y\mid X}(y\mid x)\,\varrho_X(x)}{\varrho_Y(y)}
$$

## Bayesian statistics to approximate and integrate functions

We assume that $f$, the function to approximate or integrate, is an instance of a Gaussian process, $\GP(0,K)$.  This means that

\begin{gather*}
\vf := \bigl(f(x_1), \ldots, f(x_n) \bigr)^\top \sim \Norm(\vzero, \mK), \quad \text{where }\mK : = \bigl(K(x_i,x_j)\bigr)_{i,j=1}^n \\
\tvf := \bigl(f(x_1), \ldots, f(x_n), f(x) \bigr)^\top \sim \Norm(\vzero, \tmK) \\
\text{where }
\tmK : = \left(\begin{array}{c|c} 
\mK & \vk(x) \\
\hline
\vk^\top(x) & K(x,x)
\end{array}
\right)
, \quad \vk(x) = \bigl( K(x,x_i) \bigr)_{i=1}^n
\end{gather*}
and so the conditional density of $f(x)$ given $\vf = \vy$, where $\vy$ is the observed data, is  
\begin{align*}
\varrho_{f(x) | \vf}(z,\vy) = \frac{\varrho_{\tvf}(\vy,z)}{\varrho_{\vf}(\vy)} = \frac{\frac{\exp(-\tvy^\top \tmK^{-1} \tvy/2)}{\sqrt{(2 \pi)^{n+1} \det(\tmK)}}}{\frac{\exp(-\vy^\top \mK^{-1} \vy/2)}{\sqrt{(2 \pi)^{n} \det(\mK)}}}
= \frac{\exp([-\tvy^\top \tmK^{-1} \tvy + \vy^\top \mK^{-1} \vy]/2)}{\sqrt{(2 \pi) \det(\tmK)/\det(\mK)}}
\end{align*}

## Bayesian statistics to approximate and integrate functions (cont'd)

One can show that 

\begin{gather*}
\tmK^{-1}
=
\left(\begin{array}{c|c}
\mK^{-1}+\mK^{-1}\vk(x)\sigma^{-2}(x)\vk^\top(x)\mK^{-1}
&
-\mK^{-1}\vk(x)\sigma^{-2}(x)
\\
\hline
- \sigma^{-2}(x)\vk^\top(x)\mK^{-1}
&
\sigma^{-2}(x)
\end{array}
\right)\\
\text{where }
\sigma^2(x)=K(x,x)-\vk^\top(x)\mK^{-1}\vk(x), \qquad
\sigma^{-2}(x) := \big(\sigma^2(x)\big)^{-1}
\end{gather*}

So 

\begin{multline*}
-\tvy^\top \tmK^{-1} \tvy + \vy^\top \mK^{-1} \vy \\
= -\vy^\top[\mK^{-1}+\mK^{-1}\vk(x)\sigma^{-2}(x)\vk^\top(x)\mK^{-1}]\vy -2 \vy^\top\mK^{-1}\vk(x)\sigma^{-2}(x) z + z^2/ \sigma^2(x) \\
- \vy^\top \mK^{-1} \vy \\
= \frac{1}{\sigma^2(x)} \left[ z - \vy^\top \mK^{-1} \vk(x) \right]^2
\end{multline*}

and $f(x) \mid \vf = \vy \sim \Norm\bigl(\vy^\top \mK^{-1} \vk(x), \sigma^2(x) \bigr)$

Then $\int_0 ^1 f(x)\, \dif x \mid \vf = \vy \sim \Norm\bigl(\vy^\top \mK^{-1} \int_0^1 \vk(x) \, \dif x, \sigma^2(x) \bigr)$


## Bayesian inference
	
### Bayesâ€™ rule for densities
$$
\varrho_{X\mid Y}(x\mid y)
=
\frac{\varrho_{Y\mid X}(y\mid x)\,\varrho_X(x)}{\varrho_Y(y)}
$$

Let
\begin{align*}
X & = \text{random unknown parameter} \\
Y & = \text{random observed data} \\
\varrho_X & = \text{prior density or belief about the parameter} \\
\varrho_{Y\mid X} & = \text{likelihood} \\
\varrho_{X\mid Y} & = \text{posterior density}
\end{align*}
Then
\begin{align*}
 \varrho_{X\mid Y}( \text{parameter} \mid \text{data})
& =
\frac{\varrho_{Y\mid X}(\text{data}\mid \text{parameter} ) \,\varrho_X(\text{parameter})}{\varrho_Y(\text{data})}
\\
\text{posterior density } &
\propto
\text{likelihood} \times \text{prior density}
\end{align*}

## Types of convergence{#type-conv}

Let $X_1, X_2, \ldots$ be random variables and $X$ another random variable

### Almost Sure Convergence
$$
X_n \xrightarrow{\text{a.s.}} X \iff \Prob \left( \lim_{n \to \infty} X_n = X \right) = 1
$$
For almost every outcome the sequence $X_n$ converges to $X$

### Convergence in Probability
$$
X_n \xrightarrow{\Prob} X \iff \forall \varepsilon > 0, \; 
\Prob\big( |X_n - X| > \varepsilon \big) \;\longrightarrow\; 0.
$$
The probability that $X_n$ differs significantly from $X$ goes to zero

### Convergence in Distribution
$$
X_n \xrightarrow{d} X \iff 
\lim_{n \to \infty} F_{X_n}(x) = F_X(x)
\quad \text{for all continuity points of } F_X
$$
The distributions of $X_n$ approach the distribution of $X$

## Types of convergence (cont'd)
$$
X_n \xrightarrow{\text{a.s.}} X
\;\Longrightarrow\;
X_n \xrightarrow{\Prob} X
\;\Longrightarrow\;
X_n \xrightarrow{d} X,
$$
and none of the reverse implications hold in general

- $X, X_1, X_2, \ldots \IIDsim \Norm(0,1)$ satisfies $X_n \xrightarrow{d} X$ but not $X_n \xrightarrow{\Prob} X$

- $\Prob(X_n =1) = 1/n = 1 - \Prob(X_n=0)$ and $X=0$ satisfies
$$
\Prob(\lvert X_n - X \rvert > \varepsilon  = \Prob(X_n =1) = 1/n \to 0 \text{ as } n \to \infty
$$
  But $\Prob (X_n =1 \text{ infinitely often}) = 1$ since
$$
\sum_{n=1}^{\infty} \Prob(X_n=1) = \sum_{n=1}^{\infty} \frac 1n = \infty
$$

## Central Limit Theorem{#clt}
If $X_1, X_2, \ldots \IIDsim$ some distribution with finite moment generating function, $M(t) := \Ex\bigl(\exp(tX)\bigr)$, and $\mu = \Ex(X)$ and $\sigma^2 = \var(X)$, and
$$
\barX_n := \frac 1n \left(X_1 + \cdots + X_n \right),
$$
then 
$$
\frac{\barX_n - \mu}{\sigma /\sqrt{n}} \xrightarrow{d} \Norm(0,1)
$$

## Chebyscev's Inequality
$$
\Prob\bigl( f(X) \ge r \bigr) \le \frac{\Ex[f(X)]}{r}
$$