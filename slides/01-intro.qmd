---
title: "MATH 563 â€” Mathematical Statistics"
subtitle: |
  <span class="deck-title">Introduction and Probability Review</span><br>
  <span class="deck-meta">Casella &amp; Berger Ch. 1â€“4 (parts) <br> 
  Diagnostic Quiz due 1/16 </span>
author: "Fred J. Hickernell"
date: last-modified
date-format: "MMMM D, YYYY"
reveal-options:
  disableLayout: false
---

# Statistics{data-state="goldborder"}

<div class="h3">Principled</div>
- Clear assumptions
- Probability as a basis
- Efficient and accurate computation

<div class="h3">Inference about a population</div>
- Want to know parameters that describe a large number of individuals or the relationships among them
- Should provide a level of uncertainty

<div class="h3">From Data</div> 
- Carefully sampled
- Observations, surveys, experiments, or computer simulations

&nbsp;

See [Big Statistics Questions](/pages/stats-qs.html)

# Examples

Let's look at 

- [Approval ratings](#approval-ratings)
- [Life expectancy explained by gross domestic (GDP) product per capita](#life-gdp)
- [Numerical integration (quadrature)](#trapezoidal)

## Approval ratings {#approval-ratings}

The Gallup Poll tracks the [approval ratings of US presidents](https://news.gallup.com/poll/203198/presidential-approval-ratings-donald-trump.aspx) according to a [careful polling methodology](https://www.gallup.com/224855/gallup-poll-work.aspx)

- Each day they telephone $n=1500$ adults 
- Their sampling error for the approval ratings is about $4\%$

Let $Y \sim \Bern(\mu)$, a [Bernoulli]{.alert} (free throw shooting) distribution with probability of success $\mu$, 
$$
\Prob(Y =y) = \begin{cases} \mu, & y=1, \text{ (yes, approve)}\\
1-\mu, & y = 0, \text{ (no, do not approve)}
\end{cases}
$$

:::: {.columns}
::: {.column width="70%"}

If $Y_1, \ldots, Y_n \IIDsim \Bern(\mu)$, then
\begin{align*}
T &:= Y_1 + \cdots + Y_n \sim \Bin(n,\mu), \; \Prob(T = t) = \binom{n}{t} \mu^t (1-\mu)^{n-t}\\
\barY &:= \frac 1n (Y_1 + \cdots + Y_n) \\
& \appxsim  \Norm\bigl(\mu,\mu(1-\mu)/n\bigr) \quad \text{by the }\class{alert}{\text{Central Limit Theorem}}
\end{align*}
:::

::: {.column width="27%"}

![](/classlib/classlib/notebooks/figures/approval_rating/ybar_histogram_approval.png){fig-align="center" width="100%"}
:::

::::

## Approval ratings (cont'd)

If $Y_1, \ldots, Y_n \IIDsim \Bern(\mu)$, then we can construct a [confidence interval]{.alert} that [captures]{.alert} the true approval rating with high probability:
\begin{align*}
T &:= Y_1 + \cdots + Y_n \sim \Bin(n,\mu), \quad \Prob(T = y) = \binom{n}{t} \mu^t (1-\mu)^{n-t}\\
\barY &:= \frac 1n (Y_1 + \cdots + Y_n) \appxsim  \Norm\bigl(\mu,\mu(1-\mu)/n\bigr) \quad \text{by the }\class{alert}{\text{Central Limit Theorem}}
\end{align*}

:::: {.columns}

::: {.column width="75%"}

<div class="fragment" data-fragment-index="1" style="width:100%; margin:0;" markdown="1">
\begin{align*}
95\% & \approx \Prob\Bigl[\mu - 1.96\sqrt{\mu(1-\mu)/n} \le \class{alert}{\barY} \le \mu + 1.96\sqrt{\mu(1-\mu)/n}\Bigr] \\
& = \Prob\Bigl[\barY - 1.96\sqrt{\mu(1-\mu)/n} \le \class{alert}{\mu} \le \barY + 1.96\sqrt{\mu(1-\mu)/n}\Bigr] \\
& \approx \Prob\Bigl[\barY - 1.96\sqrt{\barY(1-\barY)/n} \le \class{alert}{\mu} \le \barY + 1.96\sqrt{\barY(1-\barY)/n}\Bigr] \\
& \le \Prob\Bigl[\barY - 1/\sqrt{n} \le \class{alert}{\mu} \le \barY + 1/\sqrt{n} \Bigr] \quad \text{since } \sqrt{\barY(1-\barY)} \le 1/2
\end{align*}
For $n = 1000$ we get $1/\sqrt{n} \approx 3\%$.
<a class="button" style="margin-left:2em;" href="../classlib/classlib/notebooks/approval_rating_bernoulli_binomial_clt_ci.ipynb" download>
â¬‡ Approval Rating Jupyter ðŸ““
</a>
</div>

:::

::: {.column width="23%"}

<span class="fragment" data-fragment-index="1">
<img src="/classlib/classlib/notebooks/figures/approval_rating/ci_failure_approval.png" style="width:100%; display:block; margin:0 auto;">
</span>

:::

:::: 

## Approval ratings (cont'd)

- [Population]{.alert} is US adults
- [Model]{.alert} 
  - Bernoulli distribution
  - Ignores demographic factors that might explain approvals
- [Uncertainty]{.alert} of the estimate of the proportion, $\mu$
  - Provided by a [confidence interval (CI)]{.alert}
  - Facilitated by the [Central Limit Theorem]{.alert}
  - This is the CI capturing $\mu$, _not_ probabiity of $\mu$ in the CI

## Life expectancy vs gross domestic product (GDP) per capita {#life-gdp}

Countries differ dramatically in both [life expectancy]{.alert} and
[economic output]{.alert}. A classic dataset (World Bank / Gapminder) records, for each country,

- Life $=$ Life expectancy at birth (years)
- GDP $=$ Gross domestic product per capita (USD, purchasing-power adjusted)

Each point represents **one country**, not one individual.  We are interested in the conditional behavior of life expectancy with given national income:

:::: {.columns}
::: {.column width="50%"}

$$
\Ex(\text{Life} | \text{GDP} = x)
$$
:::

::: {.column width="47%"}

![](/classlib/classlib/notebooks/figures/health_wealth_gapminder/health_wealth_linear_scale.png){fig-align="center" width="100%"}
:::

::::


## Life expectancy vs GDP (cont'd)

The scatterplot of life expectancy versus GDP per capita reveals two important statistical features.

### Nonlinearity
Increases in GDP have a [much larger effect]{.alert} on life expectancy at low
income levels than at high income levels

### Heteroscedasticity
Countries with low GDP exhibit [much greater variability]{.alert} in life
expectancy

:::: {.columns}
::: {.column width="65%"}

Take logarithm to treat this
$$
\Ex(\text{Life}  \mid \text{GDP} = x) \approx \alpha + \beta \log(x)
$$

- $\beta$ measures years of life gained per [multiplicative]{.alert} increase in GDP
- The log scale naturally reflects [diminishing returns]{.alert}

<a class="button" style="margin-left:2em;" href="../classlib/classlib/notebooks/health_wealth_gapminder_regression.ipynb" download>
â¬‡ Health vs. Wealth ðŸ““
</a>
:::

::: {.column width="35%"}

![](/classlib/classlib/notebooks/figures/health_wealth_gapminder/health_wealth_semilogx_scale.png){fig-align="center" width="100%"}


:::

::::


## Life expectancy vs GDP (cont'd)

- [Population]{.alert} is all hypothetical countries
- [Model]{.alert} 
  - Linear regression 
  - Incorporates transformation of the explanatory variable
- [Uncertainty]{.alert}
  - $R^2$ measures the proporation of variance in $Y$ captured by the model
- [Intepretation]{.alert}
  - This describes relationship, not cause and effect
  - The variation in the residuals point to other explanatory factors that have been left out of the model

## Numerical integration {#trapezoidal}

Computational mathematics seems distinct from statistics, but there is an overlap.

Suppose that $f$ is a random function defined in $[0,1]$, in particular, a [Gaussian process]{.alert}, i.e., $f \sim \GP(0,K)$.  This means that for any distinct $x_1, \ldots, x_n \in [0,1]$, the random vector of function values, $\vf := \bigl(f(x_1), \ldots, f(x_n) \bigr)^\top$, has a [multivariate Gaussian distribution]{.alert} with 

- [Mean]{.alert} $\vmu = \Ex(\vf) = \vzero$
- [Covariance matrix]{.alert} $\mSigma := \Ex(\vf \vf^\top) = \bigl(\Ex[f(x_i)f(x_j)]\bigr)_{i,j=1}^n  = \bigl(K(x_i,x_j)\bigr)_{i,j=1}^n =: \mK$

<div class="fragment" data-fragment-index="1" style="width:100%; margin:0;" markdown="1">
If this is the Bayesian prior belief about $f$, then the [Bayesian posterior]{.alert} mean of the function conditioned on the data $\vf = \vy$ is
$$
\Ex\bigl[f(x) \mid \vf  = \vy\bigr] = \vy^\top \mK^{-1} \vk(x) , \quad 
\text{where } \vk(x) = \bigl( K(x,x_i) \bigr)_{i=1}^n
$$

The Bayesian posterior mean integral of $f$ conditioned on the data is

$$
\Ex\biggl[\int_0^1 f(x) \, \dif x \mid \vf  = \vy\biggr] = \vy^\top \mK^{-1} \int_0^1 \vk(x) \, \dif x
$$

For $K(t,x) = 2 - \lvert t - x\rvert$ this is the [trapezoidal rule]{.alert}.  <a class="button" style="margin-left:2em;" href="../classlib/classlib/notebooks/Bayesian_quadrature.ipynb" download>
â¬‡ Bayesian Quadrature ðŸ““
</a>
</div>

## Numerical integration (cont'd)

- [Population]{.alert} all (continuous) functions
- [Model]{.alert} 
  - Stochastic processes
  - Bayesian inference
  - Often one would the parameters in $K$ from data
- [Uncertainty]{.alert}
  - You may construct Bayesian credible intervals, but they depend on yur confidence in choosing a reasonable $K$
- [Intepretation]{.alert}
  - Provides a statistical interpretation of computational mathematics


# Probability Review

- [Key concepts](#prob-concepts)
- [Important distributions](#prob-distrib)
- [Conditional probability](#conditional-prob)
- [Types of convergence](#type-conv)
- [Central Limit Theorem](#clt)

## Key probability concepts{#prob-concepts}

- [Outcome]{.alert} â€” single possible result of a random process, e.g., yes, $0.5$, $(1,-2)$

- [Sample space]{.alert} â€“ set of all possible outcomes, e.g., $\{\text{yes}, \text{no} \}$, $\reals$

- [Event]{.alert} â€“ set of outcomes (subset of the sample space), e.g., $[0,\infty)$

- [Event space]{.alert} â€“ collection of all possible events 

- [Probability]{.alert} â€“ function  $\Prob:\text{event space}\to[0,1]$ satisfying
  - $0 \le \Prob(\text{event}) \le 1$
  - $\mathbb{P}(\text{sample space})=1$
  - If $A \cap B=\varnothing$, then $\Prob(A\cup B)=\Prob(A)+\Prob(B)$


## Key probability concepts (cont'd)

- [Random variable/vector/function]{.alert} â€” mathematical quantity representing an outcome

- [Cumulative distribution function (CDF)]{.alert} $F$ of random variable $X$ 
  -  $F(x) : =  \Prob(X \le x)$

- [Probability mass function (PMF)]{.alert} $\varrho$ of discrete random variable $X$ 
  - $\varrho(x):= \Prob(X = x)$

- [Probability density function (PMF)]{.alert} $\varrho$ of continuous random variable $X$ 
  - $\varrho(x):= F'(x)$

- [Expectation]{.alert} â€” average value of a random variable, weighted by its probability
  - $\displaystyle \Ex[f(X)] : = \sum_{x} f(x) \varrho(x)$  for discrete random variablees
  - $\displaystyle \Ex[f(X)] : =  \int f(x) \varrho(x) \; \dif x$  for continuous random variablees

- [Mean]{.alert} of $X$ is $\mu : = \Ex(X)$

- [Variance]{.alert} of $X$ is $\sigma^2 : = \var(X) := \Ex[(X - \mu)^2] = E(X^2) - \mu^2$
  - [Standard deviation]{.alert} of $X$ is  $\std(X) := \sigma = \sqrt{\var(X)}$

## Key probability concepts (cont'd)

- $(X,Y)$ are [independent]{.alert} iff $F_{X,Y}(x,y) = F_X(x) F_Y(y)$ or equivalently $\varrho_{X,Y}(x,y) = \varrho_X(x) \varrho_Y(y)$

- [Covariance]{.alert} $\cov(X,Y) : = \Ex[(X- \mu_X)(Y - \mu_Y)]$

- [Correlation]{.alert} $\displaystyle \corr(X,Y) : = \frac{\cov(X,Y)}{\std(X) \std(Y)}$

- Independence _implies_ zero correlation, but zero correlation _does not imply_ independence
<div class="fragment" data-fragment-index="1" style="width:100%; margin:0;" markdown="1">
  - E.g., $X = \cos(2 \pi T)$, $Y = \sin(2 \pi T)$, $T \sim \Unif[0,1]$
  \begin{align*}
  F_X(x) & = \Prob(X \le x) = \Prob[\cos(2 \pi T) \le x] \\
  & = 2\Prob[\cos^{-1}(x)/(2\pi) \le T \le 1/2] \quad \text{since $\cos^{-1}$ is a decreasing function} \\
  & = 2[1/2 - \cos^{-1}(x)/(2\pi)]\quad \text{since $T$ is uniform}\\
  &  = 1 - \cos^{-1}(x)/\pi = F_Y(x) \quad \text{by symmetry} \\
  \varrho_X(x) & = F' _X(x) = \frac{1}{\pi\sqrt{1 - x^2}} \\
  \text{BUT } 
  F_{X,Y}(-0.8,- 0.8) &= 0 \ne [1 - \cos^{-1}(0.8)/\pi ]^2 = F_X(0.8) F_Y(0.8)
  \end{align*}
</div>

## Important distributions{#prob-distrib}

- [Binomial]{.alert} (free-throw shooting, discrete) $\Bin(n,p)$
$$
\varrho(x) : = \Prob(X=x) = \binom{n}{x} p^x (1-p)^{n-x}, \ x =0, \ldots, n; \quad \Ex(X) = np, \quad \var(X) = np(1-p)
$$

- [Exponential]{.alert} (taxi waiting, continuous) $\Exp(\lambda)$
$$
F(x): = 1 - \exp(-\lambda x),\ \varrho(x) : = \exp(-\lambda x), \ x \ge 0; \quad \Ex(X) = 1/\lambda, \quad \var(X) = 1/\lambda^2
$$

- [Normal (Gaussian)]{.alert} (limiting distribution, continuous) $\Norm(\mu, \sigma^2)$
$$
\varrho(x): = \frac{\exp\bigl(-(x-\mu)^2/(2\sigma^2)\bigr)}{\sqrt{2 \pi} \sigma}, \ x \in \reals;\quad \Ex(X) = \mu, \quad \var(X) = \sigma^2
$$

- [Multivariate Normal (Gaussian)]{.alert} (limiting distribution, continuous) $\Norm(\vmu, \mSigma)$
$$
\varrho(\vx): = \frac{\exp\bigl(-(\vx-\vmu)^\top \mSigma^{-1} (\vx-\vmu) /2\bigr)}{\sqrt{(2 \pi)^{d} \det(\mSigma)}};\quad \Ex(\vX) = \vmu, \quad \cov(\vX) = \mSigma
$$

## Exponential families of distributions

If a PMF or PDF can be expressed as
$$
\varrho(\vx\mid \vtheta) = h(\vx) c(\vtheta) \exp \biggl(\sum_k w_k(\vtheta) t_k(\vx) \biggr) \quad \vx \in \reals^d,
$$
then this is called an [exponential family]{.alert}. E.g., 


- [Binomial]{.alert} for _fixed_ $n$ and $0 < p < 1$: $\quad \displaystyle \varrho(x) = \binom{n}{x} p^x (1-p)^{n-x} = \indic(x \in \{0, \ldots, n\}) \binom{n}{x} \exp\biggl(\log\biggl(\frac{p}{1-p} \biggr ) x \biggr)$

- [Exponential]{.alert}: $\quad \varrho(x) = \indic(x \ge 0) \exp(-\lambda x)$

- [Normal (Gaussian)]{.alert}:
$\quad \displaystyle \varrho(x) = \frac{\exp\bigl(-(x-\mu)^2/(2\sigma^2)\bigr)}{\sqrt{2 \pi} \sigma} = \frac{\exp\bigl(-\mu^2/(2\sigma^2)\bigr)}{\sqrt{2 \pi} \sigma} \, \exp\biggl(-\frac{x^2}{2 \sigma^2} + \frac{ \mu x }{\sigma^2}\biggr)$

- [Multivariate Normal (Gaussian)]{.alert}: 
$$
\varrho(\vx): = \frac{\exp\bigl(-(\vx-\vmu)^\top \mSigma^{-1} (\vx-\vmu) /2\bigr)}{\sqrt{(2 \pi)^{d} \det(\mSigma)}}= 
\frac{\exp(-\vmu^ \top\mSigma^{-1} \vmu/2 )}{\sqrt{(2 \pi)^{d} \det(\mSigma)}}
 \, \exp\biggl(-\frac{\vx^\top \mSigma^{-1} \vx}{2} + \vmu^\top \mSigma^{-1} \vx \biggr)
$$

## A mixed (partially discrete, partially continuous) random variable

Discrete random variables have a PMF.  Continuous random variables have a PDF.  Some random variables are [mixed]{.alert}.

[Zero-inflated exponential distribution]{.alert} â€” $X$ is the waiting time for a taxi

- A taxi waiting for you with probability $p$
- But if there is none, then you have to wait time $1/\lambda$ on average 
\begin{align*}
F(x) &= \begin{cases} 
0, &-\infty < x < 0 \\
1- (1-p)\exp(-\lambda x),  & 0 \le x < \infty
\end{cases}
\\
\mu & = \Ex(X) = \int_{-\infty}^{\infty} x \, \dif F(x) \qquad   
     \mathlink{https://en.wikipedia.org/wiki/Lebesgue-Stieltjes_integration}{(Lebesgueâ€“Stieltjes integral)}\\
& = 0 \times p + \int_0^{\infty} x \times (1-p)\exp(-\lambda x) \, \dif x = (1-p)/ \lambda
\\
\sigma^2 & = \var(X) = \Ex[(X-\mu)^2] = \frac{1-p^2}{\lambda^2}
\end{align*} 

## Conditional probability{#conditional-prob}

## Types of convergence{#type-conv}

## Central Limit Theorem{#clt}