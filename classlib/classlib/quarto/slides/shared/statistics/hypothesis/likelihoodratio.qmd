# Optimal Tests and Likelihood Methods

- [Likelihood ratio tests (LRT)](#likelihoodratiotests)

- [Neyman–Pearson lemma](#neymanpearson)

- [Large-sample behavior of the likelihood ratio statistic](#largesamplelikelihoodratio)


## From procedures to optimality {#likelihoodratiotests}

We have constructed tests, and we have computed their size and power, but 

> Among all tests with size $\alpha$, which has the highest power (at a fixed alternative)?

To compare two hypotheses

$$
H_0: \theta = \theta_0 \text{ vs } H_A: \theta = \theta_1
$$

Consider the [likelihood](02-estimator.html#maximum-likelihood-estimators) ratio statistic

$$
\Lambda(\vX)
=
\frac{L(\theta_0 \mid \vX)}
     {L(\theta_1 \mid \vX)}
$$

Small $\Lambda \implies$ evidence against $H_0$

Or for more general hypotheses, e.g., $H_0: \theta \in \Theta_0$ vs $H_A: \theta \in \Theta_A$:   

$$
\Lambda(\vX)
=
\frac{\sup_{\theta\in H_0} L(\theta \mid \vX)}
     {\sup_{\theta\in H_A} L(\theta \mid \vX)}
$$

## Neyman–Pearson lemma (simple vs simple) {.neymanpearson}

Test

$$
H_0:\theta=\theta_0
\qquad
H_A:\theta=\theta_1
$$

Among all tests with size $\alpha$,

> The likelihood ratio test is most powerful

Reject $H_0$ when

$$
\frac{L(\theta_1 \mid \vX)}
     {L(\theta_0 \mid \vX)}
> k
$$

where $k$ is chosen to make the test have size $\alpha$

---

### Example:  mean of Normal data with known $\sigma$

Test

$$
H_0:\mu=\mu_0
\qquad
H_A:\mu=\mu_1 \quad \text{with } \mu_1 > \mu_0
$$

Assume $X_1,\dots,X_n \sim \Norm(\mu,\sigma^2)$.  We know that $\barX_n$ is the MLE of $\mu$, so the [likelihood ratio]{.alert} test statistic is

$$
\Lambda
=
\frac{L(\mu_0)}{L(\barX_n)}
=
\exp\!\left(
-\frac{n}{2\sigma^2}
(\barX_n - \mu_0)^2
\right)
$$


[Reject]{.alert} when $\Lambda$ is small $\iff (\barX_n - \mu_0)^2 \iff \barX_n$  is large, because alternative is one-sided

Equivalently,

$$
\frac{\barX_n - \mu_0}{\sigma/\sqrt n}
>
z_\alpha, \qquad \text{which gives the cut-off in terms of $n$, $\sigma$, and $\alpha$}
$$

The LRT reproduces the classical $z$-test for the mean of a normal distribution with known variance

$\exstar$ Show that the LRT for testing $H_0: \mu = \mu_0$ vs $H_A: \mu  = \mu_1$ with $\mu_1 > \mu_0$ and $\sigma$ unknown is the same as the $t$-test


## From Neyman–Pearson to likelihood ratio tests

For composite hypotheses,

$$
\Lambda(\vX)
= 
\frac{\displaystyle \sup_{\theta\in \text{null model}} L(\theta \mid \vX)}
     {\displaystyle \sup_{\theta\in \text{full model}} L(\theta \mid \vX)}
$$

Reject for small $\Lambda$

## Wilks' Theorem

Under regularity conditions,

$$
-2 \log \Lambda
\dto
\chi^2_{\df}
$$
where the degrees of freedom is
$$
\df
=
\dim(\text{full model}) - \dim(\text{null model}))
$$

### Example: Normal data with unknown $\sigma$

- Full model for $\Norm(\mu,\sigma^2)$ has $\dim(\Theta) = 2$
- Null model for $H_0: \mu = \mu_0$ has $\dim(\Theta_0) = 1$
- $\df = 2 - 1 = 1$



## Why bother with the likelihood ratio test (LRT)

$$
\Lambda(x)
=
\frac{\sup_{\theta \in \Theta_0} L(\theta)}
{\sup_{\theta \in \Theta} L(\theta)} \qquad \text{Reject when $\Lambda$ is small}
$$


Even though 

- We already know the $z$, $t$, and other tests, the LRT

- We still need to choose the rejection region cut-off to get the desired size $\alpha$,

However, the LRT

- Is a unifying concept
- Handles composite hypotheses\
- Reproduces $z$, $t$, $\chi^2$, $F$ tests\
- Scales to multi-parameter models\
- Has large-sample theory (Wilks)

&nbsp;
