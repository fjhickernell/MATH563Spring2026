# Hypothesis Testing Framework

- [Null hypothesis]{.alert} $H_0$ — the default assumption, [innocence]{.alert}, often "no effect" or "status quo"
- [Alternative hypothesis]{.alert} $H_A$ — [guilty]{.alert}, the claim requiring evidence

- [Test statistic]{.alert} $T(\vX)$ — a function (summary) of the data, $\vX = (X_1, \ldots, X_n)$, used to assess $H_0$
- [Rejection region]{.alert} $\RR$ — values of $T(\vX)$ for which we reject $H_0$

- [Type I error]{.alert} $\alpha$ — [reasonable doubt]{.alert}, probability of falsely rejecting $H_0$
- [$p$-value]{.alert} — (under $H_0$) probability of observing a result at least as extreme as the one observed

- [Type II error]{.alert} $\beta$ — probability of failing to reject $H_0$ when $H_A$ is true
- [Power]{.alert} $1-\beta$ — probability of correctly rejecting $H_0$

::: {.key-point}
Hypothesis testing is a formal way of deciding whether the data provide enough evidence to overturn a protected default assumption
:::

We will use ideas from estimation, confidence intervals, and pivots to construct tests

---

```{python}
#| echo: false
#| output: false

import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as st

from classlib.plots import (
    annotate_xaxis_marks,
    shade_under_curve,
    curve_color,
    reject_color,
)

plt.rcParams["mathtext.fontset"] = "cm"

# -----------------------
# PARAMETERS (edit here)
# -----------------------
mu0 = 200
n = 10
alpha = 0.05
xbar = 176
# -----------------------

df = 2 * n
t_obs = (2 * n * xbar) / mu0
tcrit = st.chi2.ppf(alpha, df)
ycrit = st.chi2.pdf(tcrit, df)
xbar_crit = (mu0 / (2 * n)) * tcrit
pval = st.chi2.cdf(t_obs, df)
```

## Example: Server Reliability Claim (Mean Time Between Failures)

Suppose 

$$
X_1,\dots,X_n \IIDsim \Exp(1/\mu),
\qquad \Ex(X)=\mu
$$

Here 

- $X_i$ is the *time between unexpected server failures* (e.g., crashes/outages) for a particular service

```{python}
#| echo: false
#| output: asis
print(rf"- The provider claims the *mean time between failures* is $\mu_0 = {mu0:g}$ hours") 
```
- We want evidence that reliability has *worsened*, i.e. failures are happening more frequently

::: {.fragment}

### Step 1: Hypotheses

```{python}
#| echo: false
#| output: asis
print(rf"- [Null hypothesis]{{.alert}} (innocence, status quo): $\qquad H_0:\ \mu={mu0:g}$")
print(rf"- [Alternative hypothesis]{{.alert}} (guilty, change): $\qquad H_A:\ \mu<{mu0:g}$")  
```

::: 


---


### Step 2: Construct test statistic

```{python}
#| echo: false
#| output: asis
print(rf"$$\text{{Use the pivot }} \frac{{2n\barX}}{{\mu}} \sim \chi^2_{{2n}}, \qquad \text{{so that under }} H_0: \mu = {mu0:g}, \quad T(\vX) = \frac{{2n\barX}}{{{mu0:g}}} \sim \chi^2_{{2n}}$$")
```

:::{.fragment}
### Step 3: Choose significance level and construct rejection region

```{python}
#| echo: false
#| output: asis
print(rf"Choose $\alpha = {alpha}$ as the maximum tolerable risk of falsely accusing underperformance")
```

::: {.columns}

::: {.column width="40%"}

Left-tailed test:

$$
\RR
=
\left\{
T < \chi^2_{2n,1-\alpha}
\right\}
$$

Equivalently, reject when

```{python}
#| echo: false
#| output: asis
print(rf"$$\barX < \frac{{{mu0}}}{{2n}}\,\chi^2_{{2n,1-\alpha}}$$")
```

&nbsp;

::: {.key-point}
No data has been observed yet
:::


::: 


::: {.column width="60%"}

```{python}
#| echo: false
#| fig-width: 6.0
#| fig-height: 2.3

import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as st
from classlib.plots import (
    annotate_xaxis_marks,
    shade_under_curve,
    curve_color,
    reject_color,
)
plt.rcParams["mathtext.fontset"] = "cm"

x = np.linspace(0, st.chi2.ppf(0.999, df), 800)
pdf = st.chi2.pdf(x, df)

fig, ax = plt.subplots()
curve, = ax.plot(
    x,
    pdf,
    linewidth=3.0,
    color=curve_color,
)

# Shade rejection region: T < tcrit
shade = shade_under_curve(
    ax,
    x,
    pdf,
    where=(x <= tcrit),
    alpha=0.3,
    color=reject_color,
)

# Critical value 
ax.vlines(tcrit, 0, ycrit, linewidth=3.0)

ax.set_title(rf"Left-tailed test under $H_0$: $n ={n}$, $T \sim \chi^2_{{{df}}}$", fontsize=18)
ax.set_xlabel(r"$t$", fontsize=24)
ax.set_ylabel(r"$\varrho_{T}(t)$", fontsize=24)

ax.set_ylim(0, max(pdf)*1.15)

annotate_xaxis_marks(
    ax,
    [tcrit],
    [rf"$\chi^2_{{{df},{1-alpha}}} = {tcrit:.2f}$"],
    fontsize=24,
)


ax.legend(
    [curve, shade],
    [rf"$\chi^2_{{{df}}}$ density under $H_0$",
     rf"rejection region ($\alpha={alpha}$)"],
    loc="upper right",
    frameon=False,
    fontsize=16,
)
plt.tight_layout()
plt.show()
```


::: 

::: 

:::

---

### Step 4: Observed Data

Let

```{python}
#| echo: false
#| output: asis
print(rf"$$n={n},\qquad \barx_n={xbar}$$")
print("")
print("Then")
print(rf"$$t_{{\text{{obs}}}}=\frac{{2({n})({xbar})}}{{{mu0}}}={t_obs:.2f}$$")
```

::: {.fragment}

### Step 5: $p$-value and decision

The [$p$-value]{.alert} is

```{python}
#| echo: false
#| output: asis
pval = st.chi2.cdf(t_obs, df)
print(rf"$$p=\Prob\big(\chi^2_{{{df}}} \le {t_obs:.2f}\big)  \approx {pval:.2f}$$")
print("")
print(rf"Reject $H_0$ iff $p\le \alpha$")  
print(r"<div style='height:40px;'></div>")
if pval <= alpha:
  print(rf"So we REJECT $H_0$ at the $\alpha={alpha}$ level")
else:
  print(rf"So we do NOT reject $H_0$ at the $\alpha={alpha}$ level")
```
:::

---


### Type I and Type II Errors

- [Type I error]{.alert} $\alpha$:  

```{python}
#| echo: false
#| output: asis
print(rf"  Falsely conclude that reliability has worsened when the true mean time between failures is {mu0} hours")
print(rf"$$\alpha=\Prob(\text{{reject }}H_0\mid \mu={mu0})$$")
print(rf"- [Type II error]{{.alert}} $\beta(\mu_1)$ (for some $\mu_1<{mu0}$):")
```

  Fail to detect that the system is less reliable than claimed
  $$
  \beta(\mu_1)
  =
  \Prob(\text{fail to reject }H_0 \mid \mu = \mu_1)
  $$

- [Power function]{.alert}  $\power(\mu_1)$\
```{python}
#| echo: false
#| output: asis
print(rf"  The probability of correctly detecting reduced reliability (for some $\mu_1<{mu0}$)")
```
  $$
  \power(\mu_1)
  =
  \Prob(\text{reject }H_0 | \mu = \mu_1)
  =
  1-\beta(\mu_1)
  $$

---

### Power for the Server Reliability test

From Step 3, we reject $H_0$ when

```{python}
#| echo: false
#| output: asis
print(rf"$$T=\frac{{2n\barX}}{{{mu0}}} < \chi^2_{{2n,1-\alpha}}$$")
print("")
print(rf"If the true mean is actually $\mu_1$, then")
print(rf"$$\frac{{{mu0} T }}{{\mu_1}} = \frac{{2n\barX}}{{\mu_1}} \sim\chi^2_{{2n}}$$")
print("")
print(rf"Therefore the [power function]{{.alert}} is")
print(r"\begin{align*}")
print(r"\power(\mu_1)")
print(r"&= \Prob\!\left(T < \chi^2_{2n,\,1-\alpha}\mid \mu=\mu_1\right)")
print(rf"= \Prob\!\left(\frac{{{mu0}\,T}}{{\mu_1}} \sim \chi^2_{{2n}} < \frac{{{mu0}\,\chi^2_{{2n,\,1-\alpha}}}}{{\mu_1}} \,\middle|\, \mu=\mu_1\right)\\")
print(rf"&\qquad \qquad \text{{increases as $\mu_1$ decreases.}}")
print(r"\end{align*}")
```

---

### Power Curve

```{python}
#| echo: false
#| fig-width: 6.2
#| fig-height: 3.2

from classlib.plots import annotate_xaxis_marks, set_tol_bright_cycle

plt.rcParams["mathtext.fontset"] = "cm"

# --- larger sample size ---
nlarge = 200
dflarge = 2 * nlarge
tcrit_large = st.chi2.ppf(alpha, dflarge)  # upper-quantile convention

# --- grid ---
mu = np.linspace(50, 300, 400)

# --- power curves ---
power_base = st.chi2.cdf((mu0 / mu) * tcrit, df)
power_large = st.chi2.cdf((mu0 / mu) * tcrit_large, dflarge)

# --- plot ---
fig, ax = plt.subplots()

set_tol_bright_cycle(ax)

ax.plot(mu, power_base, linewidth=3.0, label=rf"$n={n}$")
ax.plot(mu, power_large, linewidth=3.0, label=rf"$n={nlarge}$")

# reference lines
ax.vlines(mu0, 0, 1, linestyles="--", linewidth=2.0)
ax.hlines(alpha, mu.min(), mu.max(), linestyles="--", linewidth=2.0)

ax.set_xlabel(r"True mean $\mu_1$", fontsize=20)
ax.set_ylabel(r"$\mathrm{power}(\mu_1)$", fontsize=24)
ax.set_ylim(-0.02, 1.02)
ax.set_title(rf"$\alpha={alpha}$", fontsize=24)

ax.legend(frameon=False, fontsize=18, loc="upper right")

plt.tight_layout()
plt.show()
```

::: {.key-point}
**Do not** use the power curve to decide whether to reject after seeing the data

Power is about the test procedure, not about the observed data
:::

---

## Two kinds of error
\begin{gather*}
H_0 \text{ null hypothesis (default, innocence) in terms of } \theta 
\\
H_A \text{ alternative hypothesis (evidence, guilt) in terms of } \theta
\end{gather*}

$H_0$ and $H_A$ are mutually exclusive

### Truth vs Decision

| Truth about $H_0$ | Reject $H_0$ | Do Not Reject $H_0$ |
|-----------------|-------------------|-------------------|
| $H_0$ true | **Type I error** <br> probability $\alpha$| Correct |
| $H_0$ false | Correct <br> probability $\power(\theta)$| **Type II error** <br> probability $\beta(\theta)$ |


## Confidence intervals & hypothesis testing

### Structural comparison

| Confidence Interval | Hypothesis Test |
|--------------------|-----------------|
| An *interval* of plausible values | A *reject / do not reject decision* |
| Random interval | Random decision |
| Statement about $\theta$ | Statement about $H_0$ |
| No special value privileged | Tests a specific value $\theta_0$ |

### Shared ingredients

| Both Use | Meaning |
|----------|---------|
| Level $\alpha$ | Willingness to be wrong |
| Pivot / test statistic | Standardized measure of evidence |
| Sampling distribution | Controls coverage / Type I error |

$$
\theta_0 \text{ rejected at level } \alpha
\iff
\theta_0 \notin \text{corresponding } (1-\alpha) \text{ confidence interval}
$$

## Hypothesis Testing: Design vs Data

```{mermaid}
%%{init: {"themeVariables":{"fontSize":"36px"}}}%%
flowchart LR
  D1T["Design Phase<br/>(Before Data)"]
  D2T["Data Phase<br/>(After Data)"]

  subgraph D1 [ ]
    A[Model<br/>Assumptions]
    B[Choose Test<br/>Statistic]
    C[Sampling<br/>Distribution<br/>under null]
    D["Fix Rejection<br/>Region<br/>(size α)"]
  end

  subgraph D2 [ ]
    E[Observe<br/>Data]
    F[Compute<br/>Test<br/>Statistic]
    G[Decision:<br/>Reject or Not]
  end

  D1T --> A
  D2T --> E
  A --> B --> C --> D --> E --> F --> G

  style D1 fill:transparent,stroke:#888,stroke-width:2px
  style D2 fill:transparent,stroke:#888,stroke-width:2px

  style D1T fill:transparent,stroke:transparent,font-weight:bold
  style D2T fill:transparent,stroke:transparent,font-weight:bold
```

### Emphasis

- Everything in the **Design Phase** defines the procedure  
- $\alpha$ and power belong to the Design Phase  
- The Data Phase only applies the pre-defined rule  

::: {.key-point}
The power curve describes the procedure, not the observed dataset
::: 


## What depends on data and what does not?

:::{.columns}

::: {.column width="50%"}

### Chosen before seeing data

| Quantity | Who chooses it? |
|----------------|----------------|
| $H_0$ and $H_A$ | Us |
| Form of test statistic $T$ | Us |
| Significance level $\alpha$ | Us |
| Sample size $n$ | Us |
| Rejection region $\RR$ | Determined by $\alpha$ & $n$<br> (once $H_0$, $T$ are fixed) |
| Type II error $\beta(\theta)$ <br>$\power(\theta)$ | Determined by $\alpha$, $n$, & $\theta$ |

Once $H_0$ and $T$ are fixed:

- One may choose two of  $\alpha$, $n$, and $\RR$,  
- But not all three independently

:::

::: {.column width="5%"}

:::

::: {.column width="45%"}

### Depends on data

| Quantity      | Why? |
|:-----------------------------------|:------|
| Test statistic value | Computed from sample |
| p-value | Function of statistic |
| Decision (reject / not reject) | Depends on statistic |

:::

:::

## Forms of $H_0$ and $H_A$

:::{.columns}

:::{.column width="45%"}

### Common structures

| Type | $H_0$ | $H_A$ |
|------|--------|--------|
| Two-sided | $\theta = \theta_0$ | $\theta \ne \theta_0$ |
| One-sided (upper) | $\theta = \theta_0$ | $\theta > \theta_0$ |
| One-sided (lower) | $\theta = \theta_0$ | $\theta < \theta_0$ |
| Two simple values | $\theta = \theta_0$ | $\theta = \theta_1$ |

- [Simple]{.alert}: a single $\theta$ value

- [Composite]{.alert}: multiple $\theta$ values

:::

:::{.column width="55%"}

### Composite null example

| Version A | Version B |   |
|------------|------------|----------|
| $H_0:\ \theta = \theta_0$ | $H_0:\ \theta \ge \theta_0$ | Same?
| $H_A:\ \theta < \theta_0$ | $H_A:\ \theta < \theta_0$ | 

The test is calibrated at the boundary:

\begin{align*}
\text{A}: \quad &\Prob(\text{Reject } H_0 \mid \theta_0)=\alpha \\
\text{B}: \quad &\sup_{\theta \ge \theta_0}
\Prob(\text{Reject } H_0 \mid \theta)
= \alpha
\end{align*}

### Key idea

- $H_0$ may be composite  
- The boundary value controls Type I error  
- Test the _least favorable_ value

:::

:::
