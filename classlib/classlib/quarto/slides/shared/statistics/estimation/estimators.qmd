# Estimators/Estimates

[Estimator:]{.alert} a random variable (or function of the sample) used to approximate an unknown parameter
[Estimate:]{.alert} the realized numerical value of an estimator after observing the data

- [Summary statistics](#summary-statistics)
- [Maximum likelihood estimators (MLE)](#maximum-likelihood-estimators)
- [Plug-in estimators](#plug-in-estimators)



## Summary statistics{#summary-statistics}

[({{< meta texts.cb.initials >}} §5.3, §5.4, §6.1; {{< meta texts.wms.initials >}} §6.7, §8.1, §9.6)]{.small}

Given IID data, $X_1, \ldots, X_n$, we often compute

- [Empirical Distribution]{.alert} $F_{\{X_i\}}(x) := \frac 1n \sum_{i=1}^n \indic(X_i \le x)$

- [Sample Mean]{.alert} $\displaystyle \barX = \barX_n := \frac{1}{n} \sum_{i=1}^n X_i = \int x \, \dif F_{\{X_i\}}(x) = \Ex_{F_{\{X_i\}}}(X)$ to approximate the population mean $\mu := \Ex[X_1]$

- [Sample Variance]{.alert} $S^2 =S^2_n := \displaystyle \frac{1}{n-1} \sum_{i=1}^n (X_i - \barX_n)^2$ to approximate the population variance $\sigma^2 := \var(X_1) := \Ex[(X_1-\mu)^2]$
  - Sometimes $\hsigma^2 = \hsigma^2_n := \displaystyle \frac{1}{\class{alert}{n}} \sum_{i=1}^n (X_i - \barX_n)^2$ 

---

- [Order Statistics]{.alert} $X_{(1)}, X_{(2)}, \ldots$, reorder the data so that 
$$ X_{(1)} \le X_{(2)} \le \cdots \le X_{(n)}, \qquad \text{i.e., } X_{(i)} = Q_{\{X_i\}}(i/n)
$$
where $Q_{\{X_i\}}$ is the [quantile function](../slides/01-intro.html#quantile-function) corresponding to the empirical distribution
  - $X_{(1)}$ is the minimum and $X_{(n)}$ is the maximum of the data
  - $X_{(i)}$ is often an estimator of the population quantile $Q_X(p)$ for $p \approx i/(n+1)$ or $(i-1/2)/n$



Given IID data, $(X_1, Y_1), \ldots, (X_n,Y_n)$, with sample mean $(\barX_n, \barY_n)$, we often compute

- [Sample Covariance]{.alert} $\displaystyle  S_{XY} := \frac{1}{n-1} \sum_{i=1}^n (X_i - \barX_n)(Y_i - \barY_n)$ to approximate the population covariance $\cov(X_1,Y_1) := \Ex[(X_1 - \mu_X)(Y_1 - \mu_Y)]$

- [Sample Correlation]{.alert} $\displaystyle  R_{XY} := \frac{S_{XY}}{\sqrt{S^2_X S^2_Y}}$ to approximate the population correlation $\displaystyle \corr(X_1,Y_1) := \frac{\cov(X_1,Y_1)}{\sigma_X \sigma_Y}$

## Maximum likelihood estimators{#maximum-likelihood-estimators}

[({{< meta texts.cb.initials >}} §7.2.2; {{< meta texts.wms.initials >}} §9.7)]{.small}

The joint density of [data]{.alert},  $\vX = (X_1, \ldots, X_n)^\top$ given a parameter, $\vtheta$, is $\varrho_{\vX \mid  \vtheta}$.  The [likelihood]{.alert}, $L$ turns that around to make the parameter the variable, so 
$$
L(\vtheta  \mid  \vx) := \varrho_{\vX  \mid  \vtheta}(\vx); \qquad L(\vtheta  \mid  \vx)  = \prod_{i=1}^n \varrho_{X_1  \mid \vtheta}(x_i) \quad \text{if } X_1, \ldots, X_n \text{ are } \IID
$$

The [maximum likelihood estimator (MLE)]{.alert} of $\vtheta$ is the one that fits the observed data best in terms of
$$
\vTheta_{\MLE}  = \Argmax{\vtheta} L(\vtheta  \mid  \vX) 
$$

It may be easier to work with the [log-likelihood]{.alert} $\ell(\vtheta \mid \vX) := \log(L(\vtheta \mid \vX))$ since the logarithm is a monotone transformation, so
$$
\vTheta_{\MLE}  = \Argmax{\vtheta} \ell(\vtheta  \mid  \vX) 
$$

---

$\exstar$ What is the MLE of $p$ for the distribution $\Bern(p)$?

::: {.exitem}
<span class="exbullet">$\exstar$</span><span>What is the MLE of $\lambda$ for $\Exp(\lambda)$? What are the MLE of $\mu=\Ex(X)$ and $\sigma^2=\var(X)$ for $X\sim\Exp(\lambda)$?</span>
:::

$\exstar$ What are the MLE of $\mu$ and $\sigma$ for $X \sim \Norm(\mu,\sigma^2)$?



## Plug-in estimators{#plug-in-estimators}
- If $\hTheta_1$ is an estimator of $\theta_1$ and $\theta_2 = g(\theta_1)$, then $\hTheta_2 : = g(\hTheta_1)$ is a [plug-in estimator]{.alert} of $\theta_2$
- If $\hTheta_1$ is MLE of $\theta_1$ and $\theta_2 = g(\theta_1)$, then $\hTheta_2 : = g(\hTheta_1)$ is an MLE of $\theta_2$

# Properties of Estimators
- [Bias and variance](#biasvar)
- [Distributions of estimators](#distribest)
- [Consistency](#consistency)

## Bias, variance, and mean squared error of estimators{#biasvar}

[({{< meta texts.cb.initials >}} §7.3.1; {{< meta texts.wms.initials >}} §§8.2–8.4)]{.small}

Suppose that $\Theta$ is an estimator of a parameter, $\theta$, of a population

- [Bias]{.alert} $\bias(\Theta) = \Ex(\Theta) - \theta$
  - Asymptotic bias is $\displaystyle \lim_{n \to \infty} \bias(\Theta_n)$, where $n$ is the size of the sample on which the estimator is based
  - An estimator is [unbiased]{.alert} if $\bias(\Theta) = 0$
  - $\barX_n$ is an unbiased estimator of $\mu = \Ex(X_1)$ for identically distributed data

- [Variance]{.alert} we already know this definition
  - $\var(\barX_n) \exeq \var(X_1)/n$ for uncorrelated, identically distributed data

- [Mean squared error]{.alert} $\mse(\Theta) := \Ex[(\Theta - \theta)^2] \exeq [\bias(\Theta)]^2 + \var(\Theta)$

- [Standard Error]{.alert} $\se(\Theta) := \sqrt{\var(\Theta)}$ is the standard deviation of the sampling distribution of $\Theta$
  -  $\se(\barX_n) = \sqrt{\var(\barX_n)}$

---

$\exstar$ Show that $S^2 := \displaystyle \frac{1}{n-1} \sum_{i=1}^n (X_i - \barX_n)^2$ is an unbiased estimator of $\sigma^2$

$\exstar$ Show that $S = \sqrt{S^2}$ as an estimator of $\sigma$ has negative bias (see Jensen's inequality)

$\exstar$ Is the MLE of $\sigma=\std(X)$ for $X\sim\Exp(\lambda)$ unbiased?

$\exstar$ What is the MLE $\theta$ of $\theta$ for $X \sim \Unif(0,\theta)$?


## Distributions of estimators (see [Important Distributions](../slides/01-intro.html#prob-distrib)) {#distribest}

[({{< meta texts.cb.initials >}} §5.2–5.4; {{< meta texts.wms.initials >}} §§7.2)]{.small}

For the sample mean $\barX_n$, based on IID data

- $n \barX_n \sim \Bin(n,p)$ if $X \sim \Bern(p)$

- $\barX_n \exsim \Gam(n, n \lambda)$ if $X \sim \Exp(\lambda)$ where $\displaystyle \varrho_{\Gam(\alpha, \beta)}(x) = \frac{\beta^{\alpha}}{\Gamma(\alpha)}\, x^{\alpha-1} \exp(-\beta x)
\quad x>0$
  - Note: $\Gamma(n) = (n-1)!$ for integer $n$
  - $\Gam(\alpha, \beta)$ is the generic gamma distribution with shape $\alpha$ and rate $\beta$
  - $\lambda n \barX_n \exsim \Gam(n, 1)$
  - $2 \lambda n \barX_n \exsim \chi^2_{2n}$, where $\displaystyle \varrho_{\chi^2_\nu}(x) = \frac{x^{\nu/2 - 1} \exp(-x/2)}{2^{\nu/2} \Gamma(\nu/2)} \quad x > 0$

---

For the sample mean $\barX_n$, based on IID data (cont'd)

- $\barX_n \exsim \Norm(\mu,\sigma^2/n)$ if $X \sim \Norm(\mu,\sigma^2)$

- $\barX_n \appxsim \Norm(\mu,\sigma^2/n)$ for arbitrary distributions and large $n$ by the [Central Limit Theorem]{.alert}

- $\displaystyle \frac{\barX_n - \mu}{S_n/\sqrt{n}} \sim t_{n-1}$ if $X \sim \Norm(\mu,\sigma^2)$ where 

  - $\displaystyle S_n^2 := \frac{1}{n-1} \sum_{i=1}^n (X_i - \barX_n)^2$
  - $t_\nu$ is the [Student's t distribution]{.alert} with $\nu$ degrees of freedom

    - $\displaystyle \varrho_{t_\nu}(x) = \frac{\Gamma\!\left(\frac{\nu+1}{2}\right)}{\sqrt{\nu\pi}\,\Gamma\!\left(\frac{\nu}{2}\right)}\left(1+\frac{x^2}{\nu}\right)^{-(\nu+1)/2}, \quad -\infty < x < \infty$
    - Symmetric about $0$
    - Heavier tails than the standard normal
    - $\exstar$ Converges to $N(0,1)$ as $\nu \to \infty$
  

---

For the unbiased sample variance, $S_n^2$, for [$\Norm(\mu,\sigma^2)$]{.alert} based on IID data

- $\displaystyle \frac{(n-1) S_n^2}{\sigma^2} \sim \chi^2_{n-1}$

&nbsp;

For order statistics, $X_{(k)}$, $\displaystyle F_{X_{(k)}}(x) = \sum_{j=k}^n \binom{n}{j} [F_X(x)]^j [1 - F_X(x)]^{n-j}$ for IID data from CDF $F_X$

&nbsp;

&nbsp;

$\exstar$ Is the MLE $\theta$ of $\theta$ for $X \sim \Unif(0,\theta)$ unbiased?  Can you modify it to be unbiased?


## Consistency of estimators {#consistency}

[({{< meta texts.cb.initials >}} §10.1; {{< meta texts.wms.initials >}} §9.3)]{.small}


Let $\Theta_n$ be an estimate $\theta$ based on a sample of size $n$.  This estimator is [consistent]{.alert} if 
$$
\Theta_n \pto \theta \quad \text{as } n \to \infty
$$

This is automatic if $\Theta_n$ is (asymptotically) unbiased and its variance  vanishes as $n \to \infty$

::: {.key-point}
The sample mean is a consistent estimator of the population mean if the variance of the data is finite
:::


