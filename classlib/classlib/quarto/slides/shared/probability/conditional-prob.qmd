## Conditional probability{#conditional-prob}

[({{< meta texts.cb.initials >}} §1.3, §4.2; {{< meta texts.wms.initials >}} ???)]{.small}

The [conditional probability]{.alert} means restricting attention to outcomes where $B$ occurs, then
measure how often $A$ occurs within that restricted universe of $A$ given $B$ (with $\Prob(B)>0$):
$$
\Prob(A \mid B)
=
\frac{\Prob(A \cap B)}{\Prob(B)},
$$
which means restricting attention to outcomes where $B$ occurs, then
measuring how often $A$ occurs within that restricted universe.

- [Multiplication rule]{.alert}:
$\Prob(A \cap B)
=
\Prob(A \mid B) \Prob(B)$

- $A$ and $B$ are [independent]{.alert} if
$\Prob(A \mid B) = \Prob(A)$ or equivalently 
$\Prob(A \cap B)=\Prob(A)\Prob(B)$

### Bayes' Rule
$$
\Prob(A \mid B)
=
\frac{\Prob(B \mid A) \, \Prob(A)}{\Prob(B)},
\qquad \Prob(B)>0
$$

---

Let $X$ and $Y$ have [joint density]{.alert}
$\varrho_{X,Y}(x,y)$.  The [conditional density]{.alert} of $X$ given $Y=y$ is
$$
\varrho_{X\mid Y}(x \mid y)
=
\frac{\varrho_{X,Y}(x,y)}{\varrho_Y(y)},
\qquad
\varrho_Y(y)=\int \varrho_{X,Y}(x,y) \, \dif x \text{ is the }\class{alert}{\text{marginal density}} \text{ of } Y
$$
	
### Bayes’ rule for densities
$$
\varrho_{X\mid Y}(x\mid y)
=
\frac{\varrho_{Y\mid X}(y\mid x)\,\varrho_X(x)}{\varrho_Y(y)}
$$

$\exstar$ If you meet an introvert, is she more likely to be a librarian or a salesman?


$\exstar$ What probability distributions defined on $[0,\infty)$ satisfy the [memoryless]{.alert} property, i.e. $\Prob[X > t + x | X > t] = \Prob[X > x]$?

## Bayesian statistics to approximate and integrate functions

We assume that $f$, the function to approximate or integrate, is an instance of a Gaussian process, $\GP(0,K)$.  This means that

\begin{gather*}
\vf := \bigl(f(x_1), \ldots, f(x_n) \bigr)^\top \sim \Norm(\vzero, \mK), \quad \text{where }\mK : = \bigl(K(x_i,x_j)\bigr)_{i,j=1}^n \\
\tvf := \bigl(f(x_1), \ldots, f(x_n), f(x) \bigr)^\top \sim \Norm(\vzero, \tmK) \\
\text{where }
\tmK : = \left(\begin{array}{c|c} 
\mK & \vk(x) \\
\hline
\vk^\top(x) & K(x,x)
\end{array}
\right)
, \quad \vk(x) = \bigl( K(x,x_i) \bigr)_{i=1}^n
\end{gather*}
and so the conditional density of $f(x)$ given $\vf = \vy$, where $\vy$ is the observed data, is  
\begin{align*}
\varrho_{f(x) | \vf}(z | \vy) = \frac{\varrho_{\tvf}(\tvy)}{\varrho_{\vf}(\vy)} = \frac{\frac{\exp(-\tvy^\top \tmK^{-1} \tvy/2)}{\sqrt{(2 \pi)^{n+1} \det(\tmK)}}}{\frac{\exp(-\vy^\top \mK^{-1} \vy/2)}{\sqrt{(2 \pi)^{n} \det(\mK)}}}
= \frac{\exp([-\tvy^\top \tmK^{-1} \tvy + \vy^\top \mK^{-1} \vy]/2)}{\sqrt{(2 \pi) \det(\tmK)/\det(\mK)}}
\end{align*}
where $\tvy = (\vy^\top, z)^\top$

---

One can show that the inverse of the covariance matrix is 

\begin{gather*}
\tmK^{-1}
=
\left(\begin{array}{c|c}
\mK^{-1}+\mK^{-1}\vk(x)\sigma^{-2}(x)\vk^\top(x)\mK^{-1}
&
-\mK^{-1}\vk(x)\sigma^{-2}(x)
\\
\hline
- \sigma^{-2}(x)\vk^\top(x)\mK^{-1}
&
\sigma^{-2}(x)
\end{array}
\right)\\
\text{where }
\sigma^2(x)=K(x,x)-\vk^\top(x)\mK^{-1}\vk(x), \qquad
\sigma^{-2}(x) := \big(\sigma^2(x)\big)^{-1}
\end{gather*}

So 

\begin{align*}
\MoveEqLeft{-\tvy^\top \tmK^{-1} \tvy + \vy^\top \mK^{-1} \vy}\\
& = -\vy^\top[\mK^{-1}+\mK^{-1}\vk(x)\sigma^{-2}(x)\vk^\top(x)\mK^{-1}]\vy + 2 \vy^\top\mK^{-1}\vk(x)\sigma^{-2}(x) z - z^2 \sigma^{-2}(x) \\
& \qquad \qquad + \vy^\top \mK^{-1} \vy \\
& = \sigma^{-2}(x) [ z - \vy^\top \mK^{-1} \vk(x)]^2
\end{align*}

and $f(x) \mid \vf = \vy \sim \Norm\bigl(\vy^\top \mK^{-1} \vk(x), \sigma^2(x) \bigr)$

::: {.vspace}

By a similar argument  $\int_0 ^1 f(x)\, \dif x \mid \vf = \vy \sim \Norm\Bigl(\vy^\top \mK^{-1} \int_0^1 \vk(x) \, \dif x, \int_0^1 \int_0^1 K(t,x) \, \dif t \dif x - \int_0^1 \vk^\top(x) \, \dif x \, \mK^{-1} \, \int_0^1 \vk(x) \, \dif x  \Bigr)$
:::

## Bayesian inference
	
[({{< meta texts.cb.initials >}} §7.2.3)]{.small}

### Bayes’ rule for densities
$$
\varrho_{X\mid Y}(x\mid y)
=
\frac{\varrho_{Y\mid X}(y\mid x)\,\varrho_X(x)}{\varrho_Y(y)}
$$

Let
\begin{align*}
X & = \text{random unknown parameter} \\
Y & = \text{random observed data} \\
\varrho_X & = \text{prior density or belief about the parameter} \\
\varrho_{Y\mid X} & = \text{likelihood} \\
\varrho_{X\mid Y} & = \text{posterior density}
\end{align*}
Then
\begin{align*}
 \varrho_{X\mid Y}( \text{parameter} \mid \text{data})
& =
\frac{\varrho_{Y\mid X}(\text{data}\mid \text{parameter} ) \,\varrho_X(\text{parameter})}{\varrho_Y(\text{data})}
\\
\text{posterior density } &
\propto
\text{likelihood} \times \text{prior density}
\end{align*}


## Conditional expectation {#conditional-exp}

Conditional expectation of [$X$ given event $A$]{.alert}
(with $\Prob(A)>0$) is
$$
\Ex[X \mid A]
=
\frac{\Ex[X \, \indic(X \in A)]}{\Prob(A)} \qquad \text{and so} \qquad \Ex[X \, \indic(X \in A)] 
=
\Prob(A)\,\Ex[X \mid A]
$$
The [indicator function]{.alert}, $\indic(\cdot)$, equals $1$ if the statement inside the parentheses is true, and $0$ otherwise.

:::{.vspace-lg}

Conditional expectation of [$X$ given $Y=y$]{.alert}
is
$$
\Ex[X \mid Y=y]
=
\int x \, \varrho_{X\mid Y}(x \mid y)\,\dif x
$$
If $g(y)=\Ex[X \mid Y=y]$, then $\Ex[X \mid Y] = g(Y)$ is a random variable
depending only on $Y$

:::

:::{.vspace-lg}

If $X$ is [independent]{.alert} of $Y$, then $\Ex[X \mid Y] = \Ex[X]$

:::

---

### Conditioning on an event

For any event $A$ with $0<\Prob(A)<1$,
$$
\Ex[X]
=
\Prob(A)\,\Ex[X \mid A]
+
\Prob(A^c)\,\Ex[X \mid A^c]
$$

More generally, if $\{A_1,\ldots,A_n\}$ is a partition of the sample space with
$\Prob(A_i)>0$, then
$$
\Ex[X]
=
\sum_{i=1}^n
\Prob(A_i)\,\Ex[X \mid A_i].
$$

### Law of total expectation 
$$
\Ex\!\bigl[\Ex[X \mid Y]\bigr]
=
\Ex[X]
$$

### Law of total variance

$$
\var(X)
=
\Ex\!\bigl[\var(X \mid Y)\bigr]
+
\var\!\bigl(\Ex[X \mid Y]\bigr)
$$
